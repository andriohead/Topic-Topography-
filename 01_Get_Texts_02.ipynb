{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** FROM Establishing an API interface for your program ***\n",
    "# ***              ELSEVIER DEVELOPER PORTAL              ***\n",
    "\n",
    "\"\"\"An example program that uses the elsapy module\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import regex\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "\n",
    "from urllib.parse import quote_plus as url_encode\n",
    "import json, pathlib\n",
    "from tqdm import tqdm  # Para mostrar barra de progreso\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union, Optional, List\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# TEXT EXTRACTION\n",
    "import unicodedata\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load configuration\n",
    "con_file = open(\"01_Extract_Information_Data/config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "## Initialize client\n",
    "client = ElsClient(config['apikey'])\n",
    "client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_BASE = {\n",
    "    \"X-ELS-APIKey\" : \"aba41ff058a2312128b290e1e4ef2408\",\n",
    "    \"X-ELS-Insttoken\": \"5d854c51cbf02f31a0a69c61ee0aafec\",\n",
    "    \"Accept\": 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TEXT EXTRACTION FROM XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONFIG ======\n",
    "XML_DIR     = \"01_Extract_Information_Data/txt_SD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== FIRST CLEAN ======\n",
    "URL_RE      = re.compile (r\"(https?://\\S+|www\\.S+)\", re.I)\n",
    "IMG_RE      = re.compile (r\"\\b(svg|jpg|png|gif|xml|altimg|amazonaws|cdn|pii|doi)\\b\", re.I)\n",
    "LONG_ID     = re.compile (r\"\\b[A-Z0_9\\-]{15,}\\b\")\n",
    "MATH_SYM    = re.compile (r\"[=¬±œÜŒ≤‚àë‚àö√ó‚â§‚â•^{}]\")\n",
    "\n",
    "PARA_TAGS   = {\"p\", \"para\", \"simple-para\"}\n",
    "TITLE_TAGS  = {\"title\", \"section-title\"}\n",
    "INTRO_RE    = re.compile (r\"\\b(introduction|motivation and background|background and introduction)\\b\", re.I)\n",
    "CONCL_RE    = re.compile (r\"\\b(?:\\d+(\\.\\d+)*)?\\s*(conclusion|conclusions|concluding remarks|summary|discussion and conclusions)\\b\", re.I)\n",
    "\n",
    "EXCLUDE_PATTERNS = [\n",
    "    # Usar \\b para l√≠mites de palabra y ^/$ para t√≠tulos exactos\n",
    "    r\"^\\s*acknowledgments?\\s*$\",\n",
    "    r\"^\\s*acknowledgements?\\s*$\",\n",
    "    r\"^\\s*ack\\.?\\s*$\",\n",
    "    r\"^\\s*references?\\s*$\", \n",
    "    r\"^\\s*bibliography\\s*$\",\n",
    "    r\"^\\s*appendix\\s*$\",\n",
    "    r\"^\\s*appendices\\s*$\",\n",
    "    r\"^\\s*supplementary\\s+(?:materials?|information|data)\\s*$\",\n",
    "    r\"^\\s*supporting\\s+information\\s*$\",\n",
    "    r\"^\\s*author\\s+contributions?\\s*$\",\n",
    "    r\"^\\s*conflict\\s+of\\s+interest\\s*$\",\n",
    "    r\"^\\s*competing\\s+interests?\\s*$\", \n",
    "    r\"^\\s*funding\\s*$\",\n",
    "    r\"^\\s*data\\s+availability\\s*$\",\n",
    "    r\"^\\s*declarations?\\s*$\",\n",
    "    r\"^\\s*credit\\s+author\\s*$\",\n",
    "    r\"^\\s*glossary\\s*$\",\n",
    "    r\"^\\s*nomenclature\\s*$\",\n",
    "    r\"^\\s*acronyms\\s*$\",\n",
    "    r\"^\\s*proof\\s+of\\s+(?:theorem|lemma|proposition|corollary)\\s*$\",\n",
    "    r\"^\\s*summary\\s*$\",\n",
    "]\n",
    "\n",
    "EXCLUDE_RE = re.compile(\"|\".join(EXCLUDE_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "SKIP_SUBTREES = {\n",
    "    \"ref_list\", \"references\", \"bibliography\", \"back\",\n",
    "    \"figure\", \"table\", \"caption\", \"thead\", \"tbody\", \"tfoot\",\n",
    "    \"nomenclature\", \"acronyms\", \"abbreviations\", \"mi\", \"mrow\", \"mo\"\n",
    "}\n",
    "\n",
    "FORMULA_CONTAINERS = {\n",
    "    \"display-formula\", \"inside-formula\", \"formula\", \"display\", \"ce-display\", \"cd:display\", \"mml:mi\"\n",
    "}\n",
    "\n",
    "MATHML_LOCAL= {\n",
    "    \"math\", \"mrow\", \"msub\", \"msup\", \"msubsup\", \"mover\", \"munder\", \"munderover\",\n",
    "    \"mi\", \"mo\", \"mn\", \"mfenced\", \"mfrac\", \"msqrt\", \"mroot\", \"mtable\", \"mtr\", \"mtd\", \"mrow\"\n",
    "}\n",
    "\n",
    "STRUCTURE_PATTERNS = [\n",
    "    r\"figure[s]?\", r\"fig\\.?\", r\"table[s]?\", r\"tab\\.?\",  # ‚ùå Quitar \\b si usas (?i)\n",
    "    r\"appendix\", r\"supplementary\", r\"caption\",\n",
    "    r\"equation\", r\"eq\\.?\", r\"formula\",\n",
    "    r\"algorithm\", r\"alg\\.?\", r\"pseudocode\",\n",
    "    r\"definition\", r\"theorem\", r\"lemma\", r\"corollary\",\n",
    "    r\"proof\", r\"proposition\", r\"axiom\",\n",
    "    r\"acknowledgments?\", r\"ack\\.?\",\n",
    "    r\"references?\", r\"bibliography\",\n",
    "    r\"introduction\", r\"conclusion\",\n",
    "    r\"methodology\", r\"methods?\",\n",
    "    r\"results?\", r\"discussion\",\n",
    "    r\"abstract\", r\"keywords\",\n",
    "    r\"background\", r\"related work\",\n",
    "    r\"future work\", r\"limitations\",\n",
    "    r\"conflict of interest\",\n",
    "    r\"data availability\",\n",
    "    r\"author contributions\",\n",
    "    r\"funding\", r\"grant\",\n",
    "    r\"peer review\",\n",
    "    r\"received.*accepted\",\n",
    "    r\"published.*elsevier\",\n",
    "    r\"journal.*elsevier\",\n",
    "    r\"¬©\\s?\\d{4}.+?elsevier.+?$\"\n",
    "]\n",
    "STRUCT_RE = re.compile(\"|\".join(STRUCTURE_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "\n",
    "UNITS_PATTERNS = [\n",
    "    r\"\\b\\d+(?:[\\.,]\\d+)?\\s?(hz|khz|mhz|db|mm|nm|ms|fps|gb|mb|km/h)\\b\",\n",
    "    r\"\\b(intel|amd|nvidia|ram|cpu|gpu)\"\n",
    "]\n",
    "UNITS_RE = re.compile(\"|\".join(UNITS_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "def _local(tag: str) -> str:\n",
    "    return tag.rsplit(\"}\", 1)[-1] if \"}\" in tag else tag\n",
    "\n",
    "def _ns(tag: str) -> str:\n",
    "    return tag[1:].split(\"}\")[0] if tag.startswith(\"{\") and \"}\" in tag else \"\"\n",
    "\n",
    "def strip_mathml_and_formulas (root: ET.Element):\n",
    "    parent_map = {child: parent for parent in root.iter() for child in parent}\n",
    "    to_remove = []\n",
    "    for element in root.iter():\n",
    "        ns = _ns(element.tag).lower()\n",
    "        loc = _local(element.tag).lower()\n",
    "        if (\"mathml\" in ns or ns.endswith(\":mathml\")):\n",
    "            to_remove.append(element); continue\n",
    "        if loc in MATHML_LOCAL:\n",
    "            to_remove.append(element); continue\n",
    "        if loc in FORMULA_CONTAINERS:\n",
    "            to_remove.append(element); continue\n",
    "        \n",
    "    for element in to_remove:\n",
    "        parent = parent_map.get(element)\n",
    "        if parent is not None:\n",
    "            try:\n",
    "                parent.remove(element)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "GREEK_BLOCKS = (\n",
    "    (0x0373, 0x03FF),\n",
    "    (0x1F00, 0x1FFF)\n",
    ")\n",
    "\n",
    "def _is_greek_char(ch: str) -> bool:\n",
    "    cp = ord(ch)\n",
    "    for a, b in GREEK_BLOCKS:\n",
    "        if a <= cp <= b:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def strip_residual_math_chars(text: str) -> str:\n",
    "    text = \"\".join(ch if unicodedata.category(ch) != \"Sm\" else \" \" for ch in text)\n",
    "    text = \"\".join(ch if not _is_greek_char(ch) else \" \" for ch in text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_text_strict (t: str) -> str:\n",
    "    if not t: return \"\"\n",
    "    t = regex.sub(r\"\\s+\", \" \", t)\n",
    "    t = unicodedata.normalize (\"NFKC\", str(t))\n",
    "    t = t.replace (\"\\u00A0\", \" \").replace(\"\\u200B\", \" \")\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = IMG_RE.sub(\" \", t)\n",
    "    t = LONG_ID.sub(\" \", t)\n",
    "    t = MATH_SYM.sub(\" [EQUATION] \", t)\n",
    "    t = STRUCT_RE.sub(\" \", t)\n",
    "    t = UNITS_RE.sub(\" \", t)\n",
    "    # compact spaces, jumps...\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s*\\n+\\s*\", \"\\n\\n\", t)\n",
    "    t = re.sub(r\"\\s*\\[EQUATION\\]\\s*\", \" \", t)\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "def find_article_body (root: ET.Element):\n",
    "    bodies = [element for element in root.iter() if _local(element.tag) == \"body\"]\n",
    "    return bodies[0] if bodies else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 EXTRACT SECTIONS OF TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SECTIONS OF TEXT\n",
    "OUT_PATH = \"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\"\n",
    "\n",
    "def _norm_title(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "    s = re.sub(r\"\\.$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def extract_sections_int_conc(root: ET.Element, doc_id: str = \"unknown\") -> str:\n",
    "    body = find_article_body(root)\n",
    "    if body is None:\n",
    "        return [\"\"]\n",
    "\n",
    "    sections = []\n",
    "    cur_title = None\n",
    "    cur_pars = []\n",
    "    inside = False\n",
    "    reading_conclusion = False\n",
    "    finished = False\n",
    "\n",
    "    def is_introduction(txt: str) -> bool:\n",
    "        t = re.sub(r\"\\s+\", \" \", (txt or \"\")).strip()\n",
    "        return bool(INTRO_RE.search(t))\n",
    "    \n",
    "    def is_conclusion(txt: str) -> bool:\n",
    "        t = re.sub(r\"\\s+\", \" \", (txt or \"\")).strip()\n",
    "        return bool(CONCL_RE.search(t))\n",
    "\n",
    "    def flush_section():\n",
    "        nonlocal cur_title, cur_pars\n",
    "        if cur_title is None and not cur_pars:\n",
    "            return\n",
    "        text = clean_text_strict(\"\\n\\n\".join([p for p in cur_pars if p]).strip())\n",
    "        if text:\n",
    "            sections.append({\"sec_title\": _norm_title(cur_title or \"\"), \"sec_text\": text})\n",
    "        cur_title, cur_pars = None, []\n",
    "    \n",
    "    skip_stack = [False]\n",
    "\n",
    "    def walk(element: ET.Element):\n",
    "        nonlocal inside, finished, cur_title, cur_pars, reading_conclusion\n",
    "        \n",
    "        if finished: \n",
    "            return\n",
    "            \n",
    "        tag_loc = _local(element.tag)\n",
    "        parent_skip = skip_stack[-1]\n",
    "        here_skip = parent_skip or (tag_loc in SKIP_SUBTREES)\n",
    "        skip_stack.append(here_skip)\n",
    "\n",
    "        if not here_skip:\n",
    "            if tag_loc in TITLE_TAGS:\n",
    "                title = \"\".join(element.itertext()).strip()\n",
    "                if title:\n",
    "                    if is_introduction(title):\n",
    "                        inside = True\n",
    "                        cur_title, cur_pars = title, []\n",
    "                        reading_conclusion = False\n",
    "                    elif inside:\n",
    "                        if is_conclusion(title):\n",
    "                            flush_section()  # Guardar secci√≥n anterior\n",
    "                            cur_title, cur_pars = title, []\n",
    "                            reading_conclusion = True\n",
    "                        else:\n",
    "                            flush_section()  # Guardar secci√≥n anterior\n",
    "                            cur_title = title\n",
    "                            # Si est√°bamos en conclusiones y encontramos otro t√≠tulo, \n",
    "                            # es una subsecci√≥n de conclusiones\n",
    "                            if reading_conclusion:\n",
    "                                cur_pars.append(f\"\\n{title}:\")  # A√±adir como marcador\n",
    "\n",
    "            # ‚úÖ CORRECCI√ìN: SOLO recolectar p√°rrafos, NO hacer flush autom√°tico\n",
    "            if tag_loc in PARA_TAGS and inside and not finished:\n",
    "                txt = \"\\n\\n\".join(element.itertext()).strip()\n",
    "                if txt:\n",
    "                    cur_pars.append(txt)\n",
    "                    # ‚ùå ELIMINADO: el flush autom√°tico que cortaba las conclusiones\n",
    "        \n",
    "        for ch in element:\n",
    "            if not finished:\n",
    "                walk(ch)\n",
    "        skip_stack.pop()\n",
    "\n",
    "    walk(body)\n",
    "\n",
    "    # ‚úÖ FLUSH FINAL para las conclusiones (no se hace autom√°ticamente)\n",
    "    if reading_conclusion and cur_pars:\n",
    "        flush_section()\n",
    "\n",
    "    # Fallback si no se encontraron secciones\n",
    "    if not sections:\n",
    "        all_pars = [\"\".join(element.itertext()).strip()\n",
    "                   for element in body.iter()\n",
    "                   if _local(element.tag) in PARA_TAGS]\n",
    "        text = clean_text_strict(\"\\n\\n\".join([p for p in all_pars if p]).strip())\n",
    "        if text:\n",
    "            sections = [{\"sec_title\": \"\", \"sec_text\": text}]\n",
    "    \n",
    "    # Metadata\n",
    "    for i, s in enumerate(sections):\n",
    "        s[\"sec_order\"] = i\n",
    "        s[\"n_chars\"] = len(s[\"sec_text\"])\n",
    "        s[\"n_words\"] = len(re.findall(r\"\\w+\", s[\"sec_text\"]))\n",
    "        s[\"doc_id\"] = doc_id\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def process_file_sections (path_xml: str) -> str:\n",
    "    doc_id = os.path.splitext(os.path.basename(path_xml))[0]\n",
    "    try:\n",
    "        root = ET.parse(path_xml).getroot()\n",
    "    except ET.ParseError:\n",
    "        print (f\"‚ùå ParseError in file: {doc_id}\")\n",
    "        return \"\"\n",
    "    strip_mathml_and_formulas(root)\n",
    "    txt = extract_sections_int_conc(root, doc_id)\n",
    "    return txt\n",
    "\n",
    "def build_sections_parquet ():\n",
    "    files = sorted (glob.glob(os.path.join(XML_DIR, \"*.xml\")))\n",
    "    rows = []\n",
    "\n",
    "    for path_xml in tqdm(files, desc=\"Extracting XML sections\"):\n",
    "        try:\n",
    "            result = process_file_sections(path_xml)\n",
    "            if result:\n",
    "                rows.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing file {os.path.basename(path_xml)}: {e}\")\n",
    "    \n",
    "    rows = [r for r in rows if isinstance(r, dict)]\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    expected = [\"doc_id\", \"sec_order\", \"sec_title\", \"sec_text\", \"n_words\", \"n_chars\"]\n",
    "    for c in expected:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\" if c in (\"sec_title\", \"sec_text\") else 0\n",
    "    df = df[expected].reset_index(drop=True)\n",
    "\n",
    "    table = pa.Table.from_pydict({\n",
    "        \"doc_id\"    : pa.array(df[\"doc_id\"].astype(str).tolist()),\n",
    "        \"sec_order\" : pa.array(df[\"sec_order\"].astype(\"int32\").tolist(), type=pa.int32()),\n",
    "        \"sec_title\" : pa.array(df[\"sec_title\"].astype(str).tolist()),\n",
    "        \"sec_text\"  : pa.array(df[\"sec_text\"].astype(str).tolist()),\n",
    "        \"n_chars\"   : pa.array(df[\"n_chars\"].astype(\"int32\").tolist(), type=pa.int32()),\n",
    "        \"n_words\"   : pa.array(df[\"n_words\"].astype(\"int32\").tolist(), type=pa.int32())\n",
    "    })\n",
    "    pq.write_table(table, OUT_PATH, compression=\"zstd\")\n",
    "    print (f\" ‚úÖ Saved: {OUT_PATH}.  ({len(df)} documents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sections_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 EXTRACT ALL SECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SECTIONS OF TEXT\n",
    "OUT_PATH = \"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_title(s: str) -> str:\n",
    "    \"\"\"Normalize title: remove extra spaces and trailing period\"\"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
    "    s = re.sub(r\"\\.$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def extract_all_sections_with_filter(root: ET.Element, doc_id: str = \"unknown\") -> list:\n",
    "    \"\"\"\n",
    "    Extract ALL sections from body and then filter unwanted sections\n",
    "    Maintains the original output structure\n",
    "    \"\"\"\n",
    "    body = find_article_body(root)\n",
    "    if body is None:\n",
    "        print(f\"‚ùå No body found in file: {doc_id}\")\n",
    "        return [\"\"]\n",
    "\n",
    "    sections    = []\n",
    "    cur_title   = None\n",
    "    cur_pars    = []\n",
    "    cur_label   = None\n",
    "    pending_label = None\n",
    "    \n",
    "    def should_exclude_section(title: str) -> bool:\n",
    "        \"\"\"Check if section should be excluded based on title\"\"\"\n",
    "        if not title:\n",
    "            return False\n",
    "        normalized_title = re.sub(r\"\\s+\", \" \", title.lower().strip())\n",
    "        return bool(EXCLUDE_RE.search(normalized_title))\n",
    "\n",
    "    def flush_section():\n",
    "        \"\"\"Save current section to results if not excluded\"\"\"\n",
    "        nonlocal cur_title, cur_pars, cur_label, pending_label\n",
    "        if cur_title is None and not cur_pars:\n",
    "            return\n",
    "        \n",
    "        # Check if we should exclude this section\n",
    "        if cur_title and should_exclude_section(cur_title):\n",
    "            print(f\"üîï Excluding section: '{cur_title}'\")\n",
    "            cur_title, cur_pars, cur_label = None, [], None\n",
    "            return\n",
    "            \n",
    "        text = clean_text_strict(\"\\n\\n\".join([p for p in cur_pars if p]).strip())\n",
    "        if text or cur_title:  # Keep sections with title even if empty text\n",
    "            sections.append({\n",
    "                \"sec_title\" : _norm_title(cur_title) if cur_title else \"\",\n",
    "                \"sec_text\"  : text,\n",
    "                \"label\"     : cur_label if cur_label else \"\"\n",
    "            })\n",
    "        cur_title, cur_pars, cur_label = None, [], None\n",
    "    \n",
    "    skip_stack = [False]\n",
    "\n",
    "    def walk(element: ET.Element):\n",
    "        \"\"\"Walk through XML tree and collect all sections\"\"\"\n",
    "        nonlocal cur_title, cur_pars, cur_label, pending_label\n",
    "        \n",
    "        tag_loc = _local(element.tag)\n",
    "        parent_skip = skip_stack[-1]\n",
    "        here_skip = parent_skip or (tag_loc in SKIP_SUBTREES)\n",
    "        skip_stack.append(here_skip)\n",
    "\n",
    "        if not here_skip:\n",
    "\n",
    "            # Extract label if present \n",
    "            if tag_loc == 'label':\n",
    "                label_text = \"\".join(element.itertext()).strip()\n",
    "                if label_text:\n",
    "                    pending_label = label_text\n",
    "\n",
    "            # Handle section titles\n",
    "            if tag_loc in TITLE_TAGS:\n",
    "                title = \"\".join(element.itertext()).strip()\n",
    "                if title:\n",
    "                    # Save previous section before starting new one\n",
    "                    flush_section()\n",
    "\n",
    "                    cur_title     = title\n",
    "                    cur_pars      = []\n",
    "                    cur_label     = pending_label       # Use previous label\n",
    "                    pending_label = None                # Reset \n",
    "            \n",
    "            # Collect paragraphs for current section\n",
    "            if tag_loc in PARA_TAGS:\n",
    "                txt = \"\\n\\n\".join(element.itertext()).strip()\n",
    "                if txt:\n",
    "                    cur_pars.append(txt)\n",
    "        \n",
    "        # Process children\n",
    "        for ch in element:\n",
    "            walk(ch)\n",
    "        skip_stack.pop()\n",
    "\n",
    "    # Extract ALL sections from body\n",
    "    walk(body)\n",
    "    \n",
    "    # Save the last section\n",
    "    flush_section()\n",
    "\n",
    "    # Fallback: if no sections found, extract all paragraphs\n",
    "    if not sections:\n",
    "        print(f\"‚ùå No sections found in file: {doc_id}\")\n",
    "        all_pars = [\"\".join(element.itertext()).strip()\n",
    "                    for element in body.iter()\n",
    "                    if _local(element.tag) in PARA_TAGS]\n",
    "        text = clean_text_strict(\"\\n\\n\".join([p for p in all_pars if p]).strip())\n",
    "        if text:\n",
    "            sections = [{\"sec_title\": \"\", \"sec_text\": text, \"label\": \"\"}]\n",
    "    \n",
    "    # Add metadata (maintaining original structure)\n",
    "    for i, s in enumerate(sections):\n",
    "        s[\"sec_order\"] = i\n",
    "        s[\"n_chars\"] = len(s[\"sec_text\"])\n",
    "        s[\"n_words\"] = len(re.findall(r\"\\w+\", s[\"sec_text\"]))\n",
    "        s[\"doc_id\"] = doc_id\n",
    "    \n",
    "    print(f\"‚úÖ {doc_id}: Extracted {len(sections)} sections\")\n",
    "    return sections\n",
    "\n",
    "def process_file_sections(path_xml: str) -> str:\n",
    "    \"\"\"Process individual XML file and extract sections\"\"\"\n",
    "    doc_id = os.path.splitext(os.path.basename(path_xml))[0]\n",
    "    try:\n",
    "        root = ET.parse(path_xml).getroot()\n",
    "    except ET.ParseError:\n",
    "        print(f\"‚ùå ParseError in file: {doc_id}\")\n",
    "        return \"\"\n",
    "    strip_mathml_and_formulas(root)\n",
    "    # Use the new comprehensive extraction function\n",
    "    txt = extract_all_sections_with_filter(root, doc_id)\n",
    "    return txt\n",
    "\n",
    "def build_sections_parquet():\n",
    "    \"\"\"Build parquet file with all extracted sections\"\"\"\n",
    "    files = sorted(glob.glob(os.path.join(XML_DIR, \"*.xml\")))\n",
    "    rows = []\n",
    "\n",
    "    for path_xml in tqdm(files, desc=\"Extracting XML sections\"):\n",
    "        try:\n",
    "            result = process_file_sections(path_xml)\n",
    "            if result:\n",
    "                rows.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing file {os.path.basename(path_xml)}: {e}\")\n",
    "    \n",
    "    # Filter and validate rows\n",
    "    rows = [r for r in rows if isinstance(r, dict)]\n",
    "    \n",
    "    # Create DataFrame with expected structure\n",
    "    df = pd.DataFrame(rows)\n",
    "    expected = [\"doc_id\", \"sec_order\", \"label\", \"sec_title\", \"sec_text\", \"n_words\", \"n_chars\"]\n",
    "    for c in expected:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\" if c in (\"sec_title\", \"sec_text\") else 0\n",
    "    df = df[expected].reset_index(drop=True)\n",
    "\n",
    "    # Create PyArrow table with proper types\n",
    "    table = pa.Table.from_pydict({\n",
    "        \"doc_id\": pa.array(df[\"doc_id\"].astype(str).tolist()),\n",
    "        \"sec_order\": pa.array(df[\"sec_order\"].astype(\"int32\").tolist(), type=pa.int32()),\n",
    "        \"label\"     : pa.array(df[\"label\"].astype(str).tolist()),\n",
    "        \"sec_title\": pa.array(df[\"sec_title\"].astype(str).tolist()),\n",
    "        \"sec_text\": pa.array(df[\"sec_text\"].astype(str).tolist()),\n",
    "        \"n_chars\": pa.array(df[\"n_chars\"].astype(\"int32\").tolist(), type=pa.int32()),\n",
    "        \"n_words\": pa.array(df[\"n_words\"].astype(\"int32\").tolist(), type=pa.int32())\n",
    "    })\n",
    "    \n",
    "    # Write to parquet\n",
    "    pq.write_table(table, OUT_PATH, compression=\"zstd\")\n",
    "    print(f\"‚úÖ Saved: {OUT_PATH}. ({len(df)} documents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sections_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\")\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOURNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_journal_name(xml_file_path: str) -> str:\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Namespaces potenciales (puedes ampliarlos si es necesario)\n",
    "        namespaces = {\n",
    "            \"prism\": \"http://prismstandard.org/namespaces/basic/2.0/\"\n",
    "        }\n",
    "\n",
    "        # 1. Intentar encontrar <prism:publicationName>\n",
    "        journal_elem = root.find(\".//prism:publicationName\", namespaces)\n",
    "        if journal_elem is not None and journal_elem.text:\n",
    "            return journal_elem.text.strip()\n",
    "\n",
    "        # 2. Buscar cualquier etiqueta que termine en publicationName (sin namespace)\n",
    "        for elem in root.iter():\n",
    "            tag_name = elem.tag.split(\"}\")[-1]  # Quitar namespace\n",
    "            if tag_name == \"publicationName\" and elem.text:\n",
    "                return elem.text.strip()\n",
    "\n",
    "        # 3. Fallback: intentar prism sin declarar namespace\n",
    "        journal_elem2 = root.find(\".//publicationName\")\n",
    "        if journal_elem2 is not None and journal_elem2.text:\n",
    "            return journal_elem2.text.strip()\n",
    "\n",
    "        return None\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"‚ö†Ô∏è ParseError en {os.path.basename(xml_file_path)}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en {os.path.basename(xml_file_path)}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = []\n",
    "for i in range (0, 3818):\n",
    "    file_path = f\"01_Extract_Information_Data/txt_SD/{i:04d}.xml\"\n",
    "    journal_name = extract_journal_name(file_path)\n",
    "\n",
    "    journals.append({\n",
    "        'doc_id'        : i,\n",
    "        'journal_name'  : journal_name\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_df = pd.DataFrame(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_df.to_csv(\"01_Extract_Information_Data/Journal.csv\")\n",
    "display(journals_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 CONCLUSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_conclusions = pd.read_parquet(\"01_Extract_Information_Data/02_Clean_Sections_SD_02.parquet\")\n",
    "df_not_conclusions['key'] = df_not_conclusions['doc_id'].astype(str) + '-' + df_not_conclusions['sec_title'] + '-' + df_not_conclusions['n_chars'].astype(str)\n",
    "\n",
    "df_with_conclusions = pd.read_parquet(\"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\")\n",
    "df_with_conclusions['key'] = df_with_conclusions['doc_id'].astype(str) + '-' + df_with_conclusions['sec_title'] + '-' + df_with_conclusions['n_chars'].astype(str)\n",
    "\n",
    "display(df_with_conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtre_01 = df_not_conclusions[df_not_conclusions['doc_id'].str.contains('0000')]\n",
    "filtre_02 = df_with_conclusions[df_with_conclusions['doc_id'].str.contains('0000')]\n",
    "\n",
    "display(filtre_01)\n",
    "display(filtre_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_conclusions = df_with_conclusions[~df_with_conclusions['key'].isin(df_not_conclusions['key'])].reset_index(drop=True)\n",
    "only_conclusions = only_conclusions.drop('key', axis=1)\n",
    "\n",
    "filtre = only_conclusions[~only_conclusions['sec_title'].str.contains('conclusion', case=False, na=False)]\n",
    "only_conclusions.to_clipboard()\n",
    "display(only_conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_conclusions.to_parquet(\"01_Extract_Information_Data/03_Only_Conclusions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OPTIONAL PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw_1.4')\n",
    "\n",
    "# Load spaCy (without parser nor ner) & stopwords\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])       # Tokenizer \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess text BASIC FIRST OPTION\n",
    "def preprocess_text(text):\n",
    "    # Normalization\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    doc= nlp(text)\n",
    "\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.lemma_ not in stop_words and token.is_alpha and len(token.lemma_) > 2\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the documents\n",
    "DIR_TO_CHANGE = \"03_Embeddings/Embeddings_SD_02.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_parquet(DIR_TO_CHANGE)\n",
    "titles = docs[\"chunk_title\"]\n",
    "print(titles.iloc[8])\n",
    "\n",
    "titles_clean = [preprocess_text(title) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(titles_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_clean = pd.DataFrame(titles_clean)\n",
    "\n",
    "titles_clean = titles_clean.rename(columns={0: \"title\"})\n",
    "print(f\"len titles: {len(titles_clean)} & len docs: {len(docs)}\")\n",
    "print(titles_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range (0, len(docs)):\n",
    "    rows.append({\n",
    "        \"doc_id\"        : docs[\"doc_id\"][i],\n",
    "        \"sec_order\"     : docs[\"sec_order\"][i],\n",
    "        \"sec_title\"     : titles_clean[\"title\"][i],\n",
    "        \"chunk_id_int\"  : docs[\"chunk_id_int\"][i],\n",
    "        \"chunk_id\"      : docs[\"chunk_id\"][i],\n",
    "        \"chunk_text\"    : docs[\"chunk_text\"][i],\n",
    "        #\"sec_text_clean\": docs_clean[\"text\"][i],\n",
    "        \"n_words\"       : docs[\"n_words\"][i],\n",
    "        \"n_chars\"       : docs[\"n_chars\"][i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_title_clean = pd.DataFrame(rows)\n",
    "display(doc_title_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_title_clean.to_parquet(DIR_TO_CHANGE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 For text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = pd.read_parquet(\"01_Extract_Information_Data/02_Clean_Sections_SD_02.parquet\")[\"sec_text\"]\n",
    "print(Texts.iloc[8])\n",
    "\n",
    "Texts_Clean = [preprocess_text(Text) for Text in Texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_parquet(\"01_Extract_Information_Data/02_Clean_Sections_SD_02.parquet\")\n",
    "\n",
    "rows = []\n",
    "for i in range (0, len(docs)):\n",
    "    rows.append({\n",
    "        \"doc_id\"        : docs[\"doc_id\"][i],\n",
    "        \"sec_order\"     : docs[\"sec_order\"][i],\n",
    "        #\"label\"         : docs[\"label\"][i],\n",
    "        \"sec_title\"     : docs[\"sec_title\"][i],\n",
    "        \"sec_text\"      : Texts_Clean[i],\n",
    "        \"n_words\"       : docs[\"n_words\"][i],\n",
    "        \"n_chars\"       : docs[\"n_chars\"][i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs_Lema = pd.DataFrame(rows)\n",
    "display(Docs_Lema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs_Lema.to_parquet(\"01_Extract_Information_Data/02_Clean_Sections_SD_Lema.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CHUNKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 FONCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Splits the text into sentences\n",
    "\n",
    "import spacy\n",
    "_SPACY = spacy.blank(\"en\")\n",
    "if \"sentencizer\" not in _SPACY.pipe_names:\n",
    "    _SPACY.add_pipe('sentencizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences (text: str) -> list[str]:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    doc = _SPACY (text)\n",
    "    return [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    \n",
    "def _count_tokens_approx (s: str) -> int:\n",
    "    return len(re.findall(r\"\\S+\", s))\n",
    "\n",
    "def _symbol_ratio (s: str) -> float:\n",
    "    if not s: return 1.0\n",
    "    total = len(s)\n",
    "    alnum = sum(ch.isalnum() or ch.isspace() for ch in s)\n",
    "    return 1.0 - (alnum/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunky_by_sentences (\n",
    "        text            : str,\n",
    "        target_tokens   : int= 400,\n",
    "        max_tokens      : int= 520,\n",
    "        min_tokens      : int= 200,\n",
    "        overlap_sents   : int= 2,\n",
    "        max_sents_sect  : int | None = None,\n",
    "        max_chars_chunk: int | None = None,\n",
    "        ensure_scientific_context : bool = True \n",
    "        ) -> list[str]:\n",
    "    \n",
    "    # Additional filtre for scientific texts\n",
    "\n",
    "    sents = split_sentences(text)\n",
    "    if not sents:\n",
    "        return []\n",
    "    \n",
    "    if ensure_scientific_context:\n",
    "        scientific_keywords = ['results', 'method', 'conclusion', 'abstract', \n",
    "                              'introduction', 'data', 'analysis', 'study']\n",
    "        sents = [s for s in sents if any(keyword in s.lower() for keyword in scientific_keywords) or len(s.split()) > 5]\n",
    "           \n",
    "    if isinstance(max_sents_sect, int) and max_sents_sect > 0:\n",
    "        sents = sents[:max_sents_sect]\n",
    "    \n",
    "    chunks  : list[str] = []\n",
    "    cur     : list[str] = []\n",
    "    cur_tok = 0\n",
    "\n",
    "    def emit_chunk():\n",
    "        nonlocal cur, cur_tok, chunks\n",
    "        if cur:\n",
    "            chunk = \" \".join(cur).strip()\n",
    "            if chunk:\n",
    "                if isinstance (max_chars_chunk, int) and max_chars_chunk > 0 and len(chunk) > max_chars_chunk:\n",
    "                    chunk = chunk[:max_chars_chunk].rsplit(\" \", 1)[0]\n",
    "                chunks.append(chunk)\n",
    "        cur = []\n",
    "        cur_tok = 0\n",
    "   \n",
    "    i = 0\n",
    "    retry_for_same_sentence = 0\n",
    "    while i < len(sents):\n",
    "        sent = sents[i]\n",
    "        t = _count_tokens_approx(sent)\n",
    "\n",
    "        # for sentences bigger than max_tokens\n",
    "        if t > max_tokens:\n",
    "            if cur_tok >= min_tokens:\n",
    "                emit_chunk()\n",
    "            chunks.append(sent.strip())\n",
    "            i += 1\n",
    "            retry_for_same_sentence = 0\n",
    "            continue\n",
    "\n",
    "        # For normal sentences\n",
    "        if cur_tok + t <= max_tokens:\n",
    "            cur.append(sent)\n",
    "            cur_tok += t\n",
    "\n",
    "            # target ok\n",
    "            if  cur_tok >= target_tokens:\n",
    "                back = cur[-overlap_sents:] if (overlap_sents > 0 and len(cur) >= overlap_sents) else []\n",
    "                emit_chunk()\n",
    "                if back:\n",
    "                    cur = back [:]\n",
    "                    cur_tok = sum(_count_tokens_approx(x) for x in cur)\n",
    "            i += 1\n",
    "            retry_for_same_sentence = 0\n",
    "            continue\n",
    "\n",
    "        # For long sentences            \n",
    "        if cur_tok >= min_tokens:\n",
    "            back = cur[-overlap_sents:] if (overlap_sents > 0 and len(cur) >= overlap_sents) else []\n",
    "            emit_chunk()\n",
    "            if back:\n",
    "                cur = back [:]\n",
    "                cur_tok = sum(_count_tokens_approx(x) for x in cur)\n",
    "            if retry_for_same_sentence == 0:\n",
    "                retry_for_same_sentence = 1\n",
    "                continue\n",
    "            else:\n",
    "                i += 1\n",
    "                retry_for_same_sentence = 0\n",
    "                continue\n",
    "        else:\n",
    "            cur.append(sent)\n",
    "            cur_tok += t\n",
    "            emit_chunk()\n",
    "            i += 1\n",
    "            retry_for_same_sentence = 0\n",
    "    \n",
    "    # Final flush\n",
    "    if cur:\n",
    "        if chunks and _count_tokens_approx(\" \".join(cur)) < min_tokens:\n",
    "            # fusion with the last one\n",
    "            last = chunks.pop()\n",
    "            chunks.append((last + \" \" + \" \".join(cur)).strip())\n",
    "        else:\n",
    "            emit_chunk()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "SECS_PARQUET     = \"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\"\n",
    "CHUNKS_PARQUET   = \"01_Extract_Information_Data/All_With_Conclusions_SD_Chunks_RAW.parquet\"\n",
    "TARGET           = 320\n",
    "MAXTOK           = 448\n",
    "MINTOK           = 192\n",
    "OVERLAP          = 2\n",
    "MIN_WORDS_CHUNK  = 15\n",
    "MAX_SYMBOL_RATIO = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunks_from_sections_vectorized():\n",
    "    df = pd.read_parquet(SECS_PARQUET)\n",
    "    \n",
    "    # Ordenar y preparar\n",
    "    df = df.sort_values([\"doc_id\", \"label\", \"sec_order\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Vectorizar operaciones\n",
    "    df[\"text_valid\"] = df[\"sec_text\"].fillna(\"\").str.strip().str.len() > 0\n",
    "    \n",
    "    # Procesar solo textos v√°lidos\n",
    "    valid_mask = df[\"text_valid\"]\n",
    "    valid_df = df[valid_mask].copy()\n",
    "    \n",
    "    # Aplicar chunky_by_sentences a todos los textos v√°lidos\n",
    "    tqdm.pandas(desc=\"Chunking sections\")\n",
    "    valid_df[\"chunks\"] = valid_df[\"sec_text\"].progress_apply(\n",
    "        lambda x: chunky_by_sentences(\n",
    "            text=x,\n",
    "            target_tokens=TARGET,\n",
    "            max_tokens=MAXTOK,\n",
    "            min_tokens=MINTOK,\n",
    "            overlap_sents=OVERLAP,\n",
    "            ensure_scientific_context=True\n",
    "        )\n",
    "    )\n",
    "    # Aplicar filtros de calidad de forma vectorizada\n",
    "    def filter_chunks(chunk_list):\n",
    "        if not chunk_list:\n",
    "            return []\n",
    "        filtered = []\n",
    "        for idx, ch in enumerate(chunk_list):\n",
    "            if len(ch.split()) < MIN_WORDS_CHUNK:\n",
    "                continue\n",
    "            if _symbol_ratio(ch) > MAX_SYMBOL_RATIO:\n",
    "                continue\n",
    "            filtered.append((idx, ch))\n",
    "        \n",
    "        if not filtered and chunk_list:\n",
    "            # Tomar el chunk m√°s largo\n",
    "            idx = max(range(len(chunk_list)), key=lambda i: len(chunk_list[i]))\n",
    "            filtered.append((idx, chunk_list[idx]))\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    valid_df[\"filtered_chunks\"] = valid_df[\"chunks\"].apply(filter_chunks)\n",
    "    \n",
    "    # Crear DataFrame expandido\n",
    "    rows = []\n",
    "    \n",
    "    # Procesar textos v√°lidos\n",
    "    for _, row in tqdm(valid_df.iterrows(), total=len(valid_df), desc=\"Expanding chunks\"):\n",
    "        doc_id = str(row[\"doc_id\"])\n",
    "        sec_order = int(row[\"sec_order\"])\n",
    "        label = str(row[\"label\"]) if pd.notna(row[\"label\"]) else \"\"\n",
    "        sec_title = str(row[\"sec_title\"]) if pd.notna(row[\"sec_title\"]) else \"\"\n",
    "        \n",
    "        if row[\"filtered_chunks\"]:\n",
    "            for j, ch in row[\"filtered_chunks\"]:\n",
    "                rows.append({\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"sec_order\": sec_order,\n",
    "                    \"label\": label,\n",
    "                    \"sec_title\": sec_title,\n",
    "                    \"chunk_id_int\": j,\n",
    "                    \"chunk_text\": ch,\n",
    "                    \"n_words\": len(ch.split()),\n",
    "                    \"n_chars\": len(ch)\n",
    "                })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"sec_order\": sec_order,\n",
    "                \"label\": label,\n",
    "                \"sec_title\": sec_title,\n",
    "                \"chunk_id_int\": 0,\n",
    "                \"chunk_text\": \"\",\n",
    "                \"n_words\": 0,\n",
    "                \"n_chars\": 0\n",
    "            })\n",
    "    \n",
    "    # Procesar textos no v√°lidos (m√°s r√°pido, sin chunks)\n",
    "    invalid_df = df[~valid_mask]\n",
    "    if len(invalid_df) > 0:\n",
    "        invalid_rows = [{\n",
    "            \"doc_id\": str(r[\"doc_id\"]),\n",
    "            \"sec_order\": int(r[\"sec_order\"]),\n",
    "            \"label\": str(r[\"label\"]) if pd.notna(r[\"label\"]) else \"\",\n",
    "            \"sec_title\": str(r[\"sec_title\"]) if pd.notna(r[\"sec_title\"]) else \"\",\n",
    "            \"chunk_id_int\": 0,\n",
    "            \"chunk_text\": \"\",\n",
    "            \"n_words\": 0,\n",
    "            \"n_chars\": 0\n",
    "        } for _, r in invalid_df.iterrows()]\n",
    "        rows.extend(invalid_rows)\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    out = pd.DataFrame(rows)\n",
    "    out = out.sort_values([\"doc_id\", \"sec_order\", \"chunk_id_int\"]).reset_index(drop=True)\n",
    "    out[\"chunk_id\"] = out.index.astype(\"int64\")\n",
    "    \n",
    "    # Guardar con PyArrow\n",
    "    table = pa.Table.from_pandas(out, preserve_index=False)\n",
    "    pq.write_table(table, CHUNKS_PARQUET, compression=\"zstd\")\n",
    "    print(f\"‚úÖ Saved: {CHUNKS_PARQUET}. ({len(out)} chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_chunks_from_sections_vectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_parquet(CHUNKS_PARQUET)\n",
    "\n",
    "terms_label   = ['√ò']\n",
    "pattern_label = '|'.join(terms_label)\n",
    "df  = df[~df['label'].str.contains(pattern_label, case=False, na=False)]\n",
    "\n",
    "terms_title   = [\n",
    "    'acknowledg', 'author contribution', 'credit author', \n",
    "    'competing interest', 'conflict of interest', 'ethical declaration',\n",
    "    'data availa', 'code availa', 'supplement', 'appendix',\n",
    "    'disclaim', 'disclosure', 'consent', 'permission']\n",
    "pattern_title = '|'.join(terms_title)\n",
    "df = df[~df['sec_title'].str.contains(pattern_title, case=False, na=False)]\n",
    "\n",
    "df = df[df['chunk_text'].str.len() > 0].reset_index(drop=True)\n",
    "df[\"chunk_id\"] = df.index.astype(\"int64\")\n",
    "cols_to_mov = ['chunk_id']\n",
    "order_new_cols = cols_to_mov + [col for col in df.columns if col not in cols_to_mov]\n",
    "df = df[order_new_cols]\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(CHUNKS_PARQUET)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. ABSTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract only SD abstracts\n",
    "\n",
    "df = pd.read_csv (\"02_AI_Topics_Models_Data/01_Old_Data/Data_20250912.csv\", sep=\";\")\n",
    "docs = df[df['SOURCE']=='SD'].reset_index()\n",
    "docs = docs[['DOI', 'YEAR', 'TITLE', 'COUNTRY', 'AB_AI']]\n",
    "\n",
    "abs = docs[\"AB_AI\"]\n",
    "AB_LEMMA = [preprocess_text(ab) for ab in abs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range (0, len(docs)):\n",
    "    rows.append({\n",
    "        \"doc_id\"        : [i],\n",
    "        \"DOI\"           : docs[\"DOI\"][i],\n",
    "        \"YEAR\"          : docs[\"YEAR\"][i],\n",
    "        \"TITLE\"         : docs[\"TITLE\"][i],\n",
    "        \"COUNTRY\"       : docs[\"COUNTRY\"][i],\n",
    "        \"ABSTRACT\"      : docs[\"AB_AI\"][i],\n",
    "        \"ABSTRACT_LEMA\" : AB_LEMMA[i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS_SD = pd.DataFrame(rows)\n",
    "ABSTRACTS_SD.to_parquet(\"01_Extract_Information_Data/02_Abstracts_SD.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Consolidation of ALL WITH CONCLUSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMAL DIR\n",
    "DIR_OLD_SECT        = \"01_Extract_Information_Data/02_Clean_Sections_SD_02.parquet\"\n",
    "DIR_NEW_SECT        = \"01_Extract_Information_Data/All_With_Conclusions_SD.parquet\"\n",
    "DIR_EMBEDDINGS      = \"03_Embeddings/Embeddings_SD_02.parquet\"\n",
    "DIR_OLD_CHUNKS      = \"01_Extract_Information_Data/02_Clean_Chunks_SD_02.parquet\"\n",
    "DIR_NEW_CHUNKS      = \"01_Extract_Information_Data/All_With_Conclusions_Chunks_SD.parquet\"\n",
    "DIR_CONS_CHUNKS     = \"01_Extract_Information_Data/All_With_Conclusions_Chunks_SD_02.parquet\"\n",
    "DIR_FOR_NEW_CHUNKS  = \"01_Extract_Information_Data/All_With_Conclusions_NEW_Chunks_SD.parquet\"\n",
    "DIR_FOR_NEW_EMBS    = \"03_Embeddings/Embeddings_NEW_Chunks.parquet\"\n",
    "DIR_CONS_EMB        = \"03_Embeddings/ALL_EMBEDDINGS_SD.parquet\"\n",
    "DIR_CONS_EMB_FILT   = \"03_Embeddings/Embeddings_All_Filtered_SD.parquet\"\n",
    "DIR_CONS_ENB_NaN    = \"03_Embeddings/Embeddings_All_Filtered_SD_nan.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMA DIR\n",
    "DIR_OLD_SECT        = \"01_Extract_Information_Data/02_Clean_Sections_SD_Lema.parquet\"\n",
    "DIR_NEW_SECT        = \"01_Extract_Information_Data/All_With_Conclusions_SD_Lema.parquet\"\n",
    "DIR_EMBEDDINGS      = \"03_Embeddings/Embeddings_SD_Lema_02.parquet\"\n",
    "DIR_OLD_CHUNKS      = \"01_Extract_Information_Data/02_Clean_Chunks_SD_Lema_02.parquet\"\n",
    "DIR_NEW_CHUNKS      = \"01_Extract_Information_Data/All_With_Conclusions_Chunks_SD_Lema.parquet\"\n",
    "DIR_CONS_CHUNKS     = \"01_Extract_Information_Data/All_With_Conclusions_Chunks_SD_Lema_02.parquet\"\n",
    "DIR_FOR_NEW_CHUNKS  = \"01_Extract_Information_Data/All_With_Conclusions_NEW_Chunks_SD_Lema.parquet\"\n",
    "DIR_FOR_NEW_EMBS    = \"03_Embeddings/Embeddings_NEW_Chunks_Lema.parquet\"\n",
    "DIR_CONS_EMB         = \"03_Embeddings/ALL_EMBEDDINGS_SD_Lema.parquet\"\n",
    "DIR_CONS_EMB_FILT   = \"03_Embeddings/Embeddings_All_Filtered_SD_Lema.parquet\"\n",
    "DIR_CONS_ENB_NaN    = \"03_Embeddings/Embeddings_All_Filtered_SD_nan_Lema.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous embeddings \n",
    "old_chunks = pd.read_parquet(DIR_OLD_CHUNKS)\n",
    "#filtre = old_chunks[old_chunks[\"sec_title\"].isna()]\n",
    "filtre = old_chunks[old_chunks[\"doc_id\"].str.contains(\"0000\")]\n",
    "display (filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous embeddings \n",
    "old_emb = pd.read_parquet(DIR_EMBEDDINGS)\n",
    "filtre = old_emb[old_emb[\"doc_id\"].str.contains(\"0000\")]\n",
    "display (filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate old embeddings\n",
    "old_emb_upgrade = []\n",
    "for i in range (0, len(old_emb)):\n",
    "\n",
    "    old_emb_upgrade.append({\n",
    "        \"doc_id\"        : old_chunks[\"doc_id\"][i],\n",
    "        \"sec_order\"     : old_chunks[\"sec_order\"][i],\n",
    "        \"sec_title\"     : old_emb[\"chunk_title\"][i],\n",
    "        \"chunk_id_int\"  : old_chunks[\"chunk_id_int\"][i],\n",
    "        \"chunk_id\"      : old_chunks[\"chunk_id\"][i],\n",
    "        \"chunk_text\"    : old_chunks[\"chunk_text\"][i],\n",
    "        \"n_words\"       : old_chunks[\"n_words\"][i],\n",
    "        \"n_chars\"       : old_chunks[\"n_chars\"][i],\n",
    "        \"chunk_emb\"     : old_emb[\"chunk_emb\"][i]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_emb_upgrade = pd.DataFrame(old_emb_upgrade)\n",
    "old_emb_upgrade[\"key\"] = old_emb_upgrade[\"doc_id\"] + '-' + old_emb_upgrade[\"sec_title\"] + '-' + old_emb_upgrade[\"n_words\"].astype(str) + '-' + old_emb_upgrade[\"n_chars\"].astype(str)\n",
    "#filtre = old_emb_upgrade[old_emb_upgrade[\"sec_title\"].isna()]\n",
    "filtre = old_emb_upgrade[old_emb_upgrade[\"doc_id\"].str.contains(\"0000\")]\n",
    "\n",
    "display(filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new chunks\n",
    "new_chunks = pd.read_parquet(DIR_NEW_CHUNKS)\n",
    "new_chunks[\"key\"] = new_chunks[\"doc_id\"] + '-' + new_chunks[\"sec_title\"] + '-' + new_chunks[\"n_words\"].astype(str) + '-' + new_chunks[\"n_chars\"].astype(str)\n",
    "#filtre = new_chunks[new_chunks[\"sec_title\"].isna()]\n",
    "filtre = new_chunks[new_chunks[\"doc_id\"].str.contains(\"0000\")]\n",
    "display (filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = old_emb_upgrade[[\"key\", \"chunk_emb\"]]\n",
    "emb = emb.drop_duplicates(subset='key')\n",
    "dup = emb[emb.duplicated('key', keep=False)]\n",
    "\n",
    "print(f\"Keys duplicadas en df_01: {len(dup)}\")\n",
    "display (emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_emb_upgrade['key'].iloc[0] == new_chunks['key'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For new embeddings\n",
    "\n",
    "new_emb = new_chunks\n",
    "new_emb['chunk_emb'] = new_emb['key'].map(emb.set_index(\"key\")[\"chunk_emb\"])\n",
    "\n",
    "filtre = new_emb[new_emb['doc_id'].str.contains('0000')]\n",
    "#filtre = new_emb[new_emb['chunk_emb'].isna()]\n",
    "\n",
    "display(filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emb.to_parquet(DIR_CONS_CHUNKS)\n",
    "filtre.to_parquet(DIR_FOR_NEW_CHUNKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Embeddings for new texts!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ========================================================================\n",
    "# ==                                                                    ==\n",
    "# ==    ALL DOCUMENTS HAVE BEEN EMBEDDED IN \"PapersWithOpenAI.ipynb\"    ==\n",
    "# ==                                                                    ==\n",
    "# ========================================================================\n",
    "# ========================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Embeddings integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_chunks = pd.read_parquet(DIR_CONS_CHUNKS)\n",
    "cons_chunks['key'] = cons_chunks[\"doc_id\"] + '-' + cons_chunks[\"sec_title\"] + '-' + cons_chunks[\"chunk_text\"].astype(str)\n",
    "display(cons_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embs = pd.read_parquet(DIR_FOR_NEW_EMBS)\n",
    "filtre = new_embs[new_embs[\"doc_id\"].str.contains('0029')]\n",
    "new_embs['key'] = new_embs[\"doc_id\"] + '-' + new_embs[\"chunk_title\"] + '-' + new_embs[\"chunk_text\"].astype(str)\n",
    "new_embs = new_embs[[\"key\", \"chunk_emb\"]]\n",
    "display(new_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embs = new_embs.drop_duplicates(subset='key')\n",
    "dup = new_embs[new_embs.duplicated('key', keep=False)]\n",
    "\n",
    "print(f\"Keys duplicadas en df_01: {len(dup)}\")\n",
    "display (dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For new embeddings\n",
    "cons_embs = cons_chunks\n",
    "cons_embs_nan = cons_embs['chunk_emb'].isna()\n",
    "display(cons_embs_nan)\n",
    "\n",
    "new_embs = new_embs.set_index('key')['chunk_emb']\n",
    "display(new_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_embs.loc[cons_embs_nan, 'chunk_emb'] = cons_embs.loc[cons_embs_nan, 'key'].map(new_embs)\n",
    "cons_embs = cons_embs.drop(['key'], axis=1)\n",
    "filtre = cons_embs[cons_embs['doc_id'].str.contains('0000')]\n",
    "\n",
    "display(filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_embs.to_parquet(DIR_CONS_EMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 FILTRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = pd.read_parquet(DIR_NEW_SECT)\n",
    "detail = detail[['doc_id','sec_title', 'label', 'sec_text']]\n",
    "#display (detail)\n",
    "detail.to_csv(\"All_With_Conclusions_SD.csv\", sep=';', encoding='utf-8')\n",
    "\n",
    "terms_label   = ['theorem','remark','proposition','proof','problem','lemma','√ò','hyperlink','definition','corollary','configuration','assumption','algorithm']\n",
    "pattern_label = '|'.join(terms_label)\n",
    "filtre_label  = detail[~detail['label'].str.contains(pattern_label, case=False, na=False)]\n",
    "\n",
    "filtre_label = filtre_label[['doc_id','sec_title', 'label', 'sec_text']]\n",
    "filtre_label.to_csv(\"All_With_Conclusions_SD_Filtre_01.csv\", sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = detail[detail['doc_id'].str.contains(\"3423\")]\n",
    "display(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Filtre Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_CONS_EMB = \"03_Embeddings/Embeddings_OpenAI.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_embs = pd.read_parquet(DIR_CONS_EMB)\n",
    "display(cons_embs)\n",
    "\n",
    "terms_label   = ['theorem','remark','proposition','proof','problem','lemma','√ò','hyperlink','definition','corollary','configuration','assumption','algorithm']\n",
    "pattern_label = '|'.join(terms_label)\n",
    "filtre_label  = cons_embs[~cons_embs['label'].str.contains(pattern_label, case=False, na=False)]\n",
    "\n",
    "terms_title   = ['acknowledg','acronym','abbrevia','algorithm','author','code availa','computational study','computational time', 'confidential',\n",
    "                 'conflict','consent','credit author','author credit','declaration of','declarations of','disclaim', 'disclosure','ethics approval','ethical approval',\n",
    "                 'ethics state','ethics declaration',]\n",
    "pattern_title = '|'.join(terms_title)\n",
    "filtre_title = filtre_label[~filtre_label['chunk_title'].str.contains(pattern_title, case=False, na=False)]\n",
    "\n",
    "filtre_nan = filtre_title[~filtre_title['chunk_emb'].isna()]\n",
    "filtre_empty = filtre_nan[filtre_nan['chunk_text'].str.len() > 0].reset_index(drop=True)\n",
    "#1 = filtre_empty[filtre_empty[\"doc_id\"].str.contains(\"0000\")]\n",
    "\n",
    "display(filtre_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtre_empty.to_parquet(\"03_Embeddings/Embeddings_OpenAI_filtered.parquet\")\n",
    "\n",
    "df = pd.read_parquet(\"03_Embeddings/Embeddings_OpenAI_filtered.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = filtre_nan[filtre_nan['doc_id'].str.contains(\"0000\")]\n",
    "display(det)\n",
    "\n",
    "#display (filtre_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVISION DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01 = pd.read_parquet(\"03_Embeddings/Embeddings_NEW_Chunks_Lema_Long.parquet\")\n",
    "f1 = df_01[df_01[\"doc_id\"].str.contains(\"0000\")]\n",
    "\n",
    "display(f1)\n",
    "display(df_01[\"chunk_text\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02 = pd.read_parquet(\"01_Extract_Information_Data/All_With_Conclusions_Chunks_SD.parquet\")\n",
    "f2 = df_02[df_02['doc_id'].str.contains('0000')]\n",
    "\n",
    "display(f2)\n",
    "display(df_02[\"chunk_text\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01[\"label\"] = df_02[\"label\"]\n",
    "df_01[\"chunk_text\"] = df_02[\"chunk_text\"]\n",
    "df_01 = df_01[[\"doc_id\", \"chunk_int_id\", \"chunk_id\", \"label\", \"chunk_title\", \"chunk_text\", \"chunk_emb\"]]\n",
    "\n",
    "f1 = df_01[df_01[\"doc_id\"].str.contains(\"0000\")]\n",
    "display(f1)\n",
    "display(df_01[\"chunk_text\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01.to_parquet(\"03_Embeddings/Embeddings_Ver.04.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_Extract_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
