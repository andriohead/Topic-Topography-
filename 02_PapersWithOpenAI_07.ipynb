{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC EXTRACTION FOR TRANSPORT ELECTRIFICATION PAPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, jupyter_client, ipykernel, zmq\n",
    "print(sys.executable)\n",
    "print(\"jc\", jupyter_client.__version__, \"ik\", ipykernel.__version__, \"zmq\", zmq.pyzmq_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI_TOPICS (Python 3.11.11)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import nbformat\n",
    "import ast\n",
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from packaging import version\n",
    "import contextlib\n",
    "import io\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import igraph as ig\n",
    "import unicodedata\n",
    "import plotly.io as pio\n",
    "\n",
    "# Processing data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import os, glob, re, unicodedata\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import sklearn \n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from tqdm import tqdm  # Para mostrar barra de progreso\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#from umap import UMAP\n",
    "\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.models import CoherenceModel                        # Coherence model\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import f_oneway, pearsonr\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "import bertopic\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.backend import BaseEmbedder\n",
    "from bertopic.cluster import BaseCluster\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "import hdbscan\n",
    "\n",
    "import math\n",
    "\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "# Embedding coherence\n",
    "from itertools import combinations\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "\n",
    "#NEW EMBEDDINGS\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load AI configuration\n",
    "con_file = open(\"02_AI_Topics_Models_Data/key_OpenAI.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()\n",
    "\n",
    "## Initialize client\n",
    "client = openai.OpenAI(api_key=config['apikey'])\n",
    "\n",
    "# First test\n",
    "models = client.models.list()\n",
    "\n",
    "for model in models:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pretraining & Embeddings - Full_Texts and SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_for_scibert (text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text_pp = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text_pp = text_pp.replace(\"\\u00A0\", \" \")            # Spaces no breakables\n",
    "    text_pp = re.sub(r'[ \\t]+', ' ', text_pp)           # Multiple spaces\n",
    "    text_pp = re.sub(r'\\s*\\n+\\s*', ' ', text_pp)        # Ligne jumps\n",
    "    # text_pp = re.sub(r'[^\\x00-\\x7F]+', ' ', text_pp)\n",
    "\n",
    "    return text_pp.strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implementation: Embeddings for full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 SCI BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def generate_embeddings_optimized (df, model_name='allenai/scibert_scivocab_uncased', \n",
    "                                      output_file=\"embeddings.parquet\"):\n",
    "    \"\"\"\n",
    "    Configuraci√≥n OPTIMIZADA para:\n",
    "    - 16GB RAM total (‚âà12GB disponibles)\n",
    "    - Intel Iris (GPU integrada, poca memoria)\n",
    "    - Intel i7 (4 cores)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. FORZAR CPU - La GPU integrada es m√°s lenta para transformers\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö° Usando CPU (GPU integrada no acelerar√° SciBERT)\")\n",
    "    \n",
    "    # 2. Cargar modelo con optimizaciones CPU\n",
    "    model = SentenceTransformer(\n",
    "        model_name, \n",
    "        device=device,\n",
    "        # Optimizaciones para CPU\n",
    "        model_kwargs={'torch_dtype': torch.float32}  # Usar float32, no float16\n",
    "    )\n",
    "    \n",
    "    # 3. Batch size MUCHO m√°s peque√±o\n",
    "    # Estimaci√≥n: cada embedding ~3KB * batch_size + overhead\n",
    "    # Con 12GB disponibles, m√°ximo seguro:\n",
    "    SAFE_BATCH_SIZE = 64  # ¬°Reducido dram√°ticamente!\n",
    "    print(f\"üì¶ Batch size seguro: {SAFE_BATCH_SIZE} (por memoria limitada)\")\n",
    "    \n",
    "    # 4. Preprocesar textos (usa tu funci√≥n)\n",
    "    texts = df[\"chunk_text\"].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    # 5. Procesar con gesti√≥n AGGRESIVA de memoria\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), SAFE_BATCH_SIZE), \n",
    "                  desc=\"Generando embeddings\", \n",
    "                  unit=\"batch\"):\n",
    "        \n",
    "        batch_texts = texts[i:i + SAFE_BATCH_SIZE]\n",
    "        \n",
    "        # Preprocesamiento del batch\n",
    "        processed_batch = [preprocess_text_for_scibert(t) for t in batch_texts]\n",
    "        \n",
    "        # Generar embeddings con batch interno PEQUE√ëO\n",
    "        try:\n",
    "            batch_emb = model.encode(\n",
    "                processed_batch,\n",
    "                batch_size=4,  # Batch interno MUY peque√±o\n",
    "                show_progress_bar=False,\n",
    "                convert_to_tensor=False,  # No usar tensor para ahorrar memoria\n",
    "                normalize_embeddings=False,\n",
    "                output_value=\"sentence_embedding\"\n",
    "            )\n",
    "            \n",
    "            all_embeddings.append(batch_emb.astype(\"float32\"))\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"‚ö†Ô∏è  Memoria insuficiente en batch {i}. Reduciendo...\")\n",
    "                # Procesar de a un texto\n",
    "                for j, text in enumerate(processed_batch):\n",
    "                    try:\n",
    "                        single_emb = model.encode(\n",
    "                            text,\n",
    "                            show_progress_bar=False,\n",
    "                            normalize_embeddings=False\n",
    "                        )\n",
    "                        all_embeddings.append(single_emb.reshape(1, -1).astype(\"float32\"))\n",
    "                    except:\n",
    "                        # Vector cero como fallback\n",
    "                        all_embeddings.append(np.zeros((1, 768), dtype=\"float32\"))\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # Limpiar memoria CADA batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # 6. Concatenar\n",
    "    final_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # 6. Crear DataFrame CON embeddings incluidos\n",
    "    print(\"üìä Creando DataFrame final...\")\n",
    "    \n",
    "    # M√©todo 1: Lista de arrays numpy (recomendado)\n",
    "    embeddings_list = [emb for emb in final_embeddings]\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        \"doc_id\": df[\"doc_id\"].values,\n",
    "        \"sec_order\": df[\"sec_order\"].values,\n",
    "        \"label\": df[\"label\"].values,\n",
    "        \"sec_title\": df[\"sec_title\"].values,\n",
    "        \"chunk_id\": df[\"chunk_id\"].values,\n",
    "        \"chunk_text\": df[\"chunk_text\"].values,\n",
    "        \"chunk_emb\": embeddings_list  # ‚Üê CORRECTO\n",
    "    })\n",
    "    \n",
    "    # 7. Verificar\n",
    "    print(f\"‚úÖ DataFrame creado: {len(result_df)} filas\")\n",
    "    print(f\"   - Primera embedding shape: {result_df['chunk_emb'].iloc[0].shape}\")\n",
    "    print(f\"   - Dimensi√≥n embeddings: {result_df['chunk_emb'].iloc[0].shape[0]}\")\n",
    "    \n",
    "    # 8. Guardar\n",
    "    result_df.to_parquet(output_file, compression='zstd')\n",
    "    print(f\"üíæ Guardado: {output_file}\")\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS_FILE = \"01_Extract_Information_Data/All_With_Conclusions_SD_Chunks_RAW.parquet\"\n",
    "OUTPUT_FILE = \"03_Embeddings/Embeddings_RAW.parquet\"\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Usar tus 4 cores\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(CHUNKS_FILE)\n",
    "embeddings_df = generate_embeddings_optimized(\n",
    "    df, \n",
    "    output_file=OUTPUT_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS_FILE = \"01_Extract_Information_Data/All_With_Conclusions_SD_Chunks_HARD.parquet\"\n",
    "OUTPUT_FILE = \"03_Embeddings/Embeddings_HARD.parquet\"\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Usar tus 4 cores\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "\n",
    "df = pd.read_parquet(CHUNKS_FILE)\n",
    "embeddings_hard = generate_embeddings_optimized(\n",
    "    df, \n",
    "    output_file=OUTPUT_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS_FILE = \"01_Extract_Information_Data/All_With_Conclusions_SD_Chunks_SOFT.parquet\"\n",
    "OUTPUT_FILE = \"03_Embeddings/Embeddings_SOFT.parquet\"\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'  # Usar tus 4 cores\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "\n",
    "df = pd.read_parquet(CHUNKS_FILE)\n",
    "embeddings_hard = generate_embeddings_optimized(\n",
    "    df, \n",
    "    output_file=OUTPUT_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 OPEN AI \"EMBEDDING LARGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FILE = \"emb_checkpoint.pkl\"\n",
    "OUTPUT_FILE = \"03_Embeddings/Embeddings_OpenAI.parquet\"\n",
    "TEXT_COL    = \"chunk_text\"\n",
    "ID_COLS     = [\"doc_id\", \"label\", \"chunk_id_int\", \"chunk_id\", \"sec_title\"]\n",
    "BATCH_SIZE  = 32  # Tama√±o de lote reducido para mejor manejo de memoria\n",
    "CHECKPOINT_INTERVAL = 1000\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, 'rb') as f:\n",
    "        rows_openAI = pickle.load(f)\n",
    "    print (f\"Checkpoint loaded: {len(rows_openAI)} lines processed\")\n",
    "    start_idx = len(rows_openAI)\n",
    "else:\n",
    "    rows_openAI = []\n",
    "    start_idx = 0\n",
    "\n",
    "print (f\"Processing from {start_idx}, until {len(df)}\")\n",
    "print(f\"üìä Size of batch: {BATCH_SIZE} documents\")\n",
    "\n",
    "def get_openai_embeddings (texts, model=\"text-embedding-3-large\"):\n",
    "    resp = client.embeddings.create(\n",
    "        model = model,\n",
    "        input = texts\n",
    "    )\n",
    "    embs = np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "    return embs\n",
    "\n",
    "for i in range(start_idx, len(df), BATCH_SIZE):\n",
    "    end_idx = min(i + BATCH_SIZE, len(df))\n",
    "    batch_indices = range(i, end_idx)\n",
    "    \n",
    "    # Preparar textos del lote\n",
    "    batch_texts = []\n",
    "    batch_metadata = []\n",
    "    \n",
    "    for idx in batch_indices:\n",
    "        text = df[TEXT_COL].iloc[idx]\n",
    "        processed_text = preprocess_text_for_scibert(text)\n",
    "\n",
    "        # If chunk is empty\n",
    "        if not processed_text:\n",
    "            processed_text = \"[EMPTY_CHUNK]\"\n",
    "\n",
    "        batch_texts.append(processed_text)\n",
    "        \n",
    "        # Guardar metadatos\n",
    "        batch_metadata.append({\n",
    "            \"doc_id\"        : df[ID_COLS[0]].iloc[idx],\n",
    "            \"label\"         : df[ID_COLS[1]].iloc[idx],\n",
    "            \"chunk_int_id\"  : df[ID_COLS[2]].iloc[idx],\n",
    "            \"chunk_id\"      : df[ID_COLS[3]].iloc[idx],\n",
    "            \"sec_title\"     : df[ID_COLS[4]].iloc[idx],\n",
    "            \"original_text\" : text\n",
    "        })\n",
    "    \n",
    "    # Generar embeddings por lote (MUCHO m√°s eficiente)\n",
    "    try:\n",
    "        batch_embeddings = get_openai_embeddings(batch_texts)\n",
    "\n",
    "        # A√±adir resultados\n",
    "        for metadata, emb in zip(batch_metadata, batch_embeddings):\n",
    "            rows_openAI.append({\n",
    "                \"doc_id\"        : metadata[\"doc_id\"],\n",
    "                \"label\"         : metadata[\"label\"],\n",
    "                \"chunk_int_id\"  : metadata[\"chunk_int_id\"],\n",
    "                \"chunk_id\"      : metadata[\"chunk_id\"],\n",
    "                \"chunk_title\"   : metadata[\"sec_title\"],\n",
    "                \"chunk_text\"    : metadata[\"original_text\"],\n",
    "                \"chunk_emb\"     : emb\n",
    "            })\n",
    "        \n",
    "        print(f\"‚úÖ Batch {i//BATCH_SIZE + 1} processed: index {i}-{end_idx-1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in batch {i}-{end_idx-1}: {e}\")\n",
    "        # Procesar individualmente como fallback\n",
    "        for idx in batch_indices:\n",
    "            try:\n",
    "                text = df[TEXT_COL].iloc[idx]\n",
    "                processed_text = preprocess_text_for_scibert(text)\n",
    "                \n",
    "                emb = get_openai_embeddings([processed_text])[0]\n",
    "                \n",
    "                rows_openAI.append({\n",
    "                    \"doc_id\"        : df[ID_COLS[0]].iloc[idx],\n",
    "                    \"label\"         : df[ID_COLS[1]].iloc[idx],\n",
    "                    \"chunk_int_id\"  : df[ID_COLS[2]].iloc[idx],\n",
    "                    \"chunk_id\"      : df[ID_COLS[3]].iloc[idx],\n",
    "                    \"sec_title\"     : df[ID_COLS[4]].iloc[idx],\n",
    "                    \"chunk_text\"    : text,\n",
    "                    \"chunk_emb\"     : emb\n",
    "                })\n",
    "                print(f\"‚úÖ Text {idx} individually processed\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Critic error of text {idx}: {e2}\")\n",
    "                # A√±adir fila vac√≠a para mantener consistencia\n",
    "                rows_openAI.append({\n",
    "                    \"doc_id\"        : df[ID_COLS[0]].iloc[idx],\n",
    "                    \"label\"         : df[ID_COLS[1]].iloc[idx],\n",
    "                    \"chunk_int_id\"  : df[ID_COLS[2]].iloc[idx],\n",
    "                    \"chunk_id\"      : df[ID_COLS[3]].iloc[idx],\n",
    "                    \"sec_title\"     : df[ID_COLS[4]].iloc[idx],\n",
    "                    \"chunk_text\"    : \"ERROR_IN_PROCESSING\",\n",
    "                    \"chunk_emb\"     : np.zeros(768, dtype=\"float32\")  # Vector cero como fallback\n",
    "                })\n",
    "    \n",
    "    # Guardar checkpoint cada CHECKPOINT_INTERVAL\n",
    "    if len(rows_openAI) % CHECKPOINT_INTERVAL == 0:\n",
    "        with open(CHECKPOINT_FILE, 'wb') as f:\n",
    "            pickle.dump(rows_openAI, f)\n",
    "        print(f\"üíæ Checkpoint saved: {len(rows_openAI)} embeddings\")\n",
    "\n",
    "# Guardar resultados finales\n",
    "print(\"üéâ Pre-processing finiched\")\n",
    "print(f\"üìà Total generated embeddings: {len(rows_openAI)}\")\n",
    "\n",
    "# Limpiar checkpoint\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(\"üßπ Checkpoint eliminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings = pd.DataFrame(rows_openAI)\n",
    "df_embeddings.to_parquet(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"01_Extract_Information_Data/02_Clean_Sections_SD_02.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtre = df('doc_id')\n",
    "filtre = df[df['doc_id'].str.contains('0011', na=False)]\n",
    "display(filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['chunk_text'].iloc[441])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles =  pd.read_csv(\"01_Extract_Information_Data/Clean_Titles_02.csv\")\n",
    "Titles = Titles[[\"0\"]]\n",
    "Titles = Titles.rename(columns={'0':'Title'})\n",
    "\n",
    "#display(Titles)\n",
    "\n",
    "#Titles = Titles.drop_duplicates(keep=\"first\").sort_values(\"Title\")\n",
    "#print (Titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Titles.value_counts().reset_index()\n",
    "freq.columns = ['Title', 'Frequency']\n",
    "\n",
    "freq['Frequency'] = pd.to_numeric(freq['Frequency'])\n",
    "freq['Title'] = freq['Title'].astype(str).str.strip().sort_values()\n",
    "\n",
    "total_Title = freq.shape[0]\n",
    "total_ocurrences = int(freq['Frequency'].sum())\n",
    "\n",
    "\n",
    "print(total_Title)\n",
    "print(total_ocurrences)\n",
    "\n",
    "freq = freq.sort_values('Frequency',ascending=False)\n",
    "print (freq)\n",
    "\n",
    "freq.to_clipboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementation: Embeddings for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"01_Extract_Information_Data/02_Abstracts_SD.parquet\")\n",
    "print(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPPOINT_FILE = \"03_Embeddings/Emb_AB_check.pkl\"\n",
    "TEXT_COL    = \"ABSTRACT\"\n",
    "ID_COLS     = [\"doc_id\",\"DOI\", \"YEAR\", \"TITLE\", \"COUNTRY\"]\n",
    "\n",
    "if os.path.exists(CHECKPPOINT_FILE):\n",
    "    with open(CHECKPPOINT_FILE, 'rb') as f:\n",
    "        rows = pickle.load(f)\n",
    "    print (f\"Checkpoint loaded: {len(rows)} lines processed\")\n",
    "    start_idx = len(rows)\n",
    "else:\n",
    "    rows = []\n",
    "    start_idx = 0\n",
    "\n",
    "print (f\"Processing from {start_idx}, until {len(df)}\")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(start_idx, len(df)):\n",
    "    text = preprocess_text_for_scibert(df[TEXT_COL][i])\n",
    "    embeddings = model.encode(\n",
    "        text,\n",
    "        show_progress_bar = True,\n",
    "        convert_to_numpy = True,\n",
    "        normalize_embeddings = True\n",
    "        ).astype(\"float32\")\n",
    "        \n",
    "    rows.append({\n",
    "        \"doc_id\"    : df[ID_COLS[0]].iloc[i],\n",
    "        \"DOI\"       : df[ID_COLS[1]].iloc[i],\n",
    "        \"YEAR\"      : df[ID_COLS[2]].iloc[i],\n",
    "        \"TITLE\"     : df[ID_COLS[3]].iloc[i],\n",
    "        \"COUNTRY\"   : df[ID_COLS[4]].iloc[i],\n",
    "        \"ABSTRACT_LEMA\"  : text,\n",
    "        \"EMB\"       : embeddings\n",
    "        })\n",
    "    print(f\"Text {i} processed ‚úÖ\")\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        with open (CHECKPPOINT_FILE, 'wb') as f:\n",
    "            pickle.dump(rows, f)\n",
    "        print (f\"Checkpoint saved: {len(rows)} embeddings\")\n",
    "\n",
    "\n",
    "if os.path.exists(CHECKPPOINT_FILE):\n",
    "    os.remove(CHECKPPOINT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings_AB_SD = pd.DataFrame(rows)\n",
    "Embeddings_AB_SD = Embeddings_AB_SD.rename(columns={\n",
    "    'ABSTRACT_LEMA': 'ABSTRACT'\n",
    "})\n",
    "display(Embeddings_AB_SD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings_AB_SD.to_parquet(\"03_Embeddings/Embeddings_AB_SD.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clusterization LEIDENN WITH GRAPH TYPE K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 LOAD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ==============================\n",
    "#  ==  Load required database  ==\n",
    "#  ==      SELECT  OPTION      ==\n",
    "#  ==============================\n",
    "\n",
    "DIR_EMB_ABS      = [\"03_Embeddings/Embeddings_AB_SD.parquet\", \"TEST_ABS\"]\n",
    "DIR_EMB_ABS_LEMA = [\"03_Embeddings/Embeddings_AB_SD_Lema.parquet\", \"TEST_ABS_LEMA\"]\n",
    "\n",
    "DIR_EMB_HARD     = [\"03_Embeddings/Embeddings_HARD.parquet\", \"TEST_EMB_HARD\"]   # Embeddings with hard pre-process \n",
    "DIR_EMB_SOFT     = [\"03_Embeddings/Embeddings_SOFT.parquet\", \"TEST_EMB_SOFT\"]   # Embeddings with soft pre-process\n",
    "DIR_EMB_RAW      = [\"03_Embeddings/Embeddings_RAW.parquet\", \"TEST_EMB_RAW\"]     # Embeddings without pre-process\n",
    "\n",
    "DIR = [DIR_EMB_ABS, DIR_EMB_ABS_LEMA, DIR_EMB_HARD, DIR_EMB_SOFT, DIR_EMB_RAW]\n",
    "DIR = pd.DataFrame(DIR)\n",
    "\n",
    "display(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings  = pd.read_parquet(DIR[0][2])[\"chunk_emb\"].tolist()\n",
    "#print(type(Embeddings))\n",
    "Texts       = pd.read_parquet(DIR[0][2])['chunk_text'].tolist()\n",
    "#print(Texts)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 FONCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(X):\n",
    "    \"\"\"Normalize rows to unit L2 norm.\"\"\"\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "    return X / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# (A) SIF / All-but-the-top on sentence embeddings\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def sif_all_but_the_top_vectorized(embeddings: np.ndarray, n_components: float = 0.7,           # Revisar 0.65 y 0.75\n",
    "                                   random_state: int = 42) -> np.ndarray:\n",
    "    X = np.asarray(embeddings, dtype=np.float32)\n",
    "    X = normalize(X)\n",
    "\n",
    "    if n_components <= 0:\n",
    "        return X\n",
    "\n",
    "    n_int = int(np.floor(n_components))\n",
    "    fraction = float(n_components - n_int)\n",
    "\n",
    "    # compute enough components once\n",
    "    k = int(np.ceil(n_components))\n",
    "    k = max(1, k)\n",
    "    k = min(k, min(X.shape) - 1)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=k, random_state=random_state)\n",
    "    U_all = svd.fit_transform(X)        # (n_docs, k)\n",
    "    Vt_all = svd.components_            # (k, dim)\n",
    "\n",
    "    # remove full components\n",
    "    X_denoised = X.copy()\n",
    "    if n_int > 0:\n",
    "        X_denoised -= U_all[:, :n_int] @ Vt_all[:n_int, :]\n",
    "\n",
    "    # remove fractional next component (if any)\n",
    "    if fraction > 0:\n",
    "        # next component is index n_int (0-based), ensure it exists\n",
    "        if n_int < k:\n",
    "            X_denoised -= fraction * (U_all[:, n_int:n_int+1] @ Vt_all[n_int:n_int+1, :])\n",
    "\n",
    "    return normalize(X_denoised)\n",
    "\n",
    "def sif_all_but_the_top(embeddings: np.ndarray, n_components: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Remove top principal components from the sentence embedding matrix.\n",
    "    Equivalent spirit to \"All-but-the-Top\" / SIF.\n",
    "    \"\"\"\n",
    "    X = np.array(embeddings, dtype=np.float64)\n",
    "    X = normalize(X)  # spherical\n",
    "    if n_components <= 0:\n",
    "        return X\n",
    "    # Use TruncatedSVD for stability / speed\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    U = svd.fit_transform(X)      # (n_docs, n_comp)\n",
    "    Vt = svd.components_          # (n_comp, dim)\n",
    "    # Project out the components\n",
    "    X_denoised = X - np.dot(U, Vt)\n",
    "    X_denoised = normalize(X_denoised)\n",
    "    return X_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Stats globales por pares aleatorios (sin sesgo)\n",
    "# -----------------------------\n",
    "def pairwise_similarity_stats(\n",
    "    X,\n",
    "    tag=None,\n",
    "    n_pairs=200_000,\n",
    "    seed=42,\n",
    "    assume_normalized=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estad√≠sticas de similitud coseno por muestreo aleatorio de pares.\n",
    "    Incluye negativos. Evita O(n^2).\n",
    "\n",
    "    X: np.ndarray (n, d)\n",
    "    tag: cualquier etiqueta (ej: (\"old\", 1) o (\"frac\", 0.7))\n",
    "    assume_normalized: si True, asume norma ~1 y usa dot; si False, normaliza.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if not assume_normalized:\n",
    "        Xn = normalize(X)\n",
    "    else:\n",
    "        # Normalizamos \"suave\" solo si detectamos desviaciones grandes.\n",
    "        norms = np.linalg.norm(X, axis=1)\n",
    "        if np.nanmax(np.abs(norms - 1.0)) > 1e-3:\n",
    "            Xn = normalize(X)\n",
    "        else:\n",
    "            Xn = X\n",
    "\n",
    "    n = Xn.shape[0]\n",
    "    i = rng.integers(0, n, size=n_pairs)\n",
    "    j = rng.integers(0, n, size=n_pairs)\n",
    "\n",
    "    mask = (i != j)\n",
    "    i, j = i[mask], j[mask]\n",
    "\n",
    "    sim = np.einsum(\"ij,ij->i\", Xn[i], Xn[j])\n",
    "\n",
    "    p10, p25, p50, p75, p90 = np.percentile(sim, [10, 25, 50, 75, 90])\n",
    "    std = sim.std()\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"mean_pair\": float(sim.mean()),\n",
    "        \"median_pair\": float(p50),\n",
    "        \"std_pair\": float(std),\n",
    "        \"p10_pair\": float(p10),\n",
    "        \"p25_pair\": float(p25),\n",
    "        \"p75_pair\": float(p75),\n",
    "        \"iqr_pair\": float(p75 - p25),\n",
    "        \"p90_pair\": float(p90),\n",
    "        \"p95_pair\": float(np.percentile(sim, 95)),\n",
    "        \"p99_pair\": float(np.percentile(sim, 99)),\n",
    "        \"snr_pair\": float((p90 - p10) / std) if std > 0 else np.nan,\n",
    "        \"neg_frac_pair\": float((sim < 0).mean()),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Stats de similitudes kNN (alineado con tu grafo)\n",
    "# -----------------------------\n",
    "def knn_similarity_stats(\n",
    "    X,\n",
    "    tag=None,\n",
    "    k=10,\n",
    "    metric=\"cosine\",\n",
    "    n_sample_nodes=20_000,\n",
    "    seed=42,\n",
    "    assume_normalized=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Estad√≠sticas de similitud sobre aristas kNN (lo que alimenta threshold 'auto').\n",
    "    - Muestrea nodos para evitar coste total cuando N es grande.\n",
    "    - Devuelve percentiles t√≠picos (q25, q50, q75) de similitudes kNN.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    if not assume_normalized:\n",
    "        Xn = normalize(X)\n",
    "    else:\n",
    "        norms = np.linalg.norm(X, axis=1)\n",
    "        if np.nanmax(np.abs(norms - 1.0)) > 1e-3:\n",
    "            Xn = normalize(X)\n",
    "        else:\n",
    "            Xn = X\n",
    "\n",
    "    N = Xn.shape[0]\n",
    "    if N <= 1:\n",
    "        return {\n",
    "            \"tag\": tag,\n",
    "            \"k\": k,\n",
    "            \"q25_knn\": np.nan,\n",
    "            \"q50_knn\": np.nan,\n",
    "            \"q75_knn\": np.nan,\n",
    "            \"mean_knn\": np.nan,\n",
    "            \"min_knn\": np.nan,\n",
    "            \"max_knn\": np.nan,\n",
    "        }\n",
    "\n",
    "    # muestreo de nodos para estad√≠sticas\n",
    "    m = min(N, int(n_sample_nodes))\n",
    "    idx = rng.choice(N, size=m, replace=False)\n",
    "    Xs = Xn[idx]\n",
    "\n",
    "    # kNN: pedimos k+1 para excluir self\n",
    "    k_eff = min(int(k), N - 1)\n",
    "    nn = NearestNeighbors(n_neighbors=k_eff + 1, metric=metric, n_jobs=-1)\n",
    "    nn.fit(Xn)\n",
    "    dists, nbrs = nn.kneighbors(Xs)\n",
    "\n",
    "    d = dists[:, 1:]  # excluye self\n",
    "    if metric == \"cosine\":\n",
    "        sim = 1.0 - d\n",
    "    else:\n",
    "        sim = 1.0 / (1.0 + d)  # misma heur√≠stica tuya\n",
    "\n",
    "    flat = sim.ravel()\n",
    "    q25, q50, q75 = np.percentile(flat, [25, 50, 75])\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"k\": int(k_eff),\n",
    "        \"q25_knn\": float(q25),\n",
    "        \"q50_knn\": float(q50),\n",
    "        \"q75_knn\": float(q75),\n",
    "        \"mean_knn\": float(flat.mean()),\n",
    "        \"min_knn\": float(flat.min()),\n",
    "        \"max_knn\": float(flat.max()),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Evaluador unificado: aplica un SIF y calcula ambos tipos de stats\n",
    "# -----------------------------\n",
    "def evaluate_sif_configs(\n",
    "    embeddings,\n",
    "    configs,\n",
    "    # stats params\n",
    "    n_pairs=200_000,\n",
    "    knn_k=10,\n",
    "    n_sample_nodes=20_000,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    embeddings: array-like (N, D)\n",
    "    configs: lista de dicts con:\n",
    "      - name: str  (ej \"old_int\" / \"frac\")\n",
    "      - n_components: float|int\n",
    "      - sif_fn: callable(embeddings, n_components) -> X_denoised NORMALIZADO\n",
    "        (puedes pasar tu sif_all_but_the_top o sif_all_but_the_top_vectorized)\n",
    "\n",
    "    Devuelve DataFrame con:\n",
    "      - stats globales (pares)\n",
    "      - stats kNN (aristas)\n",
    "    \"\"\"\n",
    "    E = np.asarray(embeddings)\n",
    "    rows = []\n",
    "\n",
    "    for cfg in configs:\n",
    "        name = cfg[\"name\"]\n",
    "        nc = cfg[\"n_components\"]\n",
    "        sif_fn = cfg[\"sif_fn\"]\n",
    "\n",
    "        # aplica SIF tal cual (ambas devuelven normalize(X_denoised))\n",
    "        Xsif = sif_fn(E, n_components=nc)\n",
    "\n",
    "        tag = (name, float(nc))\n",
    "\n",
    "        s_pair = pairwise_similarity_stats(\n",
    "            Xsif, tag=tag, n_pairs=n_pairs, seed=seed, assume_normalized=True\n",
    "        )\n",
    "        s_knn = knn_similarity_stats(\n",
    "            Xsif, tag=tag, k=knn_k, metric=\"cosine\",\n",
    "            n_sample_nodes=n_sample_nodes, seed=seed, assume_normalized=True\n",
    "        )\n",
    "\n",
    "        rows.append({**s_pair, **s_knn})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values([\"tag\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a disperse and SYMMETRIC (max) k-nn graph (weight=cosime similarity, - if metric='cosine)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def build_knn_sparse_optimized(\n",
    "    embeddings,\n",
    "    k=16,\n",
    "    metric=\"cosine\",\n",
    "    similarity_threshold=\"auto\",   # 'auto' => q25 sobre similitudes kNN\n",
    "    auto_q=25,                     # percentil para auto\n",
    "    symmetrization=\"max\",\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    "):\n",
    "    # 1) Normalize (cosine)\n",
    "    X = normalize(np.asarray(embeddings), norm=\"l2\")\n",
    "    N = X.shape[0]\n",
    "    if N <= 1:\n",
    "        return coo_matrix((N, N)).tocsr()\n",
    "\n",
    "    # 2) k sane (sin hacks peligrosos)\n",
    "    k = int(min(k, 50, N - 1))\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    if verbose:\n",
    "        print(f\"Building KNN graph: N={N}, k={k}, metric={metric}\")\n",
    "\n",
    "    # 3) Neighbors\n",
    "    nn = NearestNeighbors(n_neighbors=k + 1, metric=metric, n_jobs=n_jobs)\n",
    "    nn.fit(X)\n",
    "    dists, idxs = nn.kneighbors(X)\n",
    "\n",
    "    # excluir self\n",
    "    neighbors = idxs[:, 1:]             # (N, k)\n",
    "    d = dists[:, 1:]                    # (N, k)\n",
    "\n",
    "    # 4) Similaridades solo si las necesitas (para umbral + pesos)\n",
    "    if metric == \"cosine\":\n",
    "        sim = 1.0 - d                   # cos sim\n",
    "    else:\n",
    "        sim = 1.0 / (1.0 + d)           # heur√≠stica (ok si te sirve)\n",
    "\n",
    "    # 5) Umbral\n",
    "    if similarity_threshold == \"auto\":\n",
    "        rng = np.random.default_rng(seed)\n",
    "        flat = sim.ravel()\n",
    "        # muestreo para no reventar memoria en N grande\n",
    "        if flat.size > 2_000_000:\n",
    "            sample = rng.choice(flat, size=200_000, replace=False)\n",
    "        else:\n",
    "            sample = flat\n",
    "        threshold = float(np.percentile(sample, auto_q))\n",
    "        if verbose:\n",
    "            print(f\"  Auto-threshold (kNN sims): q{auto_q}={threshold:.3f} \"\n",
    "                  f\"(mean={sample.mean():.3f}, median={np.median(sample):.3f})\")\n",
    "    elif similarity_threshold is None:\n",
    "        threshold = -np.inf\n",
    "    else:\n",
    "        threshold = float(similarity_threshold)\n",
    "\n",
    "    # 6) Construcci√≥n sparse vectorizada\n",
    "    mask = sim >= threshold\n",
    "    rows = np.repeat(np.arange(N), k)[mask.ravel()]\n",
    "    cols = neighbors.ravel()[mask.ravel()]\n",
    "    weights = sim.ravel()[mask.ravel()]\n",
    "\n",
    "    # Fallback m√≠nimo: si te quedas sin edges, no inventes top3 (puede sesgar);\n",
    "    # mejor baja el percentil autom√°ticamente\n",
    "    if rows.size == 0:\n",
    "        if verbose:\n",
    "            print(\"Warning: empty graph; relaxing threshold to keep at least 1 neighbor per node.\")\n",
    "        # mantener top-1 por nodo\n",
    "        rows = np.repeat(np.arange(N), 1)\n",
    "        cols = neighbors[:, :1].ravel()\n",
    "        weights = sim[:, :1].ravel()\n",
    "\n",
    "    W = coo_matrix((weights, (rows, cols)), shape=(N, N))\n",
    "\n",
    "    # 7) Symmetrize\n",
    "    if symmetrization is None:\n",
    "        pass\n",
    "    elif symmetrization == \"max\":\n",
    "        W = W.maximum(W.T)\n",
    "    elif symmetrization == \"avg\":\n",
    "        W = (W + W.T) * 0.5\n",
    "    elif symmetrization == \"min\":\n",
    "        W = W.minimum(W.T)\n",
    "    else:\n",
    "        W = W.maximum(W.T)\n",
    "\n",
    "    W = W.tocsr()\n",
    "    W.eliminate_zeros()\n",
    "\n",
    "    # 8) Diagnostics\n",
    "    if verbose:\n",
    "        undirected = symmetrization is not None\n",
    "        n_edges = (W.nnz // 2) if undirected else W.nnz\n",
    "        density = n_edges / (N * (N - 1) / 2)\n",
    "        mean_deg = (2 * n_edges / N) if undirected else (W.nnz / N)\n",
    "        print(f\"  Result: edges={n_edges}, density={density:.8f}, mean_degree‚âà{mean_deg:.2f}\")\n",
    "        if W.nnz:\n",
    "            print(f\"  Edge weights: mean={W.data.mean():.3f}, \"\n",
    "                  f\"min={W.data.min():.3f}, max={W.data.max():.3f}\")\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def graph_density(W):\n",
    "    N = W.shape[0]\n",
    "    possible_edges = N * (N - 1) / 2\n",
    "    actual_edges = W.nnz / 2  # Porque es sim√©trico\n",
    "    return actual_edges / possible_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a symmetric and disperse matrix into a IGRAPH \n",
    "def scipy_to_igraph_light(\n",
    "    W,\n",
    "    weight_threshold=\"auto\",                 # None | \"auto\" | \"quantile\" | float\n",
    "    min_weight_quantile=0.65,                # usado si \"quantile\" (o base en \"auto\")\n",
    "    relax_quantiles=(0.75, 0.65, 0.55, 0.45),\n",
    "    # Best-practice control (local)\n",
    "    min_keep_per_node=4,                     # floor local de grado (recomendado 2‚Äì5)\n",
    "    per_node_candidates_cap=30,\n",
    "    min_reinject_weight=None,\n",
    "    # Debug\n",
    "    verbose=True,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert sparse similarity matrix W to igraph with:\n",
    "      1) global sparsification (threshold by quantile or absolute)\n",
    "      2) local degree floor (min_keep_per_node) re-injecting top edges from original W\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import igraph as ig\n",
    "    from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # --- 0) Normalize format ---\n",
    "    if not isinstance(W, (csr_matrix, coo_matrix)):\n",
    "        W = W.tocsr()\n",
    "\n",
    "    W = W.tocsr(copy=False)\n",
    "    W.eliminate_zeros()\n",
    "    N = W.shape[0]\n",
    "    if N == 0:\n",
    "        return ig.Graph(n=0)\n",
    "    if W.nnz == 0:\n",
    "        if verbose:\n",
    "            print(f\"Converting to igraph: empty W (N={N}).\")\n",
    "        return ig.Graph(n=N)\n",
    "\n",
    "    # Make sure we can sample weights from upper triangle\n",
    "    W_coo = W.tocoo(copy=False)\n",
    "    upper_mask_all = (W_coo.row < W_coo.col)\n",
    "    upper_data = W_coo.data[upper_mask_all]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Converting to igraph: nnz={W.nnz}, N={N}\")\n",
    "        if upper_data.size:\n",
    "            p75 = np.percentile(upper_data, 75)\n",
    "            print(f\"  Weight stats (upper): mean={upper_data.mean():.3f}, \"\n",
    "                  f\"median={np.median(upper_data):.3f}, p75={p75:.3f}\")\n",
    "        else:\n",
    "            print(\"  Warning: no upper-triangle edges found (is W symmetric?)\")\n",
    "\n",
    "    if upper_data.size == 0:\n",
    "        upper_data = W_coo.data  # fallback\n",
    "\n",
    "    # --- helper: sample for quantiles (keeps memory sane) ---\n",
    "    def _sample_for_quantiles(x):\n",
    "        if x.size > 600_000:\n",
    "            return rng.choice(x, size=250_000, replace=False)\n",
    "        return x\n",
    "\n",
    "    # --- 1) Choose threshold candidates (FIXED logic) ---\n",
    "    data_for_q = _sample_for_quantiles(upper_data)\n",
    "\n",
    "    if weight_threshold is None:\n",
    "        # No global thresholding: keep all upper-triangle edges\n",
    "        thr_list = [-np.inf]\n",
    "\n",
    "    elif isinstance(weight_threshold, str):\n",
    "        wt = weight_threshold.lower()\n",
    "\n",
    "        if wt == \"auto\":\n",
    "            # Base thr = min_weight_quantile; then relax (typically higher quantiles first)\n",
    "            thr_list = [float(np.percentile(data_for_q, int(min_weight_quantile * 100)))]\n",
    "            if relax_quantiles:\n",
    "                thr_list += [float(np.percentile(data_for_q, int(q * 100))) for q in relax_quantiles]\n",
    "            # Optional: de-duplicate + sort from strictest to loosest (high->low)\n",
    "            thr_list = sorted(set(thr_list), reverse=True)\n",
    "\n",
    "        elif wt == \"quantile\":\n",
    "            thr_list = [float(np.percentile(data_for_q, int(min_weight_quantile * 100)))]\n",
    "\n",
    "        else:\n",
    "            raise ValueError('weight_threshold must be None, \"auto\", \"quantile\", or a float.')\n",
    "\n",
    "    else:\n",
    "        # Fixed numeric threshold\n",
    "        thr_list = [float(weight_threshold)]\n",
    "\n",
    "    # --- Helpers: build filtered edge dict from threshold ---\n",
    "    def edges_from_threshold(thr: float):\n",
    "        m = (W_coo.data >= thr) & (W_coo.row < W_coo.col)\n",
    "        if not np.any(m):\n",
    "            return {}, thr, 0\n",
    "        r = W_coo.row[m].astype(np.int32, copy=False)\n",
    "        c = W_coo.col[m].astype(np.int32, copy=False)\n",
    "        w = W_coo.data[m].astype(np.float32, copy=False)\n",
    "\n",
    "        ed = {}\n",
    "        for u, v, ww in zip(r, c, w):\n",
    "            key = (int(u), int(v))\n",
    "            prev = ed.get(key)\n",
    "            if (prev is None) or (ww > prev):\n",
    "                ed[key] = float(ww)\n",
    "        return ed, thr, len(ed)\n",
    "\n",
    "    # --- 2) Pick a threshold that isn't absurdly sparse ---\n",
    "    chosen_edges = None\n",
    "    chosen_thr = None\n",
    "\n",
    "    for thr in thr_list:\n",
    "        ed, thr_used, ecount = edges_from_threshold(thr)\n",
    "        if ecount == 0:\n",
    "            if verbose:\n",
    "                if thr == -np.inf:\n",
    "                    print(\"  thr=-inf => 0 edges (unexpected).\")\n",
    "                else:\n",
    "                    print(f\"  thr={thr_used:.3f} => 0 edges, relaxing‚Ä¶\")\n",
    "            continue\n",
    "        chosen_edges, chosen_thr = ed, thr_used\n",
    "        if verbose:\n",
    "            thr_print = \"-inf\" if chosen_thr == -np.inf else f\"{chosen_thr:.3f}\"\n",
    "            print(f\"  thr={thr_print} => base edges={len(chosen_edges)} (pre min-degree)\")\n",
    "        break\n",
    "\n",
    "    if chosen_edges is None:\n",
    "        if verbose:\n",
    "            print(\"  Could not keep any edge after thresholding. Returning empty graph.\")\n",
    "        return ig.Graph(n=N)\n",
    "\n",
    "    # --- 3) Compute degrees in current edge set (undirected) ---\n",
    "    deg = np.zeros(N, dtype=np.int32)\n",
    "    for (u, v) in chosen_edges.keys():\n",
    "        deg[u] += 1\n",
    "        deg[v] += 1\n",
    "\n",
    "    # --- 4) Re-inject top edges per node if degree < min_keep_per_node ---\n",
    "    if min_keep_per_node and min_keep_per_node > 0:\n",
    "        need_nodes = np.where(deg < min_keep_per_node)[0]\n",
    "        if verbose:\n",
    "            iso = int((deg == 0).sum())\n",
    "            print(f\"  Degree stats (base): isolated={iso} ({iso/N:.2%}), \"\n",
    "                  f\"min={deg.min()}, p10={np.percentile(deg,10):.0f}, \"\n",
    "                  f\"p50={np.percentile(deg,50):.0f}, p90={np.percentile(deg,90):.0f}, max={deg.max()}\")\n",
    "\n",
    "        if need_nodes.size > 0:\n",
    "            W_csr = W  # CSR for fast row slicing\n",
    "            added = 0\n",
    "            floor_w = -np.inf if (min_reinject_weight is None) else float(min_reinject_weight)\n",
    "\n",
    "            for i in need_nodes:\n",
    "                start, end = W_csr.indptr[i], W_csr.indptr[i+1]\n",
    "                cols = W_csr.indices[start:end]\n",
    "                vals = W_csr.data[start:end]\n",
    "\n",
    "                if cols.size == 0:\n",
    "                    continue\n",
    "\n",
    "                mask = cols != i\n",
    "                cols = cols[mask]\n",
    "                vals = vals[mask]\n",
    "                if cols.size == 0:\n",
    "                    continue\n",
    "\n",
    "                cap = min(per_node_candidates_cap, cols.size)\n",
    "                if cap < cols.size:\n",
    "                    top_idx = np.argpartition(vals, -cap)[-cap:]\n",
    "                    cols_top = cols[top_idx]\n",
    "                    vals_top = vals[top_idx]\n",
    "                else:\n",
    "                    cols_top = cols\n",
    "                    vals_top = vals\n",
    "\n",
    "                order = np.argsort(vals_top)[::-1]\n",
    "                cols_top = cols_top[order]\n",
    "                vals_top = vals_top[order]\n",
    "\n",
    "                for j, w in zip(cols_top, vals_top):\n",
    "                    if float(w) < floor_w:\n",
    "                        break\n",
    "                    u, v = int(i), int(j)\n",
    "                    if u == v:\n",
    "                        continue\n",
    "                    if u > v:\n",
    "                        u, v = v, u\n",
    "\n",
    "                    prev = chosen_edges.get((u, v))\n",
    "                    if prev is None:\n",
    "                        chosen_edges[(u, v)] = float(w)\n",
    "                        deg[i] += 1\n",
    "                        deg[int(j)] += 1\n",
    "                        added += 1\n",
    "                    else:\n",
    "                        if float(w) > prev:\n",
    "                            chosen_edges[(u, v)] = float(w)\n",
    "\n",
    "                    if deg[i] >= min_keep_per_node:\n",
    "                        break\n",
    "\n",
    "            if verbose:\n",
    "                iso2 = int((deg == 0).sum())\n",
    "                print(f\"  Reinjected edges={added}. Degree stats (post): isolated={iso2} ({iso2/N:.2%}), \"\n",
    "                      f\"min={deg.min()}, p10={np.percentile(deg,10):.0f}, \"\n",
    "                      f\"p50={np.percentile(deg,50):.0f}, p90={np.percentile(deg,90):.0f}, max={deg.max()}\")\n",
    "\n",
    "    # --- 5) Build igraph ---\n",
    "    edges = np.fromiter((u for (u, v) in chosen_edges.keys()), dtype=np.int32, count=len(chosen_edges))\n",
    "    cols  = np.fromiter((v for (u, v) in chosen_edges.keys()), dtype=np.int32, count=len(chosen_edges))\n",
    "    wts   = np.fromiter((w for w in chosen_edges.values()), dtype=np.float32, count=len(chosen_edges))\n",
    "\n",
    "    edge_list = list(zip(edges.tolist(), cols.tolist()))\n",
    "    G = ig.Graph(n=N, edges=edge_list, directed=False)\n",
    "    G.es[\"weight\"] = wts.tolist()\n",
    "\n",
    "    if verbose:\n",
    "        comps = G.connected_components()\n",
    "        ew = np.array(G.es[\"weight\"], dtype=np.float32) if G.ecount() else np.array([], dtype=np.float32)\n",
    "        thr_print = \"-inf\" if chosen_thr == -np.inf else f\"{chosen_thr:.3f}\"\n",
    "        print(f\"  Final: thr={thr_print} => V={G.vcount()}, E={G.ecount()}, components={len(comps)}\")\n",
    "        if ew.size:\n",
    "            print(f\"  Weight stats (kept): mean={ew.mean():.3f}, min={ew.min():.3f}, max={ew.max():.3f}\")\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Leiden clustering\n",
    "import numpy as np\n",
    "import leidenalg\n",
    "\n",
    "def run_leiden_one_robust(G, resolution=1.0, n_restarts=5, seed=42, n_iterations=2):\n",
    "    \"\"\"\n",
    "    Corre Leiden varias veces con seeds distintos y escoge el de mejor quality().\n",
    "    Devuelve: labels, best_partition, meta(dict)\n",
    "    \"\"\"\n",
    "    if G.ecount() == 0:\n",
    "        labels = np.arange(G.vcount(), dtype=int)\n",
    "        meta = {\"resolution\": float(resolution), \"n_clusters\": G.vcount(),\n",
    "                \"largest\": 1, \"median_size\": 1, \"small_leq_10\": G.vcount(),\n",
    "                \"quality\": np.nan}\n",
    "        return labels, None, meta\n",
    "\n",
    "    weights = G.es[\"weight\"] if \"weight\" in G.es.attributes() else None\n",
    "\n",
    "    best_part = None\n",
    "    best_q = -np.inf\n",
    "\n",
    "    for i in range(n_restarts):\n",
    "        this_seed = None if seed is None else seed + i\n",
    "        part = leidenalg.find_partition(\n",
    "            G,\n",
    "            leidenalg.RBConfigurationVertexPartition,\n",
    "            weights=weights,\n",
    "            resolution_parameter=float(resolution),\n",
    "            seed=this_seed,\n",
    "            n_iterations=int(n_iterations),\n",
    "        )\n",
    "        q = float(part.quality())\n",
    "        if q > best_q:\n",
    "            best_q = q\n",
    "            best_part = part\n",
    "\n",
    "    labels = np.empty(G.vcount(), dtype=int)\n",
    "    for cid, nodes in enumerate(best_part):\n",
    "        labels[nodes] = cid\n",
    "\n",
    "    sizes = np.array([len(c) for c in best_part], dtype=int)\n",
    "    meta = {\n",
    "        \"resolution\": float(resolution),\n",
    "        \"n_clusters\": int(len(sizes)),\n",
    "        \"largest\": int(sizes.max()) if sizes.size else 0,\n",
    "        \"median_size\": int(np.median(sizes)) if sizes.size else 0,\n",
    "        \"small_leq_10\": int((sizes <= 10).sum()) if sizes.size else 0,\n",
    "        \"quality\": float(best_q),\n",
    "    }\n",
    "    return labels, best_part, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 LEIDEN IMLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings  = pd.read_parquet(DIR[0][4])[\"chunk_emb\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Ejemplo de uso (comparar viejo vs fraccional)\n",
    "# -----------------------------\n",
    "configs = [\n",
    "    {\"name\": \"old_int\", \"n_components\": 0,   \"sif_fn\": sif_all_but_the_top},\n",
    "    {\"name\": \"old_int\", \"n_components\": 1,   \"sif_fn\": sif_all_but_the_top},\n",
    "\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.1, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.2, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.3, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.4, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.5, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.6, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.7, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.8, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "    {\"name\": \"frac\",    \"n_components\": 0.9, \"sif_fn\": sif_all_but_the_top_vectorized},\n",
    "]\n",
    "df1 = evaluate_sif_configs(Embeddings, configs, n_pairs=200_000, knn_k=10, n_sample_nodes=20_000)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = pd.concat([df1, df], ignore_index=True)\n",
    "display(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar tus datos (asumiendo que est√°n en df)\n",
    "n_values = df['n_components'].values\n",
    "mean_values = df['mean'].values\n",
    "snr_values = df['signal_noise_ratio'].values\n",
    "p75_values = df['p75'].values\n",
    "p25_values = df['p25'].values\n",
    "iqr_values = df['iqr'].values\n",
    "\n",
    "# Definir rangos √≥ptimos basados en an√°lisis para Leiden\n",
    "RANGOS_OPTIMOS = {\n",
    "    'mean': (0.15, 0.35),  # Ni muy alto (bias) ni muy bajo (p√©rdida se√±al)\n",
    "    'signal_noise_ratio': (2.50, 2.65),  # Alto pero no extremo\n",
    "    'p75': (0.25, 0.45),  # Buen umbral para edges en KNN\n",
    "    'p25': (0.15, 0.40),  # Suficiente para conectividad m√≠nima\n",
    "    'iqr': (0.13, 0.18),  # Dispersi√≥n buena para diferenciaci√≥n\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. MEAN con franja √≥ptima\n",
    "ax = axes[0, 0]\n",
    "ax.plot(n_values, mean_values, 'o-', linewidth=2, markersize=6, \n",
    "        markerfacecolor='red', markeredgecolor='darkred', label='Mean')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "# Franja √≥ptima\n",
    "ax.axhspan(RANGOS_OPTIMOS['mean'][0], RANGOS_OPTIMOS['mean'][1], \n",
    "           alpha=0.1, color='blue', label='Rango √≥ptimo Leiden')\n",
    "ax.axhline(y=RANGOS_OPTIMOS['mean'][0], color='blue', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=RANGOS_OPTIMOS['mean'][1], color='blue', linestyle='--', alpha=0.5)\n",
    "# Puntos de inter√©s\n",
    "for n in [0.7, 0.8, 0.9]:\n",
    "    idx = np.where(n_values == n)[0][0]\n",
    "    ax.plot(n, mean_values[idx], 'o', markersize=10, markerfacecolor='none', \n",
    "            markeredgecolor='red', markeredgewidth=2)\n",
    "ax.set_title('Mean Similarity vs n_components\\n(Rango √≥ptimo: 0.15-0.35)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('Mean')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-0.1, 2.2)\n",
    "\n",
    "# 2. SIGNAL/NOISE RATIO con franja √≥ptima\n",
    "ax = axes[0, 1]\n",
    "ax.plot(n_values, snr_values, 's-', linewidth=2, markersize=6,\n",
    "        markerfacecolor='blue', markeredgecolor='darkblue', label='SNR')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "# Franja √≥ptima\n",
    "ax.axhspan(RANGOS_OPTIMOS['signal_noise_ratio'][0], RANGOS_OPTIMOS['signal_noise_ratio'][1],\n",
    "           alpha=0.1, color='red', label='Rango √≥ptimo Leiden')\n",
    "ax.axhline(y=RANGOS_OPTIMOS['signal_noise_ratio'][0], color='red', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=RANGOS_OPTIMOS['signal_noise_ratio'][1], color='red', linestyle='--', alpha=0.5)\n",
    "# M√°ximo\n",
    "max_idx = np.argmax(snr_values)\n",
    "ax.plot(n_values[max_idx], snr_values[max_idx], '*', markersize=15, \n",
    "        color='gold', label=f'M√°ximo: n={n_values[max_idx]:.1f}')\n",
    "ax.set_title('Signal/Noise Ratio vs n_components\\n(Rango √≥ptimo: 2.50-2.65)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('SNR')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-0.1, 2.2)\n",
    "\n",
    "# 3. P75 (umbral alto) con franja √≥ptima\n",
    "ax = axes[1, 0]\n",
    "ax.plot(n_values, p75_values, '^-', linewidth=2, markersize=6,\n",
    "        markerfacecolor='green', markeredgecolor='darkgreen', label='p75')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "# Franja √≥ptima para KNN edges\n",
    "ax.axhspan(RANGOS_OPTIMOS['p75'][0], RANGOS_OPTIMOS['p75'][1],\n",
    "           alpha=0.1, color='purple', label='Rango para KNN edges')\n",
    "ax.axhline(y=RANGOS_OPTIMOS['p75'][0], color='purple', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=RANGOS_OPTIMOS['p75'][1], color='purple', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Percentil 75 (similitudes altas) vs n_components\\n(Rango para KNN: 0.25-0.45)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('p75')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-0.1, 2.2)\n",
    "\n",
    "# 4. P25 (conectividad m√≠nima) con franja √≥ptima\n",
    "ax = axes[1, 1]\n",
    "ax.plot(n_values, p25_values, 'v-', linewidth=2, markersize=6,\n",
    "        markerfacecolor='orange', markeredgecolor='darkorange', label='p25')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "# Franja cr√≠tica para conectividad\n",
    "ax.axhspan(RANGOS_OPTIMOS['p25'][0], RANGOS_OPTIMOS['p25'][1],\n",
    "           alpha=0.1, color='brown', label='M√≠nimo para conectividad')\n",
    "ax.axhline(y=RANGOS_OPTIMOS['p25'][0], color='brown', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=RANGOS_OPTIMOS['p25'][1], color='brown', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Percentil 25 (conectividad m√≠nima) vs n_components\\n(Rango m√≠nimo: 0.15-0.40)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('p25')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-0.1, 3.2)\n",
    "\n",
    "# 5. IQR (dispersi√≥n) con franja √≥ptima\n",
    "ax = axes[2, 0]\n",
    "ax.plot(n_values, iqr_values, 'd-', linewidth=2, markersize=6,\n",
    "        markerfacecolor='purple', markeredgecolor='purple', label='IQR')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "# Franja √≥ptima para diferenciaci√≥n\n",
    "ax.axhspan(RANGOS_OPTIMOS['iqr'][0], RANGOS_OPTIMOS['iqr'][1],\n",
    "           alpha=0.1, color='teal', label='Rango para diferenciaci√≥n')\n",
    "ax.axhline(y=RANGOS_OPTIMOS['iqr'][0], color='teal', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=RANGOS_OPTIMOS['iqr'][1], color='teal', linestyle='--', alpha=0.5)\n",
    "ax.set_title('IQR (dispersi√≥n intercuartil) vs n_components\\n(Rango √≥ptimo: 0.13-0.18)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('IQR')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(-0.1, 3.2)\n",
    "\n",
    "# 6. ZONA √ìPTIMA INTEGRADA\n",
    "ax = axes[2, 1]\n",
    "# Calcular puntuaci√≥n compuesta\n",
    "score_mean = np.clip((mean_values - 0.15) / (0.35 - 0.15), 0, 1)\n",
    "score_snr = np.clip((snr_values - 2.50) / (2.65 - 2.50), 0, 1)\n",
    "score_p75 = np.clip((p75_values - 0.25) / (0.45 - 0.25), 0, 1)\n",
    "score_p25 = np.clip((p25_values - 0.15) / (0.40 - 0.15), 0, 1)\n",
    "score_iqr = np.clip((iqr_values - 0.13) / (0.18 - 0.13), 0, 1)\n",
    "\n",
    "# Ponderaciones seg√∫n importancia para Leiden\n",
    "pesos = {'mean': 0.2, 'snr': 0.3, 'p75': 0.2, 'p25': 0.2, 'iqr': 0.1}\n",
    "score_total = (score_mean * pesos['mean'] + score_snr * pesos['snr'] + \n",
    "               score_p75 * pesos['p75'] + score_p25 * pesos['p25'] + \n",
    "               score_iqr * pesos['iqr'])\n",
    "\n",
    "ax.plot(n_values, score_total, 'o-', linewidth=3, markersize=8,\n",
    "        markerfacecolor='darkred', markeredgecolor='black', label='Puntuaci√≥n Leiden')\n",
    "ax.axvspan(0.7, 0.9, alpha=0.2, color='green', label='Zona cr√≠tica')\n",
    "\n",
    "# Franja √≥ptima (score > 0.7)\n",
    "ax.axhspan(0.7, 1.0, alpha=0.1, color='gold', label='Zona √≥ptima (score > 0.7)')\n",
    "ax.axhline(y=0.7, color='gold', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Encontrar √≥ptimos\n",
    "optimal_idx = np.argmax(score_total)\n",
    "ax.plot(n_values[optimal_idx], score_total[optimal_idx], '*', \n",
    "        markersize=20, color='gold', label=f'√ìptimo: n={n_values[optimal_idx]:.1f}')\n",
    "\n",
    "# Marcar puntos de inter√©s\n",
    "for n in [0.6, 0.7, 0.8, 0.9]:\n",
    "    idx = np.where(np.abs(n_values - n) < 0.01)[0][0]\n",
    "    ax.plot(n, score_total[idx], 'o', markersize=10, markerfacecolor='none',\n",
    "            markeredgecolor='red', markeredgewidth=2)\n",
    "\n",
    "ax.set_title('Puntuaci√≥n Integrada para Leiden\\n(Combinaci√≥n de m√©tricas)')\n",
    "ax.set_xlabel('n_components')\n",
    "ax.set_ylabel('Puntuaci√≥n Leiden (0-1)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim(-0.1, 3.2)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tabla de puntuaciones para la zona cr√≠tica\n",
    "print(\"=== AN√ÅLISIS DE LA ZONA CR√çTICA (0.6-0.9) ===\")\n",
    "zona_critica = df[(df['n_components'] >= 0.6) & (df['n_components'] <= 0.9)].copy()\n",
    "zona_critica['score_leiden'] = score_total[(n_values >= 0.6) & (n_values <= 0.9)]\n",
    "\n",
    "print(zona_critica[['n_components', 'mean', 'signal_noise_ratio', 'p25', 'p75', 'score_leiden']].to_string())\n",
    "print(f\"\\n‚òÖ RECOMENDACI√ìN FINAL: n = {n_values[optimal_idx]:.1f} (score: {score_total[optimal_idx]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_apox_1 = np.log2(len(embeddings))\n",
    "k_apox_2 = np.sqrt(len(embeddings))\n",
    "print(k_apox_1)\n",
    "print(k_apox_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings_SIF = sif_all_but_the_top_vectorized(embeddings=embeddings, n_components=0.7)\n",
    "# Verificar calidad de embeddings post-SIF\n",
    "print(f\"Embeddings shape: {Embeddings_SIF.shape}\")\n",
    "print(f\"Normas despu√©s de L2: {np.linalg.norm(Embeddings_SIF, axis=1)[:5]}\")\n",
    "print(f\"Similaridad promedio entre primeros 30000 documentos: {cosine_similarity(Embeddings_SIF[:30000]).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinar densidad y distribuci√≥n de pesos\n",
    "W = build_knn_sparse(Embeddings_SIF, k=15)\n",
    "print(f\"Densidad del grafo: {W.nnz / (W.shape[0]**2):.6f}\")\n",
    "print(f\"Distribuci√≥n de pesos: min={W.data.min():.3f}, \"\n",
    "      f\"mediana={np.median(W.data):.3f}, max={W.data.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple implementation\n",
    "\n",
    "ks = 60\n",
    "\n",
    "W = build_knn_sparse (\n",
    "    Embeddings,     # Optional Embeddings_SIF\n",
    "    k = ks, \n",
    "    metric = \"cosine\", \n",
    "    n_jobs = -1\n",
    "    )\n",
    "G = scipy_to_igraph (W)\n",
    "\n",
    "labels_LEIDEN, partition_LEIDEN = run_leiden_robust(\n",
    "    G, \n",
    "    resolution = 5,\n",
    "    n_iterations = 1,\n",
    "    seed = 42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 LEIDEN EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_semantic_centroids (Embeddings, labels):\n",
    "    centroids = {}\n",
    "    for t in np.unique(labels):\n",
    "        mask = (labels == t)\n",
    "        centroids[t] = Embeddings[mask].mean(axis=0)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sizes (labels):\n",
    "    cnt = Counter(labels)\n",
    "    sizes = np.array(list(cnt.values()), dtype=int)\n",
    "    return sizes\n",
    "\n",
    "def max_share (labels):\n",
    "    sizes = cluster_sizes(labels)\n",
    "    N = sizes.sum()\n",
    "    return sizes.max() / N\n",
    "\n",
    "def gini_of_sizes (labels, eps=1e-9):\n",
    "    x = cluster_sizes(labels).astype(float)\n",
    "    x = np.sort(x)\n",
    "    n = x.size\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    return (2.0 * np.sum((np.arange(1 , n+1) * x)) /(n * (x.sum() + eps))) - (n + 1) / n\n",
    "\n",
    "def comprehensive_cluster_metrics(labels, embeddings = None, graph = None):\n",
    "\n",
    "    sizes = cluster_sizes(labels)\n",
    "    n_clusters = len(sizes)\n",
    "    n_docs = len(labels)\n",
    "\n",
    "    metrics = {\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_singletons': np.sum(sizes == 1),\n",
    "        'max_cluster_size': sizes.max(),\n",
    "        'min_cluster_size': sizes.min(),\n",
    "        'avg_cluster_size': sizes.mean(),\n",
    "        'max_share': max_share(labels),\n",
    "        'gini': gini_of_sizes(labels),\n",
    "        'size_std': sizes.std()\n",
    "    }\n",
    "    \n",
    "    # Silueta si tenemos embeddings\n",
    "    if embeddings is not None and n_clusters > 1:\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        try:\n",
    "            # Muestrear para eficiencia si hay muchos datos\n",
    "            if n_docs > 10000:\n",
    "                sample_idx = np.random.choice(n_docs, 10000, replace=False)\n",
    "                sample_emb = embeddings[sample_idx]\n",
    "                sample_labels = labels[sample_idx]\n",
    "            else:\n",
    "                sample_emb = embeddings\n",
    "                sample_labels = labels\n",
    "            \n",
    "            metrics['silhouette'] = silhouette_score(sample_emb, sample_labels, metric='cosine')\n",
    "        except:\n",
    "            metrics['silhouette'] = -1\n",
    "    else:\n",
    "        metrics['silhouette'] = -1\n",
    "    \n",
    "    # Modularidad si tenemos el grafo\n",
    "    if graph is not None and graph.ecount() > 0:\n",
    "        try:\n",
    "            partition = leidenalg.RBConfigurationVertexPartition(graph, weights=graph.es['weight'])\n",
    "            partition.membership = labels.tolist()\n",
    "            metrics['modularity'] = partition.quality()\n",
    "        except:\n",
    "            metrics['modularity'] = -1\n",
    "    else:\n",
    "        metrics['modularity'] = -1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scientific_metrics(labels, embeddings,\n",
    "                           sample_size=12000,\n",
    "                           per_cluster_cap=600,\n",
    "                           seed=42,\n",
    "                           coverage_threshold=0.7,\n",
    "                           coverage_min_cluster_size=30):\n",
    "    \"\"\"\n",
    "    Safe scientific metrics for large N.\n",
    "    - silhouette: sample (as you already do)\n",
    "    - calinski_harabasz / davies_bouldin: sample (otherwise too heavy at 100k)\n",
    "    - coverage_0.7: capped per cluster + only for sufficiently large clusters\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        silhouette_score,\n",
    "        calinski_harabasz_score,\n",
    "        davies_bouldin_score\n",
    "    )\n",
    "\n",
    "    X = np.asarray(embeddings, dtype=np.float32)\n",
    "    y = np.asarray(labels)\n",
    "\n",
    "    metrics = {}\n",
    "    uniq = np.unique(y)\n",
    "    if uniq.size <= 1:\n",
    "        # single cluster => undefined\n",
    "        metrics[\"silhouette\"] = -1\n",
    "        metrics[\"calinski_harabasz\"] = -1\n",
    "        metrics[\"davies_bouldin\"] = float(\"inf\")\n",
    "        metrics[\"coverage_0.7\"] = -1\n",
    "        metrics[\"sci_sample_n\"] = 0\n",
    "        return metrics\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # ---------- Build a stratified sample for global metrics (sil/CH/DB) ----------\n",
    "    idxs = []\n",
    "    for c in uniq:\n",
    "        members = np.where(y == c)[0]\n",
    "        if members.size == 0:\n",
    "            continue\n",
    "        take = min(per_cluster_cap, members.size)\n",
    "        idxs.append(rng.choice(members, size=take, replace=False))\n",
    "    idx = np.concatenate(idxs) if idxs else rng.choice(np.arange(len(y)), size=min(sample_size, len(y)), replace=False)\n",
    "\n",
    "    if idx.size > sample_size:\n",
    "        idx = rng.choice(idx, size=sample_size, replace=False)\n",
    "\n",
    "    Xs = X[idx]\n",
    "    ys = y[idx]\n",
    "    metrics[\"sci_sample_n\"] = int(idx.size)\n",
    "\n",
    "    # ---------- 1) Silhouette (cosine) ----------\n",
    "    try:\n",
    "        metrics[\"silhouette\"] = float(silhouette_score(Xs, ys, metric=\"cosine\"))\n",
    "    except Exception:\n",
    "        metrics[\"silhouette\"] = -1\n",
    "\n",
    "    # ---------- 2) Calinski-Harabasz (sampled) ----------\n",
    "    # Note: CH is Euclidean-based; still ok as heuristic on embeddings.\n",
    "    try:\n",
    "        metrics[\"calinski_harabasz\"] = float(calinski_harabasz_score(Xs, ys))\n",
    "    except Exception:\n",
    "        metrics[\"calinski_harabasz\"] = -1\n",
    "\n",
    "    # ---------- 3) Davies-Bouldin (sampled) ----------\n",
    "    try:\n",
    "        metrics[\"davies_bouldin\"] = float(davies_bouldin_score(Xs, ys))\n",
    "    except Exception:\n",
    "        metrics[\"davies_bouldin\"] = float(\"inf\")\n",
    "\n",
    "    # ---------- 4) Coverage (Amig√≥ et al., 2009-inspired) ----------\n",
    "    # We compute: for each (large-enough) cluster, fraction of points with cos(sim) to centroid > threshold.\n",
    "    # Use caps to avoid heavy loops with many microclusters.\n",
    "    try:\n",
    "        # normalize once for cosine via dot\n",
    "        Xn = normalize(X, norm=\"l2\")\n",
    "        coverage_scores = []\n",
    "        for c in uniq:\n",
    "            members = np.where(y == c)[0]\n",
    "            if members.size < coverage_min_cluster_size:\n",
    "                continue  # skip microclusters to avoid huge loop count + noise\n",
    "            # cap per cluster\n",
    "            if members.size > per_cluster_cap:\n",
    "                members = rng.choice(members, size=per_cluster_cap, replace=False)\n",
    "\n",
    "            # centroid on normalized vectors (stable)\n",
    "            centroid = Xn[members].mean(axis=0)\n",
    "            # normalize centroid\n",
    "            centroid /= (np.linalg.norm(centroid) + 1e-12)\n",
    "\n",
    "            sims = Xn[members] @ centroid  # fast cosine similarity\n",
    "            coverage_scores.append(float(np.mean(sims > coverage_threshold)))\n",
    "\n",
    "        metrics[\"coverage_0.7\"] = float(np.mean(coverage_scores)) if coverage_scores else -1\n",
    "    except Exception:\n",
    "        metrics[\"coverage_0.7\"] = -1\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory_usage(details, keep_full_data=False):\n",
    "    \"\"\"Reduce memoria manteniendo informaci√≥n esencial\"\"\"\n",
    "    optimized_details = {}\n",
    "    \n",
    "    for config, data in details.items():\n",
    "        if keep_full_data:\n",
    "            # Mantener todo para an√°lisis posterior\n",
    "            optimized_details[config] = data\n",
    "        else:\n",
    "            # Solo m√©tricas agregadas (90% menos memoria)\n",
    "            optimized_details[config] = {\n",
    "                'm1_summary': data['first_mesure'].describe().to_dict(),\n",
    "                'm3_summary': data['third_mesure'].describe().to_dict(),\n",
    "                'cluster_sizes': cluster_sizes(data['labels']),\n",
    "                # No guardar labels a menos que sea necesario\n",
    "                # 'labels': data['labels']  \n",
    "            }\n",
    "    \n",
    "    return optimized_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scientific_scoring(df_summary):\n",
    "    \"\"\"Sistema de scoring con base cient√≠fica s√≥lida\"\"\"\n",
    "    return (\n",
    "        + 2.0 * df_summary[\"m1_margin_mean\"]           # Separaci√≥n (Silhouette-like)\n",
    "        + 1.5 * df_summary[\"m3_cos_mean_w\"]            # Cohesi√≥n (Purity)\n",
    "        - 1.0 * df_summary[\"m3_share_below_0.6_w\"]     # Calidad m√≠nima\n",
    "        - 0.5 * (df_summary[\"small_leq_10\"] / df_summary[\"n_clusters\"].clip(lower=1))  # Fragmentaci√≥n\n",
    "        + 1.0 * df_summary.get(\"silhouette\", 0)        # M√©trica establecida\n",
    "        - 0.3 * df_summary.get(\"davies_bouldin\", 0)    # Compactness vs Separation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST MESURE\n",
    "# INTER topics vs ENTRE topics - FAST VERSION\n",
    "\n",
    "def cluster_separation_fast (Embeddings, labels):\n",
    "    \"\"\"\n",
    "    First mesure\n",
    "     - intra_mean_doc_centroid\n",
    "     - intra_p10_doc_centroid\n",
    "     - inter_mean_doc_centroid\n",
    "     - inter_max_doc_centroid\n",
    "     margin_vs_intr_max = intra_mean - inter_max (‚â•0 no desirable)\n",
    "    \"\"\"\n",
    "    centroids = compute_semantic_centroids(Embeddings, labels)\n",
    "    cols = [\"topic\",\"n_docs\",\n",
    "            \"intra_mean_doc_centroid\",\n",
    "            \"intra_p10_doc_centroid\",\n",
    "            \"inter_mean_doc_centroid\",\n",
    "            \"inter_max_doc_centroid\",\n",
    "            \"margin_vs_inter_max\"]\n",
    "    if not centroids:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    topics = sorted (centroids.keys())\n",
    "    MU = np.vstack ([centroids [t] for t in topics])\n",
    "    MU_norm = MU / np.linalg.norm(MU, axis=1, keepdims=True)\n",
    "    S_cc = MU_norm @ MU_norm.T\n",
    "    np.fill_diagonal (S_cc, -1)\n",
    "\n",
    "    rows = []\n",
    "    for t in topics:\n",
    "        mask = (labels == t)\n",
    "        X = Embeddings[mask]\n",
    "        MU_t = centroids[t][None, :]\n",
    "        intra_cos = cosine_similarity (X, MU_t)[:, 0]\n",
    "        inter_mean = S_cc[topics.index(t)].mean() if len(topics) > 1 else -1.0\n",
    "        inter_max = S_cc[topics.index(t)].max() if len(topics) >1 else -1.0\n",
    "        rows.append ({\n",
    "            \"topic\": t,\n",
    "            \"n_docs\": int(mask.sum()),\n",
    "            \"intra_mean_doc_centroid\": float(intra_cos.mean()),\n",
    "            \"intra_p10_doc_centroid\": float(np.percentile(intra_cos, 10)),\n",
    "            \"inter_mean_doc_centroid\": float(inter_mean),\n",
    "            \"inter_max_doc_centroid\": float(inter_max),\n",
    "            \"margin_vs_inter_max\": float(intra_cos.mean() - inter_max)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"margin_vs_inter_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD MESURE\n",
    "# COVERAGE vs PURITY\n",
    "\n",
    "def coverage_purity (Embeddings, labels, thresholds = (0.5, 0.6, 0.7)):\n",
    "    centroids = compute_semantic_centroids(Embeddings, labels)\n",
    "    cols = [\"topic\",\n",
    "            \"n_docs\",\n",
    "            \"cos_mean\",\n",
    "            \"cos_p10\",\n",
    "            \"cos_p50\",\n",
    "            \"cos_p90\"] + \\\n",
    "            [f\"share_beloww_{th:.1f}\" for th in thresholds]\n",
    "    if not centroids:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    \n",
    "    rows = []\n",
    "    for t, mu in centroids.items():\n",
    "        mask = (labels == t)\n",
    "        X = Embeddings[mask]\n",
    "        sims = cosine_similarity(X, mu[None, :])[:, 0]\n",
    "        row = {\n",
    "            \"topic\": t,\n",
    "            \"n_docs\": int(mask.sum()),\n",
    "            \"cos_mean\": float(sims.mean()),\n",
    "            \"cos_p10\": float(np.percentile(sims, 10)),\n",
    "            \"cos_p50\": float(np.percentile(sims, 50)),\n",
    "            \"cos_p90\": float(np.percentile(sims, 90)),\n",
    "        }\n",
    "        for th in thresholds:\n",
    "            row[f\"share_below_{th:.1f}\"] = float((sims < th).mean())\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows, columns= cols).sort_values(\"cos_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEIDEN with GRAPH K_NN Second Experiment\n",
    "# Experiment to assest the \"Intra vs Inter Topics\" & \"Coverage vs Purity\"\n",
    "\n",
    "def summarize_sizes (labels):\n",
    "    \"\"\"Cluster size distribution for a given label\"\"\"\n",
    "    vc = pd.Series(labels).value_counts().sort_values(ascending=False)\n",
    "    return {\n",
    "        \"n_clusters\": int(vc.shape[0]),\n",
    "        \"largest\": int(vc.iloc[0]),\n",
    "        \"median_size\": int(vc.median()),\n",
    "        \"small_leq_10\": int((vc <= 10).sum())\n",
    "    }\n",
    "\n",
    "def summarize_m1 (df_m1):\n",
    "    return {\n",
    "        \"m1_margin_mean\": float(df_m1[\"margin_vs_inter_max\"].mean()),\n",
    "        \"m1_margin_p10\": float(df_m1[\"margin_vs_inter_max\"].quantile(0.10)),\n",
    "        \"m1_intra_mean\": float(df_m1[\"intra_mean_doc_centroid\"].mean()),\n",
    "        \"m1_inter_max_mean\": float(df_m1[\"inter_max_doc_centroid\"].mean())\n",
    "    }\n",
    "\n",
    "def summarize_m3 (df_m3):\n",
    "    w = df_m3[\"n_docs\"].clip(lower=1)       # Avoid division by zero\n",
    "    w = w / w.sum()                         # Average ponderatrion by size\n",
    "    out = {\n",
    "        \"m3_cos_mean_w\": float((df_m3[\"cos_mean\"] * w).sum()),\n",
    "        \"m3_cos_p50_w\": float((df_m3[\"cos_p50\"] * w).sum()),\n",
    "        \"m3_share_below_0.6_w\": float((df_m3.get(\"share_below_0.6\", 0) * w).sum()),\n",
    "        \"m3_share_below_0.7_w\": float((df_m3.get(\"share_below_0.7\", 0) * w).sum()),\n",
    "    }\n",
    "    out [\"m3_cos_mean_min\"] = float (df_m3[\"cos_mean\"].min())\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires:\n",
    "# - build_knn_sparse_optimized(...)\n",
    "# - scipy_to_igraph_light(...)  (sin enforce_connectivity)\n",
    "# - run_leiden_one_robust(...)  (la versi√≥n robusta ‚Äúuna resoluci√≥n‚Äù)\n",
    "# - cluster_separation_fast(...), coverage_purity(...), max_share(...), gini_of_sizes(...)\n",
    "# - summarize_sizes(...), summarize_m1(...), summarize_m3(...)\n",
    "# - add_scientific_metrics(...)\n",
    "\n",
    "\n",
    "def compare_configs_leiden(\n",
    "    Embeddings,\n",
    "    configs=((40, 6.0), (40, 7.0)),\n",
    "    seed=42,\n",
    "    save_prefix=\"COMPARE_LEIDEN\",\n",
    "    # Robust Leiden\n",
    "    n_restarts=3,\n",
    "    n_iterations=2,\n",
    "    # kNN / graph\n",
    "    knn_jobs=8,\n",
    "    knn_similarity_threshold=\"auto\",\n",
    "    ig_weight_threshold=\"auto\",\n",
    "    ig_min_weight_quantile=0.75,\n",
    "    # IO / memory\n",
    "    store_details=False,          # << default False: evita guardar df grandes\n",
    "    save_details=False,\n",
    "    verbose_graph=True,\n",
    "    verbose_igraph=True,\n",
    "    # Debug\n",
    "    debug_checkpoints=True,       # << imprime etapa exacta\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust + safer compare loop.\n",
    "    Key changes:\n",
    "      - uses run_leiden_one_robust (single resolution, restarts)\n",
    "      - does NOT store big dfs in details unless store_details=True\n",
    "      - checkpoint prints to locate crash stage\n",
    "      - forces float32 embeddings to reduce peaks and avoid weird broadcasting\n",
    "    \"\"\"\n",
    "    # ---- 0) Sanity & types ----\n",
    "    X = np.asarray(Embeddings, dtype=np.float32)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"Embeddings must be 2D array, got shape={X.shape}\")\n",
    "    N, D = X.shape\n",
    "    if debug_checkpoints:\n",
    "        print(f\"[DEBUG] Embeddings shape: N={N}, D={D}, dtype={X.dtype}\")\n",
    "\n",
    "    if len(configs) == 0:\n",
    "        raise ValueError(\"configs is empty\")\n",
    "\n",
    "    out_dir = os.path.dirname(save_prefix)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- 1) Build graph per k (store only G to avoid memory peaks) ----\n",
    "    ks = sorted(set(int(k) for k, _ in configs))\n",
    "    graphs_by_k = {}\n",
    "\n",
    "    for k in ks:\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: build_graph k={k}\")\n",
    "        W = build_knn_sparse_optimized(\n",
    "            X, k=int(k), metric=\"cosine\",\n",
    "            similarity_threshold=knn_similarity_threshold,\n",
    "            symmetrization=\"max\",\n",
    "            n_jobs=knn_jobs,\n",
    "            verbose=verbose_graph\n",
    "        )\n",
    "\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: to_igraph k={k} (W.nnz={W.nnz})\")\n",
    "        G = scipy_to_igraph_light(\n",
    "            W,\n",
    "            weight_threshold=ig_weight_threshold,\n",
    "            min_weight_quantile=ig_min_weight_quantile,\n",
    "            relax_quantiles=None,\n",
    "            min_keep_per_node=1,                     # asegura >=2 edges por nodo (recomendado 2‚Äì5)\n",
    "            per_node_candidates_cap=30,              # solo mira hasta top-30 por nodo al reinyectar\n",
    "            min_reinject_weight=None,                # None o float (ej 0.0, o p10 de kNN sims)\n",
    "            verbose=verbose_igraph,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        graphs_by_k[int(k)] = G\n",
    "\n",
    "        # free W aggressively\n",
    "        del W\n",
    "        gc.collect()\n",
    "\n",
    "    # ---- 2) Iterate configs ----\n",
    "    summary_rows = []\n",
    "    details = {}\n",
    "\n",
    "    for idx_cfg, (k, res) in enumerate(configs):\n",
    "        k = int(k)\n",
    "        res = float(res)\n",
    "        G = graphs_by_k[k]\n",
    "\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: leiden (k={k}, res={res})\")\n",
    "\n",
    "        labels, part, meta = run_leiden_one_robust(\n",
    "            G, resolution=res,\n",
    "            n_restarts=int(n_restarts),\n",
    "            seed=seed,\n",
    "            n_iterations=int(n_iterations)\n",
    "        )\n",
    "\n",
    "        meta_clean = {\n",
    "            \"n_clusters\": int(meta.get(\"n_clusters\", -1)),\n",
    "            \"largest\": int(meta.get(\"largest\", -1)),\n",
    "            \"median_size\": int(meta.get(\"median_size\", -1)),\n",
    "            \"small_leq_10\": int(meta.get(\"small_leq_10\", -1)),\n",
    "            \"modularity_like\": float(meta.get(\"quality\", np.nan)),\n",
    "        }\n",
    "\n",
    "        # ---- Metrics block with checkpoints ----\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: sizes (k={k}, res={res})\")\n",
    "        sizes_sum = summarize_sizes(labels)\n",
    "        max_share_val = float(max_share(labels))\n",
    "        gini_sizes_val = float(gini_of_sizes(labels))\n",
    "\n",
    "        # m1\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: m1 cluster_separation_fast (k={k}, res={res})\")\n",
    "        df_m1 = cluster_separation_fast(X, labels)\n",
    "        m1_sum = summarize_m1(df_m1)\n",
    "\n",
    "        # m3\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: m3 coverage_purity (k={k}, res={res})\")\n",
    "        df_m3 = coverage_purity(X, labels, thresholds=(0.5, 0.6, 0.7))\n",
    "        m3_sum = summarize_m3(df_m3)\n",
    "\n",
    "        # scientific metrics\n",
    "        if debug_checkpoints:\n",
    "            print(f\"[DEBUG] Stage: scientific add_scientific_metrics (k={k}, res={res})\")\n",
    "        scientific_metrics = add_scientific_metrics(labels, X)\n",
    "\n",
    "        row = {\n",
    "            \"k\": k,\n",
    "            \"resolution\": res,\n",
    "            # leiden meta\n",
    "            **meta_clean,\n",
    "            # summaries\n",
    "            **sizes_sum,\n",
    "            **m1_sum,\n",
    "            **m3_sum,\n",
    "            \"max_share\": max_share_val,\n",
    "            \"gini\": gini_sizes_val,\n",
    "            **scientific_metrics,\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        # Store minimal details unless explicitly requested\n",
    "        if store_details:\n",
    "            details[(k, res)] = {\n",
    "                \"labels\": labels,\n",
    "                \"partition\": part,\n",
    "                \"meta\": meta_clean,\n",
    "                \"first_mesure\": df_m1,\n",
    "                \"third_mesure\": df_m3\n",
    "            }\n",
    "        else:\n",
    "            details[(k, res)] = {\"meta\": meta_clean}  # m√≠nimo\n",
    "\n",
    "        # Optional saving of heavy dfs (off by default)\n",
    "        if save_details:\n",
    "            df_m1.to_csv(f\"{save_prefix}_M1_K{k}_res{res}.csv\", index=False)\n",
    "            df_m3.to_csv(f\"{save_prefix}_M3_K{k}_res{res}.csv\", index=False)\n",
    "\n",
    "        # free big objects each loop\n",
    "        del labels, part, df_m1, df_m3, scientific_metrics\n",
    "        gc.collect()\n",
    "\n",
    "        # periodic GC\n",
    "        if (idx_cfg + 1) % 5 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    df_summary = (\n",
    "        pd.DataFrame(summary_rows)\n",
    "        .sort_values([\"k\", \"resolution\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_summary.to_csv(f\"{save_prefix}_SUMMARY.csv\", index=False)\n",
    "\n",
    "    # ---- Ranking (safe get) ----\n",
    "    df_rank = (\n",
    "        df_summary\n",
    "        .assign(\n",
    "            score=(\n",
    "                + 2.0 * df_summary.get(\"m1_margin_mean\", 0)\n",
    "                + 1.5 * df_summary.get(\"m3_cos_mean_w\", 0)\n",
    "                - 0.8 * df_summary.get(\"m3_share_below_0.6_w\", 0)\n",
    "                - 0.2 * (df_summary.get(\"small_leq_10\", 0) / df_summary.get(\"n_clusters\", 1).clip(lower=1))\n",
    "                + 1.0 * df_summary.get(\"silhouette\", 0)\n",
    "                - 0.3 * df_summary.get(\"davies_bouldin\", 0)\n",
    "            )\n",
    "        )\n",
    "        .sort_values(\"score\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_rank.to_csv(f\"{save_prefix}_RANK.csv\", index=False)\n",
    "\n",
    "    print(\"\\n=== TOP 5 CONFIGS (composed score) ===\")\n",
    "    cols = [c for c in [\n",
    "        \"k\",\"resolution\",\"n_clusters\",\"largest\",\"median_size\",\"small_leq_10\",\n",
    "        \"m1_margin_mean\",\"m3_cos_mean_w\",\"m3_share_below_0.6_w\",\"max_share\",\"gini\",\n",
    "        \"silhouette\",\"calinski_harabasz\",\"score\"\n",
    "    ] if c in df_rank.columns]\n",
    "    print(df_rank.head(5)[cols])\n",
    "\n",
    "    return df_summary, details, df_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Experiment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "[(10, 0.1), (10, 0.5), (10, 1), (10, 2), (10, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Second Experiment implementation\n",
    "from itertools import product\n",
    "ks= (10, 15, 20, 30, 40, 50, 60, 70)\n",
    "resolutions=(0.1, 0.5, 1, 2, 3, 5, 8, 10, 11, 12, 14, 16, 18, 20)\n",
    "\n",
    "#Configs = [(50, 5), (50, 10), (50, 15)]\n",
    "Configs = list(product(ks, resolutions))\n",
    "print (len(Configs))\n",
    "print (Configs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: TEST_EMB_HARD\n",
      "[DEBUG] Embeddings shape: N=100044, D=768, dtype=float32\n",
      "[DEBUG] Stage: build_graph k=10\n",
      "Building KNN graph: N=100044, k=10, metric=cosine\n",
      "  Result: edges=784247, density=0.00015671, mean_degree‚âà15.68\n",
      "  Edge weights: mean=0.702, min=0.387, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=10 (W.nnz=1568495)\n",
      "Converting to igraph: nnz=1568495, N=100044\n",
      "  Weight stats (upper): mean=0.702, median=0.700, p75=0.739\n",
      "  thr=-inf => base edges=784167 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=9, p10=10, p50=13, p90=24, max=156\n",
      "  Final: thr=-inf => V=100044, E=784167, components=1\n",
      "  Weight stats (kept): mean=0.702, min=0.387, max=1.000\n",
      "[DEBUG] Stage: build_graph k=15\n",
      "Building KNN graph: N=100044, k=15, metric=cosine\n",
      "  Result: edges=1175835, density=0.00023496, mean_degree‚âà23.51\n",
      "  Edge weights: mean=0.692, min=0.352, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=15 (W.nnz=2351670)\n",
      "Converting to igraph: nnz=2351670, N=100044\n",
      "  Weight stats (upper): mean=0.692, median=0.690, p75=0.729\n",
      "  thr=-inf => base edges=1175748 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=15, p10=15, p50=20, p90=36, max=225\n",
      "  Final: thr=-inf => V=100044, E=1175748, components=1\n",
      "  Weight stats (kept): mean=0.692, min=0.352, max=1.000\n",
      "[DEBUG] Stage: build_graph k=20\n",
      "Building KNN graph: N=100044, k=20, metric=cosine\n",
      "  Result: edges=1567133, density=0.00031315, mean_degree‚âà31.33\n",
      "  Edge weights: mean=0.684, min=0.342, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=20 (W.nnz=3134267)\n",
      "Converting to igraph: nnz=3134267, N=100044\n",
      "  Weight stats (upper): mean=0.684, median=0.683, p75=0.721\n",
      "  thr=-inf => base edges=1567044 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=20, p10=20, p50=26, p90=48, max=295\n",
      "  Final: thr=-inf => V=100044, E=1567044, components=1\n",
      "  Weight stats (kept): mean=0.684, min=0.342, max=1.000\n",
      "[DEBUG] Stage: build_graph k=30\n",
      "Building KNN graph: N=100044, k=30, metric=cosine\n",
      "  Result: edges=2345006, density=0.00046859, mean_degree‚âà46.88\n",
      "  Edge weights: mean=0.674, min=0.325, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=30 (W.nnz=4690012)\n",
      "Converting to igraph: nnz=4690012, N=100044\n",
      "  Weight stats (upper): mean=0.674, median=0.673, p75=0.711\n",
      "  thr=-inf => base edges=2344917 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=30, p10=31, p50=40, p90=72, max=404\n",
      "  Final: thr=-inf => V=100044, E=2344917, components=1\n",
      "  Weight stats (kept): mean=0.674, min=0.325, max=1.000\n",
      "[DEBUG] Stage: build_graph k=40\n",
      "Building KNN graph: N=100044, k=40, metric=cosine\n",
      "  Result: edges=3118577, density=0.00062317, mean_degree‚âà62.34\n",
      "  Edge weights: mean=0.666, min=0.320, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=40 (W.nnz=6237154)\n",
      "Converting to igraph: nnz=6237154, N=100044\n",
      "  Weight stats (upper): mean=0.666, median=0.665, p75=0.704\n",
      "  thr=-inf => base edges=3118487 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=40, p10=41, p50=53, p90=95, max=508\n",
      "  Final: thr=-inf => V=100044, E=3118487, components=1\n",
      "  Weight stats (kept): mean=0.666, min=0.320, max=1.000\n",
      "[DEBUG] Stage: build_graph k=50\n",
      "Building KNN graph: N=100044, k=50, metric=cosine\n",
      "  Result: edges=3888869, density=0.00077710, mean_degree‚âà77.74\n",
      "  Edge weights: mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=50 (W.nnz=7777738)\n",
      "Converting to igraph: nnz=7777738, N=100044\n",
      "  Weight stats (upper): mean=0.660, median=0.659, p75=0.698\n",
      "  thr=-inf => base edges=3888778 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=50, p10=52, p50=66, p90=118, max=599\n",
      "  Final: thr=-inf => V=100044, E=3888778, components=1\n",
      "  Weight stats (kept): mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: build_graph k=60\n",
      "Building KNN graph: N=100044, k=50, metric=cosine\n",
      "  Result: edges=3888869, density=0.00077710, mean_degree‚âà77.74\n",
      "  Edge weights: mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=60 (W.nnz=7777738)\n",
      "Converting to igraph: nnz=7777738, N=100044\n",
      "  Weight stats (upper): mean=0.660, median=0.659, p75=0.698\n",
      "  thr=-inf => base edges=3888778 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=50, p10=52, p50=66, p90=118, max=599\n",
      "  Final: thr=-inf => V=100044, E=3888778, components=1\n",
      "  Weight stats (kept): mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: build_graph k=70\n",
      "Building KNN graph: N=100044, k=50, metric=cosine\n",
      "  Result: edges=3888869, density=0.00077710, mean_degree‚âà77.74\n",
      "  Edge weights: mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: to_igraph k=70 (W.nnz=7777738)\n",
      "Converting to igraph: nnz=7777738, N=100044\n",
      "  Weight stats (upper): mean=0.660, median=0.659, p75=0.698\n",
      "  thr=-inf => base edges=3888778 (pre min-degree)\n",
      "  Degree stats (base): isolated=0 (0.00%), min=50, p10=52, p50=66, p90=118, max=599\n",
      "  Final: thr=-inf => V=100044, E=3888778, components=1\n",
      "  Weight stats (kept): mean=0.660, min=0.314, max=1.000\n",
      "[DEBUG] Stage: leiden (k=10, res=0.1)\n",
      "[DEBUG] Stage: sizes (k=10, res=0.1)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=0.1)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=0.1)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=0.1)\n",
      "[DEBUG] Stage: leiden (k=10, res=0.5)\n",
      "[DEBUG] Stage: sizes (k=10, res=0.5)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=0.5)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=0.5)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=0.5)\n",
      "[DEBUG] Stage: leiden (k=10, res=1.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=1.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=1.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=1.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=1.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=2.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=2.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=2.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=2.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=2.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=3.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=3.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=3.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=3.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=3.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=5.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=5.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=5.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=5.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=5.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=8.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=8.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=8.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=8.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=8.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=10.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=10.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=10.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=10.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=10.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=11.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=11.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=11.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=11.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=11.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=12.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=12.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=12.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=12.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=12.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=14.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=14.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=14.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=14.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=14.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=16.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=16.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=16.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=16.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=16.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=18.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=18.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=18.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=18.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=18.0)\n",
      "[DEBUG] Stage: leiden (k=10, res=20.0)\n",
      "[DEBUG] Stage: sizes (k=10, res=20.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=10, res=20.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=10, res=20.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=10, res=20.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=0.1)\n",
      "[DEBUG] Stage: sizes (k=15, res=0.1)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=0.1)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=0.1)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=0.1)\n",
      "[DEBUG] Stage: leiden (k=15, res=0.5)\n",
      "[DEBUG] Stage: sizes (k=15, res=0.5)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=0.5)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=0.5)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=0.5)\n",
      "[DEBUG] Stage: leiden (k=15, res=1.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=1.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=1.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=1.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=1.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=2.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=2.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=2.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=2.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=2.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=3.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=3.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=3.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=3.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=3.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=5.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=5.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=5.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=5.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=5.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=8.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=8.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=8.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=8.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=8.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=10.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=10.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=10.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=10.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=10.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=11.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=11.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=11.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=11.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=11.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=12.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=12.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=12.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=12.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=12.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=14.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=14.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=14.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=14.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=14.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=16.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=16.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=16.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=16.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=16.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=18.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=18.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=18.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=18.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=18.0)\n",
      "[DEBUG] Stage: leiden (k=15, res=20.0)\n",
      "[DEBUG] Stage: sizes (k=15, res=20.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=15, res=20.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=15, res=20.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=15, res=20.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=0.1)\n",
      "[DEBUG] Stage: sizes (k=20, res=0.1)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=0.1)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=0.1)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=0.1)\n",
      "[DEBUG] Stage: leiden (k=20, res=0.5)\n",
      "[DEBUG] Stage: sizes (k=20, res=0.5)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=0.5)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=0.5)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=0.5)\n",
      "[DEBUG] Stage: leiden (k=20, res=1.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=1.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=1.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=1.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=1.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=2.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=2.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=2.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=2.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=2.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=3.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=3.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=3.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=3.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=3.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=5.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=5.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=5.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=5.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=5.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=8.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=8.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=8.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=8.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=8.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=10.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=10.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=10.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=10.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=10.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=11.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=11.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=11.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=11.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=11.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=12.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=12.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=12.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=12.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=12.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=14.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=14.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=14.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=14.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=14.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=16.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=16.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=16.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=16.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=16.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=18.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=18.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=18.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=18.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=18.0)\n",
      "[DEBUG] Stage: leiden (k=20, res=20.0)\n",
      "[DEBUG] Stage: sizes (k=20, res=20.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=20, res=20.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=20, res=20.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=20, res=20.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=0.1)\n",
      "[DEBUG] Stage: sizes (k=30, res=0.1)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=0.1)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=0.1)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=0.1)\n",
      "[DEBUG] Stage: leiden (k=30, res=0.5)\n",
      "[DEBUG] Stage: sizes (k=30, res=0.5)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=0.5)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=0.5)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=0.5)\n",
      "[DEBUG] Stage: leiden (k=30, res=1.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=1.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=1.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=1.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=1.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=2.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=2.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=2.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=2.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=2.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=3.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=3.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=3.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=3.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=3.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=5.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=5.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=5.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=5.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=5.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=8.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=8.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=8.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=8.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=8.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=10.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=10.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=10.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=10.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=10.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=11.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=11.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=11.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=11.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=11.0)\n",
      "[DEBUG] Stage: leiden (k=30, res=12.0)\n",
      "[DEBUG] Stage: sizes (k=30, res=12.0)\n",
      "[DEBUG] Stage: m1 cluster_separation_fast (k=30, res=12.0)\n",
      "[DEBUG] Stage: m3 coverage_purity (k=30, res=12.0)\n",
      "[DEBUG] Stage: scientific add_scientific_metrics (k=30, res=12.0)\n"
     ]
    }
   ],
   "source": [
    "n_comp = 0.7\n",
    "\n",
    "for i in range (2,5):\n",
    "    print(f\"Working on: {DIR[1][i]}\")\n",
    "\n",
    "    Embeddings  = pd.read_parquet(DIR[0][i])[\"chunk_emb\"].tolist()\n",
    "    Texts       = pd.read_parquet(DIR[0][i])[\"chunk_text\"].tolist()\n",
    "\n",
    "    Embeddings_SIF = sif_all_but_the_top_vectorized(embeddings=Embeddings, n_components=n_comp)\n",
    "\n",
    "    df_summary, details, df_rank = compare_configs_leiden(\n",
    "    Embeddings  = Embeddings_SIF,\n",
    "    configs     = Configs,   # tuples (k, res)\n",
    "    seed        = 42,\n",
    "    save_prefix = f\"04_Emb & K_Selection/COMPARE_LEIDEN_{DIR[1][i]}_COMP_{n_comp}\",\n",
    "    # Robust Leiden\n",
    "    n_restarts  = 3,\n",
    "    n_iterations= 2,\n",
    "    # kNN / graph\n",
    "    knn_jobs    = 8,\n",
    "    knn_similarity_threshold    = None,\n",
    "    ig_weight_threshold         = None,\n",
    "    ig_min_weight_quantile      = 0.0,\n",
    "    # IO\n",
    "    save_details                = False,\n",
    "    verbose_graph               = True,\n",
    "    verbose_igraph              = True\n",
    "    )\n",
    "\n",
    "    print(f\"{DIR[1][i]} done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings_SIF = sif_all_but_the_top_vectorized(embeddings=Embeddings, n_components=0.65)\n",
    "\n",
    "W = build_knn_sparse_optimized(Embeddings_SIF, k=30, metric=\"cosine\",\n",
    "                               similarity_threshold=\"auto\",\n",
    "                               symmetrization=\"max\", n_jobs=8, verbose=True)\n",
    "print(W.shape, W.nnz, W.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = scipy_to_igraph_light(W, weight_threshold=\"auto\", min_weight_quantile=0.75, verbose=True)\n",
    "print(G.vcount(), G.ecount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, part, meta = run_leiden_one_robust(G, resolution=5.0, n_restarts=3, seed=42, n_iterations=2)\n",
    "print(meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, scipy\n",
    "import igraph, leidenalg\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"scipy\", scipy.__version__)\n",
    "print(\"igraph\", igraph.__version__)\n",
    "print(\"leidenalg\", leidenalg.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 LEIDEN ANALYSE OF RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_csv(f\"04_Tests/{DIR[1][0]}_SUMMARY.csv\")\n",
    "df_0[\"emb_family\"] = DIR[1][0]\n",
    "\n",
    "df_1 = pd.read_csv(f\"04_Tests/{DIR[1][1]}_SUMMARY.csv\")\n",
    "df_1[\"emb_family\"] = DIR[1][1]\n",
    "\n",
    "df_2 = pd.read_csv(f\"04_Tests/{DIR[1][2]}_SUMMARY.csv\")\n",
    "df_2[\"emb_family\"] = DIR[1][2]\n",
    "\n",
    "df_3 = pd.read_csv(f\"04_Tests/{DIR[1][3]}_SUMMARY.csv\")\n",
    "df_3[\"emb_family\"] = DIR[1][3]\n",
    "\n",
    "df_4 = pd.read_csv(f\"04_Tests/{DIR[1][4]}_SUMMARY.csv\")\n",
    "df_4[\"emb_family\"] = DIR[1][4]\n",
    "\n",
    "df_5 = pd.read_csv(f\"04_Tests/{DIR[1][5]}_SUMMARY.csv\")\n",
    "df_5[\"emb_family\"] = DIR[1][5]\n",
    "\n",
    "df_6 = pd.read_csv(f\"04_Tests/{DIR[1][6]}_SUMMARY.csv\")\n",
    "df_6[\"emb_family\"] = DIR[1][6]\n",
    "\n",
    "df_7 = pd.read_csv(f\"04_Tests/{DIR[1][7]}_SUMMARY.csv\")\n",
    "df_7[\"emb_family\"] = DIR[1][7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_df = pd.concat([df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7],\n",
    "               ignore_index=True\n",
    "               )\n",
    "TEST_df[\"small_leq_10_pc\"] = TEST_df[\"small_leq_10\"] / TEST_df[\"n_clusters\"]\n",
    "display(TEST_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 EMBEDDINGS SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_df = pd.concat([df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7],\n",
    "               ignore_index=True\n",
    "               )\n",
    "TEST_df[\"small_leq_10_pc\"] = TEST_df[\"small_leq_10\"] / TEST_df[\"n_clusters\"]\n",
    "display(TEST_df)\n",
    "\n",
    "# POST METRICS\n",
    "\n",
    "# Metrics to maximize\n",
    "metrics_pos =[\n",
    "    \"m1_margin_mean\",\n",
    "    \"silhouette\",\n",
    "    \"calinski_harabasz\",\n",
    "    \"coverage_0.7\",\n",
    "    \"m3_cos_mean_min\"\n",
    "]\n",
    "\n",
    "# Metrics to minimize\n",
    "metrics_neg =[\n",
    "    \"davies_bouldin\",\n",
    "    \"gini\"\n",
    "]\n",
    "small_10 = [\n",
    "    \"small_leq_10_pc\"\n",
    "]\n",
    "\n",
    "metrics_all = metrics_pos + metrics_neg + small_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalization [0,1]\n",
    "TEST_norm = TEST_df.copy()\n",
    "\n",
    "for m in metrics_pos:\n",
    "    TEST_norm[m+\"_norm\"] = (TEST_df[m] - TEST_df[m].min()) / (TEST_df[m].max() - TEST_df[m].min())\n",
    "\n",
    "for m in metrics_neg:\n",
    "    tmp = (TEST_df[m] - TEST_df[m].min()) / (TEST_df[m].max() - TEST_df[m].min())\n",
    "    TEST_norm[m+\"_norm\"] = 1 - tmp\n",
    "\n",
    "for m in small_10:\n",
    "    min_10 = 0.03\n",
    "    max_10 = 0.10\n",
    "\n",
    "    TEST_norm[m+\"_norm\"] = np.where (\n",
    "        TEST_df[m] > max_10,\n",
    "        0,\n",
    "        np.where (\n",
    "            TEST_df[m] <= min_10,\n",
    "            1,\n",
    "            1 - ((TEST_df[m] - min_10) / (max_10 - min_10))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 2. Compossed index\n",
    "weights = {m+\"_norm\": 1 for m in metrics_all}\n",
    "\n",
    "def compute_Q (row):\n",
    "    return sum(row[m] * w for m, w in weights.items())\n",
    "\n",
    "TEST_norm[\"Q\"] = TEST_norm.apply(compute_Q, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_df.to_clipboard()\n",
    "display(TEST_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability metric (stab)\n",
    "\n",
    "def stability_metric (group, col):\n",
    "    mean = group[col].mean()\n",
    "    std = group[col].std()\n",
    "    cv = std / (mean + 1e-9)\n",
    "    return 1 / (1 + cv)\n",
    "\n",
    "stab_results = (\n",
    "    TEST_norm.groupby([\"emb_family\", \"k\"])\n",
    "    .apply(lambda g: {m+\"_stab\": stability_metric(g, m+\"_norm\") for m in metrics_all})\n",
    "    .apply(pd.Series)\n",
    "    .reset_index()\n",
    "    )\n",
    "display(stab_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-Stability Index S(e,k)\n",
    "\n",
    "Q_stats = TEST_norm.groupby([\"emb_family\", \"k\"])[\"Q\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "Q_stats.rename(columns={\"mean\": \"Q_mean\", \"std\": \"Q_std\"}, inplace=True)\n",
    "\n",
    "Q_stats[\"S_e_k\"] = Q_stats[\"Q_mean\"] / (1 + Q_stats[\"Q_std\"])\n",
    "\n",
    "Q_stats.to_clipboard()\n",
    "display(Q_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global index\n",
    "\n",
    "def p90(x):\n",
    "    return x.quantile(0.9)\n",
    "\n",
    "ranking_emb = (\n",
    "    Q_stats.\n",
    "    groupby(\"emb_family\")[\"S_e_k\"]\n",
    "    .agg(\n",
    "        Q_mean = \"median\",\n",
    "        Q_p90 = p90,\n",
    "        Q_std = \"std\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ranking_emb[\"potential\"] = ranking_emb[\"Q_p90\"] - ranking_emb[\"Q_mean\"]\n",
    "ranking_emb[\"Total\"] = (\n",
    "    ranking_emb[\"Q_mean\"] + \n",
    "    ranking_emb[\"potential\"] -\n",
    "    ranking_emb[\"Q_std\"]\n",
    ")\n",
    "\n",
    "ranking_emb = ranking_emb.sort_values(\"Total\", ascending = False)\n",
    "\n",
    "display(ranking_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPHICS EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global configuration\n",
    "\n",
    "# Style for paper\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\"        : 150,\n",
    "    \"figure.figsize\"    : (8, 5),\n",
    "    \"font.size\"         : 11,\n",
    "    \"axes.labelsize\"    : 11,\n",
    "    \"axes.titlesize\"    : 12,\n",
    "    \"xtick.labelsize\"   : 9,\n",
    "    \"ytick.labelsize\"   : 9,\n",
    "    \"legend.fontsize\"   : 9,\n",
    "    \"axes.spines.top\"   : False,\n",
    "    \"axes.spines.right\" : False    \n",
    "    })\n",
    "\n",
    "emb_order = ranking_emb.sort_values(\"Total\", ascending=False)[\"emb_family\"].tolist()\n",
    "\n",
    "all_SeK = Q_stats[\"S_e_k\"].values\n",
    "ymin, ymax = all_SeK.min() - 0.1, all_SeK.max() +0.1\n",
    "\n",
    "data = [Q_stats[Q_stats[\"emb_family\"] == emb] [\"S_e_k\"].values\n",
    "        for emb in emb_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.A.  VIOLIN PLOT (PAPER) üëçüèºüëçüèº\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Basic violin\n",
    "parts = ax.violinplot (\n",
    "    data,\n",
    "    positions = np.arange(len(emb_order)),\n",
    "    showmeans = False,\n",
    "    showmedians = False,\n",
    "    showextrema = False\n",
    ")\n",
    "\n",
    "# Adjust transparency (optional)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_alpha(0.4)\n",
    "\n",
    "# Statistic values\n",
    "medians = [np.median(d) for d in data]\n",
    "means   = [np.mean(d) for d in data]\n",
    "\n",
    "xpos = np.arange(len(emb_order))\n",
    "\n",
    "# Line and median\n",
    "for x, vals in zip(xpos, data):\n",
    "    q1, q3 = np.percentile (vals, [25, 75])\n",
    "    ax.vlines (x, q1, q3, linewidth=1)\n",
    "    ax.hlines (medians[x], x - 0.1, x + 0.1, linewidth=1)\n",
    "\n",
    "# Mean as a point\n",
    "ax.scatter (xpos, means, marker=\"o\", s=15, zorder=3)\n",
    "\n",
    "ax.set_xticks(xpos)\n",
    "ax.set_xticklabels(emb_order, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_ylabel(r\"$S_{e,k}$ (quality-stability index)\")\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title(r\"$S_{e,k}$ distribution by family of embeddings\")\n",
    "\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.A BOXPLOT PAPER üëçüèºüëçüèº\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Boxplot with means\n",
    "bp = ax.boxplot(\n",
    "    data,\n",
    "    positions = np.arange(len(emb_order)),\n",
    "    showmeans = True,\n",
    "    meanline = False,\n",
    "    widths = 0.6\n",
    ")\n",
    "\n",
    "# Make the lines thiner (Optional)\n",
    "for element in ['boxes', 'whiskers', 'caps', 'medians', 'means']:\n",
    "    for line in bp[element]:\n",
    "        line.set_color(\"navy\")\n",
    "        line.set_linewidth(0.6)\n",
    "\n",
    "ax.set_xticks (np.arange(len(emb_order)))\n",
    "ax.set_xticklabels (emb_order, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_ylabel (r\"$S_{e, k}$ (quality-stability index)\")\n",
    "ax.set_ylim (ymin, ymax)\n",
    "ax.set_title (r\"$S_{e, k}$ by family of embeddings\")\n",
    "\n",
    "ax.grid (axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.A. SCATTER \"QUALITY vs STABILITY\" (PAPER) üëçüèºüëçüèº\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = ranking_emb[\"Q_std\"].values\n",
    "y = ranking_emb[\"Q_mean\"].values\n",
    "labels = ranking_emb[\"emb_family\"].tolist()\n",
    "\n",
    "ax.scatter (x, y, s=35)\n",
    "\n",
    "for xi, yi, lab in zip(x, y, labels):\n",
    "    ax.text(\n",
    "        xi + 0.002, yi + 0.005, lab,\n",
    "        fontsize=8, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "\n",
    "ax.set_xlabel (r\"$\\mathrm{std}(S_{e, k})$ (stability - lower is better)\")\n",
    "ax.set_ylabel (r\"$\\mathrm{median}(S_{e, k})$ (base quality - higher is better)\")\n",
    "ax.set_title (\"Quality vs stability by embeddings family\")\n",
    "\n",
    "x_margin = (x.max() - x.min()) * 0.15\n",
    "y_margin = (x.max() - x.min()) * 0.10\n",
    "ax.set_xlim(x.min() - x_margin, x.max() + x_margin)\n",
    "ax.set_ylim(y.min() - y_margin, y.max() + y_margin)\n",
    "\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BEESWARM / STRIPPLOT\n",
    "\n",
    "plt.figure (figsize = (8, 5))\n",
    "\n",
    "for i , emb in enumerate (emb_order):\n",
    "    sub = Q_stats[Q_stats[\"emb_family\"] == emb][\"S_e_k\"].values\n",
    "    # Jitter in x axe\n",
    "    x = np.random.normal(loc=i+1, scale=0.04, size=len(sub))\n",
    "    plt.scatter(x, sub, alpha=0.6, s=20)\n",
    "\n",
    "plt.xticks (\n",
    "    ticks = np.arange(1, len(emb_order) + 1),\n",
    "    labels = emb_order,\n",
    "    rotation = 45,\n",
    "    ha = \"right\"\n",
    ")\n",
    "\n",
    "plt.ylabel (\"S_e_k\")\n",
    "plt.title (\"S_e_k distribution by embedding (beeswarm)\")\n",
    "plt.grid (True, linestyle=\"--\", alpha = 0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 K SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_df_k = pd.concat([df_2, df_4, df_6],\n",
    "               ignore_index=True\n",
    "               )\n",
    "TEST_df_k[\"small_leq_10_pc\"] = TEST_df_k[\"small_leq_10\"] / TEST_df_k[\"n_clusters\"]\n",
    "display(TEST_df_k)\n",
    "\n",
    "\n",
    "# POST METRICS\n",
    "\n",
    "# Metrics to maximize\n",
    "metrics_pos =[\n",
    "    \"m1_margin_mean\",\n",
    "    \"silhouette\",\n",
    "    \"calinski_harabasz\",\n",
    "    \"coverage_0.7\",\n",
    "    \"m3_cos_mean_min\"\n",
    "]\n",
    "\n",
    "# Metrics to minimize\n",
    "metrics_neg =[\n",
    "    \"davies_bouldin\",\n",
    "    \"gini\"\n",
    "]\n",
    "small_10 = [\n",
    "    \"small_leq_10_pc\"\n",
    "]\n",
    "\n",
    "metrics_all = metrics_pos + metrics_neg + small_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalization [0,1]\n",
    "TEST_norm_k = TEST_df_k.copy()\n",
    "\n",
    "for m in metrics_pos:\n",
    "    TEST_norm_k[m+\"_norm\"] = (TEST_df_k[m] - TEST_df_k[m].min()) / (TEST_df_k[m].max() - TEST_df_k[m].min())\n",
    "\n",
    "for m in metrics_neg:\n",
    "    tmp = (TEST_df_k[m] - TEST_df_k[m].min()) / (TEST_df_k[m].max() - TEST_df_k[m].min())\n",
    "    TEST_norm_k[m+\"_norm\"] = 1 - tmp\n",
    "\n",
    "for m in small_10:\n",
    "    min_10 = 0.03\n",
    "    max_10 = 0.10\n",
    "\n",
    "    TEST_norm_k[m+\"_norm\"] = np.where (\n",
    "        TEST_df_k[m] > max_10,\n",
    "        0,\n",
    "        np.where (\n",
    "            TEST_df_k[m] <= min_10,\n",
    "            1,\n",
    "            1 - ((TEST_df_k[m] - min_10) / (max_10 - min_10))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 2. Compossed index\n",
    "weights = {m+\"_norm\": 1 for m in metrics_all}\n",
    "\n",
    "def compute_Q (row):\n",
    "    return sum(row[m] * w for m, w in weights.items())\n",
    "\n",
    "TEST_norm_k[\"Q\"] = TEST_norm_k.apply(compute_Q, axis=1)\n",
    "\n",
    "display(TEST_norm_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability metric (stab)\n",
    "\n",
    "def stability_metric (group, col):\n",
    "    mean = group[col].mean()\n",
    "    std = group[col].std()\n",
    "    cv = std / (mean + 1e-9)\n",
    "    return 1 / (1 + cv)\n",
    "\n",
    "stab_results_k = (\n",
    "    TEST_norm_k.groupby([\"emb_family\", \"k\"])\n",
    "    .apply(lambda g: {m+\"_stab\": stability_metric(g, m+\"_norm\") for m in metrics_all})\n",
    "    .apply(pd.Series)\n",
    "    .reset_index()\n",
    "    )\n",
    "display(stab_results_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality-Stability Index S(e,k)\n",
    "\n",
    "Q_stats_k = TEST_norm_k.groupby([\"emb_family\", \"k\"])[\"Q\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "Q_stats_k.rename(columns={\"mean\": \"Q_mean\", \"std\": \"Q_std\"}, inplace=True)\n",
    "\n",
    "Q_stats_k[\"S_e_k\"] = Q_stats_k[\"Q_mean\"] / (1 + Q_stats_k[\"Q_std\"])\n",
    "\n",
    "display(Q_stats_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K index\n",
    "\n",
    "def p90(x):\n",
    "    return x.quantile(0.9)\n",
    "\n",
    "ranking_emb_k = (\n",
    "    Q_stats_k.\n",
    "    groupby(\"k\")[\"S_e_k\"]\n",
    "    .agg(\n",
    "        Q_mean = \"median\",\n",
    "        Q_p90 = p90,\n",
    "        Q_std = \"std\"\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ranking_emb_k[\"potential\"] = ranking_emb_k[\"Q_p90\"] - ranking_emb_k[\"Q_mean\"]\n",
    "ranking_emb_k[\"Total\"] = (\n",
    "    ranking_emb_k[\"Q_mean\"] + \n",
    "    ranking_emb_k[\"potential\"] -\n",
    "    ranking_emb_k[\"Q_std\"]\n",
    ")\n",
    "\n",
    "#ranking_emb_k = ranking_emb_k.sort_values(\"Total\", ascending = False)\n",
    "\n",
    "display(ranking_emb_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRAPHICS K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global configuration\n",
    "\n",
    "# Style for paper\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\"        : 150,\n",
    "    \"figure.figsize\"    : (8, 5),\n",
    "    \"font.size\"         : 11,\n",
    "    \"axes.labelsize\"    : 11,\n",
    "    \"axes.titlesize\"    : 12,\n",
    "    \"xtick.labelsize\"   : 9,\n",
    "    \"ytick.labelsize\"   : 9,\n",
    "    \"legend.fontsize\"    : 9,\n",
    "    \"axes.spines.top\"   : False,\n",
    "    \"axes.spines.right\" : False    \n",
    "    })\n",
    "\n",
    "emb_order_k = ranking_emb_k[\"k\"].tolist()\n",
    "all_SeK = Q_stats_k[\"S_e_k\"].values\n",
    "ymin, ymax = all_SeK.min() - 0.1, all_SeK.max() +0.1\n",
    "\n",
    "data_k = [Q_stats_k[Q_stats_k[\"k\"] == emb] [\"S_e_k\"].values\n",
    "        for emb in emb_order_k]\n",
    "\n",
    "display(Q_stats_k)\n",
    "display(data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.A.  VIOLIN PLOT (PAPER) üëçüèºüëçüèº\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Basic violin\n",
    "parts = ax.violinplot (\n",
    "    data_k,\n",
    "    positions = np.arange(len(emb_order_k)),\n",
    "    showmeans = False,\n",
    "    showmedians = False,\n",
    "    showextrema = False\n",
    ")\n",
    "\n",
    "# Adjust transparency (optional)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_alpha(0.4)\n",
    "\n",
    "# Statistic values\n",
    "medians = [np.median(d) for d in data_k]\n",
    "means   = [np.mean(d) for d in data_k]\n",
    "\n",
    "xpos = np.arange(len(emb_order_k))\n",
    "\n",
    "# Line and median\n",
    "for x, vals in zip(xpos, data_k):\n",
    "    q1, q3 = np.percentile (vals, [25, 75])\n",
    "    ax.vlines (x, q1, q3, linewidth=1)\n",
    "    ax.hlines (medians[x], x - 0.1, x + 0.1, linewidth=1)\n",
    "\n",
    "# Mean as a point\n",
    "ax.scatter (xpos, means, marker=\"o\", s=15, zorder=3)\n",
    "\n",
    "ax.set_xticks(xpos)\n",
    "ax.set_xticklabels(emb_order_k, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_ylabel(r\"$S_{e,k}$ (quality-stability index)\")\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title(r\"$S_{e,k}$ distribution by k\")\n",
    "\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.A BOXPLOT PAPER\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Boxplot with means\n",
    "bp = ax.boxplot(\n",
    "    data_k,\n",
    "    positions = np.arange(len(emb_order_k)),\n",
    "    showmeans = True,\n",
    "    meanline = False,\n",
    "    widths = 0.6\n",
    ")\n",
    "\n",
    "# Make the lines thiner (Optional)\n",
    "for element in ['boxes', 'whiskers', 'caps', 'medians', 'means']:\n",
    "    for line in bp[element]:\n",
    "        line.set_color(\"navy\")\n",
    "        line.set_linewidth(0.6)\n",
    "\n",
    "ax.set_xticks (np.arange(len(emb_order_k)))\n",
    "ax.set_xticklabels (emb_order_k, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_ylabel (r\"$S_{e, k}$ (quality-stability index)\")\n",
    "ax.set_ylim (ymin, ymax)\n",
    "ax.set_title (r\"$S_{e, k}$ by k\")\n",
    "\n",
    "ax.grid (axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.A. SCATTER \"QUALITY vs STABILITY\" (PAPER) üëçüèºüëçüèº\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = ranking_emb_k[\"Q_std\"].values\n",
    "y = ranking_emb_k[\"Q_mean\"].values\n",
    "labels = ranking_emb_k[\"k\"].tolist()\n",
    "\n",
    "ax.scatter (x, y, s=35)\n",
    "\n",
    "for xi, yi, lab in zip(x, y, labels):\n",
    "    ax.text(\n",
    "        xi + 0.002, yi + 0.005, lab,\n",
    "        fontsize=8, va=\"bottom\", ha=\"left\"\n",
    "    )\n",
    "\n",
    "ax.set_xlabel (r\"$\\mathrm{std}(S_{e, k})$ (stability - lower is better)\")\n",
    "ax.set_ylabel (r\"$\\mathrm{median}(S_{e, k})$ (base quality - higher is better)\")\n",
    "ax.set_title (\"Quality vs stability by k\")\n",
    "\n",
    "x_margin = (x.max() - x.min()) * 0.15\n",
    "y_margin = (x.max() - x.min()) * 0.10\n",
    "ax.set_xlim(x.min() - x_margin, x.max() + x_margin)\n",
    "ax.set_ylim(y.min() - y_margin, y.max() + y_margin)\n",
    "\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. BEESWARM / STRIPPLOT\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xpos = np.arange(len(emb_order_k))\n",
    "\n",
    "plt.figure (figsize = (8, 5))\n",
    "\n",
    "for i, embs in enumerate (data_k):\n",
    "        # Jitter in x axe\n",
    "    x_jitter = np.random.normal(loc=xpos[i], scale=0.05, size=len(embs))\n",
    "    ax.scatter(x_jitter, embs, s=30, alpha=0.7)\n",
    "\n",
    "ax.set_xticks(xpos)\n",
    "ax.set_xticklabels(emb_order_k)\n",
    "ax.set_ylabel(\"$S_{e, k}$\")\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title(r\"$S_{e, k}$ distribution by $k$ (beeswarm)\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 RESOLUTION SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_df_ = pd.concat([df_2, df_4, df_6],\n",
    "               ignore_index=True\n",
    "               )\n",
    "TEST_df_res = TEST_df_[TEST_df_['k'].isin([50,60,70,80])]\n",
    "\n",
    "TEST_df_res[\"small_leq_10_pc\"] = TEST_df_res[\"small_leq_10\"] / TEST_df_res[\"n_clusters\"]\n",
    "#display(TEST_df_res)\n",
    "\n",
    "# POST METRICS\n",
    "\n",
    "# Metrics to maximize\n",
    "metrics_pos =[\n",
    "    \"m1_margin_mean\",\n",
    "    \"silhouette\",\n",
    "    \"calinski_harabasz\",\n",
    "    \"coverage_0.7\",\n",
    "    \"m3_cos_mean_min\"\n",
    "]\n",
    "\n",
    "# Metrics to minimize\n",
    "metrics_neg =[\n",
    "    \"davies_bouldin\",\n",
    "    \"gini\"\n",
    "]\n",
    "small_10 = [\n",
    "    \"small_leq_10_pc\"\n",
    "]\n",
    "\n",
    "metrics_all = metrics_pos + metrics_neg + small_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global configuration\n",
    "\n",
    "# Style for paper\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\"        : 150,\n",
    "    \"figure.figsize\"    : (8, 5),\n",
    "    \"font.size\"         : 11,\n",
    "    \"axes.labelsize\"    : 11,\n",
    "    \"axes.titlesize\"    : 12,\n",
    "    \"xtick.labelsize\"   : 9,\n",
    "    \"ytick.labelsize\"   : 9,\n",
    "    \"legend.fontsize\"    : 9,\n",
    "    \"axes.spines.top\"   : False,\n",
    "    \"axes.spines.right\" : False    \n",
    "    })\n",
    "\n",
    "emb_order_res = TEST_df_res['resolution'].unique().tolist()\n",
    "#display(emb_order_res)\n",
    "all_n_cluster = TEST_df_res[\"n_clusters\"].values\n",
    "ymin, ymax = all_n_cluster.min() - 5, all_n_cluster.max() +5\n",
    "\n",
    "\n",
    "n_clusters = [TEST_df_res[TEST_df_res[\"resolution\"] == res] [\"n_clusters\"].values\n",
    "        for res in emb_order_res]\n",
    "#display(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BEESWARM / STRIPPLOT\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xpos = np.arange(len(emb_order_res))\n",
    "\n",
    "plt.figure (figsize = (8, 5))\n",
    "\n",
    "for i, res in enumerate (n_clusters):\n",
    "        # Jitter in x axe\n",
    "    x_jitter = np.random.normal(loc=xpos[i], scale=0.05, size=len(res))\n",
    "    ax.scatter(x_jitter, res, s=30, alpha=0.7)\n",
    "\n",
    "ax.set_xticks(xpos)\n",
    "ax.set_xticklabels(emb_order_res)\n",
    "ax.set_ylabel(\"Number of clusters\")\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_title(r\"Number of clusters by $resolution$ (beeswarm)\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "\n",
    "# Ejemplo conceptual (a√±adir despu√©s de crear ax):\n",
    "\n",
    "ax.axhspan(10, 60,  facecolor=\"grey\", alpha=0.08, label=\"Macro\")\n",
    "ax.axhspan(60, 150, facecolor=\"blue\", alpha=0.08, label=\"Meso\")\n",
    "ax.axhspan(150, 260, facecolor=\"violet\", alpha=0.08, label=\"Micro\")\n",
    "\n",
    "# (Opcional) texto en el margen derecho\n",
    "ax.text(18.3, 35,  \"Macro\", va=\"center\")\n",
    "ax.text(18.3, 105, \"Meso\",  va=\"center\")\n",
    "ax.text(18.3, 205, \"Micro\", va=\"center\")\n",
    "\n",
    "ax.axvspan(0, 3.5,  alpha=0.08, color=\"grey\")   # macro\n",
    "ax.axvspan(3.5, 11.5, alpha=0.08, color=\"blue\")   # meso\n",
    "ax.axvspan(11.5, 20.5,alpha=0.08, color=\"violet\")  # micro\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER LEIDEN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEIDEN-KNN Implementation with the best options from experimentation\n",
    "\n",
    "DIR_BTT_EMB = [DIR_EMB_ALL, DIR_EMB_ALL_LONG, DIR_EMB_ALL_LEMA_LONG, DIR_EMB_ALL_SOFT]\n",
    "DIR_BTT_EMB = pd.DataFrame(DIR_BTT_EMB)\n",
    "display(DIR_BTT_EMB)\n",
    "ks =60\n",
    "res = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "os.makedirs('05_Partitions', exist_ok = True)\n",
    "\n",
    "df=pd.read_parquet(\"03_Embeddings/Embeddings_SOFT_Ver.05.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 4):\n",
    "    print(f\"Working on: {DIR_BTT_EMB[1][i]}\")\n",
    "\n",
    "    Embeddings = pd.read_parquet(DIR_BTT_EMB[0][i])[\"chunk_emb\"].tolist()       # Embeddings Soft\n",
    "    Texts = pd.read_parquet(DIR_BTT_EMB[0][i])[\"chunk_text\"].tolist()           # Texts TEST ALL LONG\n",
    "\n",
    "    Embeddings_SIF = sif_all_but_the_top(embeddings=Embeddings, n_components=1)\n",
    "\n",
    "    W = build_knn_sparse(\n",
    "        Embeddings,\n",
    "        k=ks, \n",
    "        metric=\"cosine\", \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    G = scipy_to_igraph(W)   \n",
    "\n",
    "    # CORRECCI√ìN: enumerate devuelve (√≠ndice, valor)\n",
    "    for idx, r in enumerate(res):\n",
    "        print(f\"Executing configuration {idx+1} of {len(res)} with resolution={r}\")\n",
    "\n",
    "        labels_LEIDEN, partition_LEIDEN = run_leiden(\n",
    "            G, \n",
    "            resolution=r, \n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # CORRECCI√ìN: Usar labels_LEIDEN en lugar de labels\n",
    "        empty_embedder = BaseEmbedder()\n",
    "        empty_dimred = BaseDimensionalityReduction()\n",
    "        empty_cluster = BaseCluster()\n",
    "        ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "        vectorizer = CountVectorizer(lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "        Topic_Model_Mod = BERTopic(\n",
    "            embedding_model=empty_embedder,\n",
    "            umap_model=empty_dimred,\n",
    "            hdbscan_model=empty_cluster,\n",
    "            vectorizer_model=vectorizer,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            top_n_words=30,\n",
    "            calculate_probabilities=True,\n",
    "            verbose=True\n",
    "        )       \n",
    "\n",
    "        topics, _ = Topic_Model_Mod.fit_transform(Texts, y=labels_LEIDEN)  # CORREGIDO\n",
    "\n",
    "        TOPICS = Topic_Model_Mod.get_topic_info()\n",
    "        TOPICS.to_csv(f\"05_Topics/Topics{DIR_BTT_EMB[1][i]}_{ks}_{r}_SOFTER.csv\", sep=\";\")\n",
    "    \n",
    "        TOPICSvsDOCS = Topic_Model_Mod.get_document_info(Texts)\n",
    "        TOPICSvsDOCS.to_csv(f\"05_Topics/Topics_By_Paper{DIR_BTT_EMB[1][i]}_{ks}_{r}_SOFTER.csv\", sep=\";\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. BERTopic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_embedder = BaseEmbedder()\n",
    "empty_dimred   = BaseDimensionalityReduction()\n",
    "empty_cluster  = BaseCluster()\n",
    "ctfidf_model   = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "vectorizer = CountVectorizer(lowercase = False, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "Topic_Model_Mod = BERTopic(\n",
    "    embedding_model = empty_embedder,\n",
    "    umap_model = empty_dimred,\n",
    "    hdbscan_model = empty_cluster,\n",
    "    vectorizer_model = vectorizer,      # Current vectorizer\n",
    "    ctfidf_model = ctfidf_model,\n",
    "    top_n_words = 30,                   # \n",
    "    calculate_probabilities = True,     # Agglomerative doesn't give probs\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_LEIDEN                 # Labels according to selected path (01, 02, 03)\n",
    "\n",
    "topics, _ = Topic_Model_Mod.fit_transform(Texts, y=labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] \n",
    "ks = 60\n",
    "res = [5]\n",
    "for i, resolution in enumerate(res):\n",
    "    labels = np.load(f'05_Partitions/{ks}{i+1}_Labels_LEIDEN_{ks}_{resolution}.npy')\n",
    "    topics, _ = Topic_Model_Mod.fit_transform(Texts, y=labels)\n",
    "    \n",
    "    TOPICS = Topic_Model_Mod.get_topic_info()\n",
    "    TOPICS.to_csv(f\"06_Topics/{ks}{i+1}_TopicsPapers_All_Text_{ks}_{resolution}.csv\", sep = \";\")\n",
    "    \n",
    "    TOPICSvsDOCS = Topic_Model_Mod.get_document_info(Texts)\n",
    "    TOPICSvsDOCS.to_csv(f\"06_Topics/{ks}{i+1}_Topics_VS_Papers_All_Text_{ks}_{resolution}.csv\", sep=\";\")\n",
    "    \n",
    "    Fig_01 = Topic_Model_Mod.visualize_heatmap()\n",
    "    plt.savefig(f\"06_Topics/{ks}{i+1}_Heatmap_All_Text_{ks}_{resolution}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    Fig_02 = Topic_Model_Mod.visualize_topics()\n",
    "    plt.savefig(f\"06_Topics/{ks}{i+1}_Topics_All_Text_{ks}_{resolution}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    Fig_03 = Topic_Model_Mod.visualize_barchart()\n",
    "    plt.savefig(f\"06_Topics/{ks}{i+1}_Barchart_All_Text_{ks}_{resolution}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Graphics Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic_Model = Topic_Model_Base\n",
    "Topic_Model = Topic_Model_Mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = pd.read_csv(\"acid_test_SIF_607_PROMPT2.csv\", sep = \";\", encoding='utf-8')\n",
    "new_labels = new_labels[\"label\"].tolist()\n",
    "print(new_labels)\n",
    "Topic_Model.set_topic_labels(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_camel_case (word):\n",
    "    return re.sub(r'(?<!^)(?=[A-Z])', ' ', word).lower()\n",
    "\n",
    "Keywords_Model_Brut = Topic_Model.get_topic_info()[\"Representation\"].tolist()\n",
    "\n",
    "Keywords_Model = [\n",
    "    [split_camel_case(word) for word in group]\n",
    "    for group in Keywords_Model_Brut\n",
    "]\n",
    "print (Keywords_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_Model.visualize_hierarchy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_Model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_Model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BarchartModel = Topic_Model.visualize_barchart(top_n_topics = 80, custom_labels=True)\n",
    "BarchartModel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = Topic_Model.hierarchical_topics(Texts)\n",
    "topic_tree = Topic_Model.get_topic_tree(hierarchical_topics)\n",
    "print(hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = Topic_Model.get_topic_info()\n",
    "#TOPICS = TOPICS.drop('Representative_Docs' axis=1)\n",
    "display (TOPICS)\n",
    "TOPICS.to_csv(\"TopicsProblem_Sci_Ver04.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICSvsDOCS = Topic_Model.get_document_info(Texts)\n",
    "print(TOPICSvsDOCS)\n",
    "TOPICSvsDOCS.to_csv(\"Topics_vs_Papers_FULL_01.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "Topic_Model.save(\"allenaiTopics__ngrams_01\")\n",
    "# Save topics and probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_info = Topic_Model.get_topic_info()\n",
    "#pd.DataFrame({\n",
    "#    \"Document\": Docs_Clean,\n",
    "#    \"Topic\": TOPICS,\n",
    "#}).to_csv(\"allenai_topics.csv\", index=False)\n",
    "\n",
    "## Save everything together in a compressed file\n",
    "#full_data = {\n",
    "#    \"topics\": TOPICS,\n",
    "#    \"topic_info\": topic_info,\n",
    "#    \"docs\": Docs_Clean\n",
    "#}\n",
    "#joblib.dump(full_data, \"allenaiTopics_01.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. ANALYSE TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Coherence test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only topic words, without weights\n",
    "\n",
    "Keywords_Model = Topic_Model.get_topic_info()[\"Representation\"].tolist()\n",
    "docs = Texts\n",
    "\n",
    "coh_keywords = [words[:30] for words in Keywords_Model]\n",
    "print (coh_keywords)\n",
    "\n",
    "#      7.2 Get only topic words, without weights\n",
    "id2word = Dictionary ([doc.split() for doc in docs])  # Creating dictionary\n",
    "\n",
    "coherence_model = CoherenceModel (\n",
    "    topics = coh_keywords,\n",
    "    texts = [doc.split() for doc in docs],\n",
    "    dictionary = id2word,\n",
    "    topn = 30,\n",
    "    coherence = 'c_v',                                       # 'u_mass' or 'c_v'\n",
    "    )\n",
    "\n",
    "coherence_score = coherence_model.get_coherence ()\n",
    "print(f\"Coh√©rence Th√©matique: {coherence_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Sklearn Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster separation metric\n",
    "silhouette_mpnet = silhouette_score(Embeddings, TOPICS)\n",
    "print(f\"Silhouette Score: {silhouette_mpnet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity metric\n",
    "\n",
    "top_k = 10\n",
    "MPNetTopics = Topic_Model.get_topics()\n",
    "\n",
    "MPNetTopic_words = [\n",
    "    [word for word, _ in MPNetTopics[i][:top_k]]\n",
    "    for i in MPNetTopics if i != -1                     # Exclude outliers\n",
    "]\n",
    "\n",
    "all_words = sum(MPNetTopic_words, [])                   #Flatten the list of lists\n",
    "unique_words = set(all_words)\n",
    "\n",
    "diversity_mpnet = len(unique_words) / len(all_words)\n",
    "\n",
    "print (f\"Diversity Score top-{top_k}: {diversity_mpnet:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Embedding Coherence (E-Coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cache = {}\n",
    "\n",
    "def get_word_embedding (word, model):\n",
    "    if word in word_cache:\n",
    "        return word_cache[word]\n",
    "    \n",
    "    tokens = model.tokenizer(word, return_tensors = \"pt\", add_special_tokens = False)\n",
    "    with torch.no_grad():\n",
    "        out = model(tokens)\n",
    "        vec = out[\"token_embeddings\"].mean(dim=1).squeeze().numpy()\n",
    "    word_cache[word] = vec\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"word_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_cache, f)\n",
    "\n",
    "try:\n",
    "    with open(\"word_cache.pkl\", \"rb\") as f:\n",
    "        word_cache = pickle.load(f)\n",
    "    print(f\"Word cache loaded: {len(word_cache)} words\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Cache not found, creating new dictionary...\")\n",
    "    word_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_coherence (topic_words, model):\n",
    "    vecs = [get_word_embedding (w, model) for w in topic_words]\n",
    "    sims = [cosine_similarity ([v1], [v2]) [0][0] for v1, v2 in combinations(vecs, 2)]\n",
    "    return float (np.mean(sims)) if sims else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emb_Coh (KW, MODEL) :\n",
    "    EmbCoh_scores = []\n",
    "\n",
    "    for tid, words in enumerate(KW):\n",
    "        coh = embedding_coherence (words, MODEL)\n",
    "        EmbCoh_scores.append ({\n",
    "            \"Topic\": tid,\n",
    "            \"E-Coh\": coh\n",
    "        })\n",
    "    \n",
    "    E_Coh_df = pd.DataFrame(EmbCoh_scores).sort_values(\"E-Coh\", ascending = True).reset_index(drop = True)\n",
    "    EmbCoh_mean = E_Coh_df['E-Coh'].mean()\n",
    "\n",
    "    return EmbCoh_scores, EmbCoh_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbCoh, EmbCohmean = Emb_Coh(Keywords_Model, model)\n",
    "print (EmbCohmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Document Coherence (Doc-Coh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_coherence(doc_embs):\n",
    "    sim = cosine_similarity(doc_embs)\n",
    "    upper = sim[np.triu_indices_from(sim, 1)]\n",
    "    return np.mean(upper) if len(upper) > 0 else 0           # intra-topic densidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc_Coh (topics, embeddings):\n",
    "    \n",
    "    unique_topics_DocCoh = sorted (set ([t for t in topics if t != -1]))\n",
    "\n",
    "    DocCoh_scores = []\n",
    "\n",
    "    for tid in unique_topics_DocCoh:\n",
    "        idx = [i for i, t in enumerate(topics) if t ==tid]\n",
    "        if len (idx) > 1:\n",
    "            coh = doc_coherence(embeddings[idx])\n",
    "        else:\n",
    "            coh = 0.0\n",
    "        DocCoh_scores.append({\"Topic\": tid,\n",
    "                               \"Doc_Coh\": coh,\n",
    "                               \"Docs_Count\": len(idx)\n",
    "                               })\n",
    "    \n",
    "    Doc_Coh_df = pd.DataFrame(DocCoh_scores).sort_values(\"Doc_Coh\", ascending = False).reset_index(drop = True)\n",
    "    DocCoh_mean = Doc_Coh_df['Doc_Coh'].mean()\n",
    "\n",
    "    return DocCoh_scores, DocCoh_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsDocHoc = Topic_Model.topics_\n",
    "DocCoh_scores, DocCoh_mean = Doc_Coh (topicsDocHoc, Embeddings)\n",
    "print (topicsDocHoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Combinned Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOPIC_MODEL (docs, embeddings, clusters) : \n",
    "    clusterer = AgglomerativeClustering(n_clusters = clusters, distance_threshold = None)  \n",
    "\n",
    "    umap_model = UMAP (\n",
    "        n_neighbors = 5, \n",
    "        n_components = 2,\n",
    "        metric = \"cosine\",\n",
    "        min_dist = 0.1,\n",
    "        random_state = 42\n",
    "        )\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase = False, token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "    Topic_Model_Test = BERTopic (\n",
    "                        hdbscan_model = clusterer,\n",
    "                        vectorizer_model = vectorizer,\n",
    "                        umap_model = umap_model,\n",
    "                        calculate_probabilities = False,\n",
    "                        verbose = False,\n",
    "                        top_n_words = 20\n",
    "                        )\n",
    "    \n",
    "    topics, _ = Topic_Model_Test.fit_transform (docs, embeddings)\n",
    "\n",
    "    keywords_brut = Topic_Model_Test.get_topic_info()[\"Representation\"].tolist()\n",
    "\n",
    "    topics_list = Topic_Model_Test.topics_\n",
    "\n",
    "    return keywords_brut, topics, topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COHERENCE_TEST (keywords, docs) :\n",
    "    \n",
    "    texts = [doc.split() for doc in docs]           # In the original \"coherence test\" I used \"docs_clean\" instead \"docs\"\n",
    "    id2word = Dictionary (texts)\n",
    "    \n",
    "    coherence_model = CoherenceModel (\n",
    "        topics = keywords,\n",
    "        texts = texts,\n",
    "        dictionary = id2word,\n",
    "        coherence = 'c_v',\n",
    "        topn = 30\n",
    "        )\n",
    "    \n",
    "    return coherence_model.get_coherence ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIVERSITY_TEST (keywords) :\n",
    "    \n",
    "    all_words = sum(keywords, [])                   #Flatten the list of lists\n",
    "    unique_words = set(all_words)\n",
    "    \n",
    "    return len(unique_words) / len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ClusterNumber = []\n",
    "\n",
    "for k in range (60, 82, 2) :\n",
    "    KW_brut, TopicLabels, TopicsList = TOPIC_MODEL (docs_clean_ngram, Embeddings, k)\n",
    "    KW = [\n",
    "        [split_camel_case(word) for word in group]\n",
    "        for group in KW_brut\n",
    "        ]\n",
    "    COH = COHERENCE_TEST (KW, docs_clean)\n",
    "    SIL = silhouette_score (Embeddings, TopicLabels)\n",
    "    DIV = DIVERSITY_TEST (KW)\n",
    "    EmCoh_scores, EmbCoh_mean = Emb_Coh (KW, model)\n",
    "    DocCoh_scores, DocCoh_mean = Doc_Coh (TopicsList, Embeddings)\n",
    "\n",
    "    print (f\"Processing {k} clusters...\")\n",
    "\n",
    "    metrics_ClusterNumber.append ({\"Clusters\": k,\n",
    "                     \"Coherence\": COH,\n",
    "                     \"Silhouette\": SIL,\n",
    "                     \"Diversity\": DIV,\n",
    "                     \"E_Coherence\": EmbCoh_mean,\n",
    "                     \"Doc_Coherence\": DocCoh_mean,\n",
    "                     \"keywords\": KW})\n",
    "    \n",
    "    print (f\"Clusters: {k} | Coherence: {COH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ClusterNumber_df = pd.DataFrame (metrics_ClusterNumber)\n",
    "print(metrics_ClusterNumber_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ClusterNumber_df.to_csv (\"MetricsClusterNumber_302.csv\", sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_ClusterNumber_df = pd.read_csv(\"MetricsClusterNumber.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtre = metrics_ClusterNumber_df[\"Clusters\"].between(0, 150)\n",
    "metrics_ClusterNumber_df = metrics_ClusterNumber_df.loc[filtre]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_ClusterNumber_df[\"Clusters\"], metrics_ClusterNumber_df[\"Coherence\"],   marker='o', label='Coherence (c_v)')\n",
    "plt.plot(metrics_ClusterNumber_df[\"Clusters\"], metrics_ClusterNumber_df[\"Silhouette\"],   marker='o', label='Silhouette')\n",
    "plt.plot(metrics_ClusterNumber_df[\"Clusters\"], metrics_ClusterNumber_df[\"Diversity\"],   marker='o', label='Diversity')\n",
    "plt.plot(metrics_ClusterNumber_df[\"Clusters\"], metrics_ClusterNumber_df[\"E_Coherence\"],   marker='o', label='E-Coherence')\n",
    "plt.plot(metrics_ClusterNumber_df[\"Clusters\"], metrics_ClusterNumber_df[\"Doc_Coherence\"],   marker='o', label='Doc-Coherence')\n",
    "\n",
    "plt.xticks(metrics_ClusterNumber_df[\"Clusters\"])\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Metrics evolution by number of clusters\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. AI TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Define mean variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of Topics ID by document\n",
    "doc_topics = Topic_Model.get_document_info(Docs_Clean)[\"Topic\"].to_numpy()\n",
    "print(len(doc_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of abstracts\n",
    "df = pd.read_csv(\"Data_20250506.csv\", sep = \";\", encoding='utf-8')\n",
    "docs = df [\"AB_AI\"].tolist()\n",
    "display (docs[1613])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of keywords by topic\n",
    "topics_ids = [t for t in Topic_Model.get_topics()]\n",
    "\n",
    "topic_keywords = {\n",
    "    t: [split_camel_case(w) for w, _ in Topic_Model.get_topic(t)[:30]]\n",
    "    for t in topics_ids\n",
    "}\n",
    "\n",
    "print (len(topics_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Define sample of papers by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size (N, e = 0.1, z = 1.96, p = 0.5, n_max = 25):\n",
    "    \"\"\"\n",
    "    calculate sample size for a given population size N, margin of error e, z-score z, and proportion p.\n",
    "    \n",
    "    N: Population size\n",
    "    e: Margin of error (default 0.1)\n",
    "    z: Z-score for the confidence level (default 1.96 for 95% confidence)\n",
    "    p: Estimated proportion of the population (default 0.5 for maximum variability)\n",
    "    n_max: Maximum sample size (default 25)\n",
    "    \"\"\"\n",
    "    \n",
    "    num = (z**2) * p * (1 - p)\n",
    "    denom = e**2\n",
    "    n0 = num / denom\n",
    "    n = (n0* N) / (n0 + N - 1)\n",
    "    return int(min(n_max, round(n)))  # Ensure the sample size does not exceed n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ids (index_list, seed = 42, **kwargs):\n",
    "    \"\"\"Sample a random set of IDs from the index.\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    N = len(index_list)\n",
    "    k = sample_size(N, **kwargs)\n",
    "    return index_list if k >= N else random.sample(index_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_to_index = {}                                     # Create an empty dict\n",
    "\n",
    "for idx, t_id in enumerate(doc_topics):                 # enumerate run by doc_topics and returns \"idx\": position, \"t_id\": ID topic\n",
    "    if t_id == -1 :\n",
    "        continue\n",
    "    topic_to_index.setdefault(t_id, []).append(idx)     # setdefault returns or create a t_id list; append adds \"idx\" to \"t_id\" list\n",
    "\n",
    "print (len(topic_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, idxs in topic_to_index.items():\n",
    "    print(f\"Topic {t:>2}: {len(idxs)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************\n",
    "# *** Configurate SEED ***\n",
    "# ************************\n",
    "\n",
    "PARAMS = dict(\n",
    "    e = 0.1,  # Margin of error\n",
    "    z = 1.96,  # Z-score for 95% confidence\n",
    "    p = 0.5,  # Estimated proportion of the population\n",
    "    n_max = 25  # Maximum sample size\n",
    ")\n",
    "samples_by_topic = {\n",
    "    t_id: sample_ids(idxs, seed=42, **PARAMS)\n",
    "    for t_id, idxs in topic_to_index.items()\n",
    "}\n",
    "print (samples_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracts\n",
    "\n",
    "abstracts = {\n",
    "    t_id: [docs[i] for i in idxs]\n",
    "    for t_id, idxs in samples_by_topic.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Define Acid Test fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMP TEMPLATE\n",
    "\n",
    "PROMPT_ACID_BASE = \"\"\"\n",
    "SYSTEM:\n",
    "You are a senior researcher in electric-transport studies. \n",
    "Think carefully; respond only with the JSON requested.\n",
    "\n",
    "USER:\n",
    "We generated topics automatically. \n",
    "Below you see one topic defined by a list of keywords and a random sample of <N> abstracts assigned to it.\n",
    "\n",
    "TASKS\n",
    "A) Score the topic on three scales (1-10):\n",
    "   ‚Ä¢ accuracy   ‚Äì do the keywords accurately reflect the abstracts' content?\n",
    "   ‚Ä¢ coverage   ‚Äì is the theme coherent (no obvious sub-themes mixed)?\n",
    "B) If any score is ‚â§7, propose up to three **short** improvements:\n",
    "   ‚Ä¢ Better keywords or sub-labels\n",
    "   ‚Ä¢ Suggest splitting or merging with another theme\n",
    "If scores are all >7, improvements can be an empty list [].\n",
    "C) Labelling: Based on the keywords and the area of study, Five candidate labels (1-3 words each), ordered from most specific to most generic. \n",
    "   ‚Ä¢ Avoid filler words like \"system, analysis, energy, electric, electric transport, transport electrification\" unless absolutely needed.\n",
    "   ‚Ä¢ Then, choose the *best* (most specific) candidate as `\"label\"`, having in mind the result of area of study\n",
    "\n",
    "Return STRICTLY this JSON format (no extra text):\n",
    "{{\n",
    " \"accuracy\": <int>,\n",
    " \"coverage\": <int>,\n",
    " \"comment\": \"<one-sentence justification>\",\n",
    " \"improvements\": [\"<suggestion1>\", \"<suggestion2>\"]\n",
    " \"candidates\": [\"...\", \"...\", \"...\", \"...\", \"...\"],\n",
    " \"label\": \"<1-3 words>\"\n",
    "}}\n",
    "\n",
    "TOPIC_ID: {TID}\n",
    "KEYWORDS: {KW}\n",
    "ABSTRACTS: \n",
    "{LIST_OF_ABSTRACTS}\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_ACID = \"\"\"\n",
    "SYSTEM:\n",
    "You are a senior researcher in electric-transport studies. \n",
    "Think carefully; respond only with the JSON requested.\n",
    "\n",
    "USER:\n",
    "We generated topics automatically. \n",
    "Below you see one topic defined by a list of keywords and a random sample of <N> abstracts assigned to it.\n",
    "\n",
    "TASKS\n",
    "A) Score the topic on three scales (1-10):\n",
    "   ‚Ä¢ accuracy   ‚Äì do the keywords accurately reflect the abstracts' content?\n",
    "   ‚Ä¢ coverage   ‚Äì is the theme coherent (no obvious sub-themes mixed)?\n",
    "   ‚Ä¢ specificity ‚Äì are the keywords sufficiently technical/specific (not overly generic)?\n",
    "\n",
    "   Notes:\n",
    "   - If the cluster mixes multiple **technical sub-areas** that all belong clearly to the same domain (e.g. battery materials vs battery recycling), still consider it coherent.\n",
    "   - Do not penalize clusters for having **diverse technical angles** within one broader domain.\n",
    "\n",
    "B) If any score is ‚â§7, propose up to three **short** improvements:\n",
    "   ‚Ä¢ Suggest better keywords or sub-labels\n",
    "   ‚Ä¢ Indicate if splitting into subdomains is needed\n",
    "   ‚Ä¢ Indicate if merging with a closely related theme is needed\n",
    "If all scores are >7, improvements can be an empty list [].\n",
    "\n",
    "C) Labelling: Based on the keywords and the area of study, propose five candidate labels (1‚Äì3 words each), ordered from most specific to most generic. \n",
    "   ‚Ä¢ Avoid filler words like \"system, analysis, energy, electric transport\" unless truly necessary.\n",
    "   ‚Ä¢ Then, choose the *best* (most specific and representative) candidate as `\"label\"`.\n",
    "\n",
    "Return STRICTLY this JSON format (no extra text):\n",
    "{{\n",
    " \"accuracy\": <int>,\n",
    " \"coverage\": <int>,\n",
    " \"specificity\": <int>,\n",
    " \"comment\": \"<one-sentence justification>\",\n",
    " \"improvements\": [\"<suggestion1>\", \"<suggestion2>\"],\n",
    " \"candidates\": [\"...\", \"...\", \"...\", \"...\", \"...\"],\n",
    " \"label\": \"<1-3 words>\"\n",
    "}}\n",
    "\n",
    "TOPIC_ID: {TID}\n",
    "KEYWORDS: {KW}\n",
    "ABSTRACTS: \n",
    "{LIST_OF_ABSTRACTS}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(tid, topic_keywords, abstracts):\n",
    "    \"\"\"Build a clean, human-readable prompt for the LLM.\"\"\"\n",
    "\n",
    "    kw_raw = topic_keywords[tid]\n",
    "    abs_raw = abstracts[tid]\n",
    "\n",
    "    keywords_str = \", \".join(kw_raw)\n",
    "    abstracts_str = \"\\n\".join([f\"{i+1}. {abs_text}\" for i, abs_text in enumerate (abs_raw)])\n",
    "    \n",
    "    return PROMPT_ACID.format (\n",
    "        TID = tid,\n",
    "        KW = keywords_str,\n",
    "        LIST_OF_ABSTRACTS = abstracts_str\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_abstracts = abstracts[59]\n",
    "raw_topics = topic_keywords[59]\n",
    "print (raw_abstracts)\n",
    "print (raw_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = build_prompt(1, topic_keywords, abstracts)\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acid_test(tid, keywords, abstracts):\n",
    "    \"\"\" Evaluation of topics using LLM \"\"\"\n",
    "    prompt = build_prompt (tid, keywords, abstracts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "        )\n",
    "    \n",
    "    response_content = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        result = json.loads (response_content)\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[‚ö†Ô∏è] Error al parsear JSON para topic {tid}. Respuesta bruta:\")\n",
    "        print(response_content)\n",
    "        return {\n",
    "            \"accuracy\": None,\n",
    "            \"coverage\": None,\n",
    "            \"specificity\": None,\n",
    "            \"comment\": None,\n",
    "            \"improvements\": []\n",
    "        }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 ACID TEST IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_acid_test = []\n",
    "\n",
    "for tid in range (0, len(topics_ids)) :\n",
    "    print(f\"[‚ñ∂] Testing topic {tid}...\")\n",
    "\n",
    "    kw = topic_keywords[tid]\n",
    "    result = acid_test(tid, topic_keywords, abstracts)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_acid_test.append({\n",
    "        \"topic_id\": tid,\n",
    "        \"n_sample\": len(abstracts[tid]),\n",
    "        \"keywords\": \", \".join(kw),\n",
    "        \"accuracy\": result.get(\"accuracy\"),\n",
    "        \"coverage\": result.get(\"coverage\"),\n",
    "        \"specificity\": result.get(\"specificity\"),\n",
    "        \"comment\": result.get(\"comment\"),\n",
    "        \"improvements\": \"; \".join(result.get(\"improvements\", [])),\n",
    "        \"candidates\": \"; \".join(result.get(\"candidates\", [])),\n",
    "        \"label\": result.get(\"label\")\n",
    "    })\n",
    "\n",
    "    print(f\"[‚úì] Topic {tid:>2} ‚Üí relevance {result.get('relevance')}, accuracy {result.get('accuracy')}, coverage {result.get('coverage')}\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_acid = pd.DataFrame(results_acid_test)\n",
    "df_results_acid.to_csv(f\"acid_test_SIF_6678_P2.csv\", sep = \";\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 LLM TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMP TEMPLATE\n",
    "\n",
    "PROMPT_LLM = \"\"\"\n",
    "\n",
    "SYSTEM:\n",
    "You are a senior researcher in electrification of transport research.\n",
    "Think carefully; respond only with the JSON requested.\n",
    "\n",
    "USER:\n",
    "We generated topics automatically. \n",
    "Below you see one topic defined by 10 keywords and a random sample of <N> abstracts assigned to it.\n",
    "\n",
    "TASKS\n",
    "\n",
    "TASK A ‚Äì COHERENCE SCORE  \n",
    "Rate, on a scale 1-10, how internally coherent the following list of 10 keywords and abstracst is (1 = very inconsistent, 10 = very coherent).  \n",
    "Give one short justification.\n",
    "\n",
    "TASK B ‚Äì AREA OF STUDY  \n",
    "Ignoring country names, summarise in **2‚Äì3 concise phrases** what area of study these papers most likely cover.\n",
    "\n",
    "TASK C ‚Äì Labelling\n",
    "Based on the keywords and the area of study, Five candidate labels (1-3 words each), ordered from most specific to most generic. \n",
    "   Avoid filler words like \"system, analysis, energy, electric, electric transport, transport electrification\" unless absolutely needed.\n",
    "   Then, choose the *best* (most specific) candidate as `\"label\"`, having in mind the result of area of study\n",
    "\n",
    "Return STRICTLY this JSON format (no extra text):\n",
    "{{\n",
    "  \"score\": <integer>, \n",
    "  \"comment\": \"<reason>\", \n",
    "  \"area\": \"<2-3 phrases>\",\n",
    "  \"candidates\": [\"...\", \"...\", \"...\", \"...\", \"...\"],\n",
    "  \"label\": \"<1-3 words>\"\n",
    "}}\n",
    "\n",
    "TOPIC_ID: {TID}\n",
    "KEYWORDS: {KW}\n",
    "ABSTRACTS: \n",
    "{LIST_OF_ABSTRACTS}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_LLM(tid, topic_keywords, abstracts):\n",
    "    \"\"\"Build a clean, human-readable prompt for the LLM.\"\"\"\n",
    "\n",
    "    kw_raw = topic_keywords[tid]\n",
    "    abs_raw = abstracts[tid]\n",
    "\n",
    "    keywords_str = \", \".join(kw_raw)\n",
    "    abstracts_str = \"\\n\".join([f\"{i+1}. {abs_text}\" for i, abs_text in enumerate (abs_raw)])\n",
    "    \n",
    "    return PROMPT_LLM.format (\n",
    "        TID = tid,\n",
    "        KW = keywords_str,\n",
    "        LIST_OF_ABSTRACTS = abstracts_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = build_prompt_LLM(7, topic_keywords, abstracts)\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_score(tid, keywords, abstracts):\n",
    "    \"\"\"Call the model to evaluate the coherence of a topic based on keywords.\"\"\"\n",
    "    prompt = build_prompt_LLM (tid, keywords, abstracts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "        )\n",
    "    \n",
    "    response_content = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        result = json.loads (response_content)\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[‚ö†Ô∏è] Error al parsear JSON para topic {tid}. Respuesta bruta:\")\n",
    "        print(response_content)\n",
    "        return {\n",
    "            \"score\": None,\n",
    "            \"comment\": None,\n",
    "            \"area\": None,\n",
    "            \"candidates\": [],\n",
    "            \"label\": None\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_LLM_score = []\n",
    "\n",
    "for tid in range (0, 75):\n",
    "    print(f\"[‚ñ∂] Testing topic {tid}...\")\n",
    "\n",
    "    kw = topic_keywords[tid]\n",
    "    result = llm_score(tid, topic_keywords, abstracts)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_LLM_score.append({\n",
    "        \"topic_id\": tid,\n",
    "        \"keywords\": \", \".join(kw),\n",
    "        \"score\": result.get(\"score\"),\n",
    "        \"comment\": result.get(\"comment\"),\n",
    "        \"area\": result.get(\"area\"),\n",
    "        \"candidates\": \"; \".join(result.get(\"candidates\", [])),\n",
    "        \"label\": result.get(\"label\")\n",
    "    })\n",
    "\n",
    "    print(f\"[‚úì] Topic {tid:>2} ‚Üí LLM Score {result.get('score')}\")\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_LLM = pd.DataFrame(results_LLM_score)\n",
    "df_results_LLM.to_csv(\"LLM_AI_Analyse_SEED_42.csv\", sep = \";\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 NEW AI LABELS FOR CLUSTERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.1 For created clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMP TEMPLATE\n",
    "\n",
    "PROMPT_AI_LABEL = \"\"\"\n",
    "\n",
    "You are labeling transport research topics. From the following abstracts that all belong to the same Leiden k-NN cluster, generate up to 20 concise topic labels.\n",
    "\n",
    "OUTPUT\n",
    "- Return ONLY a JSON array of unique strings (no markdown, no keys, no comments).\n",
    "- Each label: 1‚Äì2 words, lowercase, ASCII, noun phrase.\n",
    "- No punctuation or quotes. Prefer singular form.\n",
    "- Allow domain abbreviations (hvac, v2g, lca, tco, ocpp, iso15118, co2).\n",
    "\n",
    "GUIDANCE\n",
    "- Capture shared themes across the cluster (not methods, not results).\n",
    "- Deduplicate near synonyms (e.g., ‚Äúheat pump‚Äù and ‚Äúheat pumps‚Äù ‚Üí ‚Äúheat pump‚Äù).\n",
    "- Avoid generic words such as: [\"electrification\",\"energy\",\"transport\",\"transportation\",\"system\",\"systems\",\"analysis\",\"study\",\"impact\",\"method\",\"methods\",\"model\",\"modeling\",\"approach\",\"framework\",\"technology\",\"technologies\"].\n",
    "\n",
    "Return STRICTLY this JSON format (no extra text):\n",
    "\n",
    "{{\n",
    "  \"labels\": [\"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "TOPIC_ID: {TID}\n",
    "ABSTRACTS: \n",
    "{LIST_OF_ABSTRACTS}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_AI_label(tid, abstracts):\n",
    "    \"\"\"Build a clean, human-readable prompt for the LLM.\"\"\"\n",
    "\n",
    "    abs_raw = abstracts[tid]\n",
    "\n",
    "    abstracts_str = \"\\n\".join([f\"{i+1}. {abs_text}\" for i, abs_text in enumerate (abs_raw)])\n",
    "    \n",
    "    return PROMPT_AI_LABEL.format (\n",
    "        TID = tid,\n",
    "        LIST_OF_ABSTRACTS = abstracts_str\n",
    "    )\n",
    "\n",
    "test = build_prompt_AI_label(927, abstracts)\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI_labeling(tid, abstracts):\n",
    "    \"\"\"Call the model to evaluate the coherence of a topic based on keywords.\"\"\"\n",
    "    prompt = build_prompt_AI_label (tid, abstracts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "        )\n",
    "    \n",
    "    response_content = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        result = json.loads (response_content)\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[‚ö†Ô∏è] Error al parsear JSON para topic {tid}. Respuesta bruta:\")\n",
    "        print(response_content)\n",
    "        return {\n",
    "            \"labell\": []\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_AI_labeling = []\n",
    "\n",
    "for tid in range (0,len(topic_to_index)):\n",
    "    print(f\"[‚ñ∂] Labeling topic {tid}...\")\n",
    "\n",
    "    result = AI_labeling(tid, abstracts)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_AI_labeling.append({\n",
    "        \"labels\": \"; \".join(result.get(\"labels\", [])),\n",
    "    })\n",
    "\n",
    "    print(f\"[‚úì] Topic {tid:>2} ‚Üí Done\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (results_AI_labeling)\n",
    "df_AI_labeling = pd.DataFrame(results_AI_labeling)\n",
    "df_AI_labeling.to_csv(f\"AI_labeling_Leiden_607.csv\", sep = \";\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 For all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMP TEMPLATE\n",
    "\n",
    "PROMPT_individual_AI_LABEL = \"\"\"\n",
    "\n",
    "You are labeling transport research topics. From the abstract below, generate up to 12 concise topic labels.\n",
    "\n",
    "OUTPUT\n",
    "- Return ONLY a JSON array of unique strings (no markdown, no keys, no comments).\n",
    "- Try go from general to specific topics\n",
    "- Each label: 1‚Äì2 words, lowercase, ASCII, noun phrase.\n",
    "- No punctuation or quotes. Prefer singular form.\n",
    "- Allow domain abbreviations (hvac, v2g, lca, tco, ocpp, iso15118, co2).\n",
    "\n",
    "GUIDANCE\n",
    "- Capture the paper‚Äôs main topics (not methods, not numerical results).\n",
    "- Deduplicate near synonyms (e.g., ‚Äúheat pump‚Äù and ‚Äúheat pumps‚Äù ‚Üí ‚Äúheat pump‚Äù).\n",
    "- Given that all text is focussed in transport electrificaction, avoid generic topics\n",
    "\n",
    "Return STRICTLY this JSON format (no extra text):\n",
    "\n",
    "{{\n",
    "  \"labels\": [...; ...; ...; ...; ...; ...; ...; ...; ...; ...; ...; ...; ...; ...; ...]\n",
    "}}\n",
    "\n",
    "TOPIC_ID: {TID}\n",
    "ABSTRACTS: \n",
    "{ABSTRACT}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_individual_AI_label(tid, abstract):\n",
    "    \"\"\"Build a clean, human-readable prompt for the LLM.\"\"\"\n",
    "  \n",
    "    return PROMPT_individual_AI_LABEL.format (\n",
    "        TID = tid,\n",
    "        ABSTRACT = abstract[tid]\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(\"Data_20250506.csv\", sep = \";\", encoding='utf-8')\n",
    "abstracts = df [\"AB_AI\"].tolist()\n",
    "test = build_prompt_individual_AI_label(937,abstracts)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_AI_labeling(tid, abstracts):\n",
    "    \"\"\"Call the model to evaluate the coherence of a topic based on keywords.\"\"\"\n",
    "    prompt = build_prompt_individual_AI_label (tid, abstracts)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "        )\n",
    "    \n",
    "    response_content = resp.choices[0].message.content.strip()\n",
    "    \n",
    "    try:\n",
    "        result = json.loads (response_content)\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"[‚ö†Ô∏è] Error al parsear JSON para topic {tid}. Respuesta bruta:\")\n",
    "        print(response_content)\n",
    "        return {\n",
    "            \"labell\": []\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_individual_AI_labeling = []\n",
    "\n",
    "for tid in (1000, 1999):\n",
    "    print(f\"[‚ñ∂] Labeling topic {tid}...\")\n",
    "\n",
    "    result = individual_AI_labeling(tid, abstracts)\n",
    "\n",
    "    # Guardar resultados\n",
    "    results_individual_AI_labeling.append({\n",
    "        \"ID\": tid,\n",
    "        \"labels\": \"; \".join(result.get(\"labels\", [])),\n",
    "    })\n",
    "\n",
    "    print(f\"[‚úì] Abstract {tid:>2} ‚Üí Done\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (results_individual_AI_labeling)\n",
    "df_individual_AI_labeling = pd.DataFrame(results_individual_AI_labeling)\n",
    "df_individual_AI_labeling.to_csv(f\"AI_individual_labeling_others.csv\", sep = \";\", index = False, encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02_Topics_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
